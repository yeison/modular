# ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
# REQUIRES: H100-GPU
# RUN: %mojo-no-debug %s | FileCheck %s

from sys.info import _current_arch, _is_sm_8x, _is_sm_9x

from builtin.io import _printf
from gpu import barrier
from gpu.host import DeviceContext
from gpu.host._compile import _get_gpu_target
from gpu.id import thread_idx
from gpu.intrinsics import threadfence
from gpu.memory import AddressSpace
from gpu.mma import (
    WGMMADescriptor,
    wgmma_async,
    wgmma_commit_group_sync,
    wgmma_fence_aligned,
    wgmma_wait_group_sync,
)
from layout import IntTuple, Layout, LayoutTensor
from layout._utils import ManagedLayoutTensor
from layout._fillers import arange
from layout.layout import print_layout
from memory import bitcast


fn wgmma_f32_kernel[
    M: Int,
    N: Int,
    K: Int,
    WMMA_M: Int,
    WMMA_N: Int,
    WMMA_K: Int,
    smem_operand_a_layout: Layout,
    smem_operand_b_layout: Layout,
    a_type: DType,
    b_type: DType,
](
    operand_a: LayoutTensor[a_type, Layout.row_major(M, K), MutableAnyOrigin],
    operand_b: LayoutTensor[b_type, Layout.row_major(K, N), MutableAnyOrigin],
    result_c: LayoutTensor[
        DType.float32, Layout.row_major(M, N), MutableAnyOrigin
    ],
):
    var smem_operand_a = LayoutTensor[
        a_type,
        smem_operand_a_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var smem_operand_b = LayoutTensor[
        b_type,
        smem_operand_b_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var c_reg = SIMD[DType.float32, 4](0)

    for k_i in range(K // WMMA_K):
        var operand_a_tile = operand_a.tile[M, WMMA_K](0, k_i)
        var operand_b_tile = operand_b.tile[WMMA_K, N](k_i, 0)
        var operand_a_sm_tile = smem_operand_a.tile[M, WMMA_K](0, k_i)
        var operand_b_sm_tile = smem_operand_b.tile[WMMA_K, N](k_i, 0)

        if thread_idx.x == 0:
            operand_a_sm_tile.copy_from(operand_a_tile)
            operand_b_sm_tile.copy_from(operand_b_tile)

        barrier()

        var mat_a_desc = WGMMADescriptor.create[8, 64](operand_a_sm_tile.ptr)
        var mat_b_desc = WGMMADescriptor.create[1, 8](operand_b_sm_tile.ptr)

        wgmma_fence_aligned()

        c_reg = wgmma_async[
            WMMA_M,
            WMMA_N,
            WMMA_K,
            a_type=a_type,
            b_type=b_type,
        ](mat_a_desc, mat_b_desc, c_reg)
        wgmma_commit_group_sync()
        wgmma_wait_group_sync()
        threadfence()
        wgmma_fence_aligned()

    var warp_id = thread_idx.x // 32
    var lan_id = thread_idx.x % 32
    # Refer to this layout:
    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-D.png
    # Each warp updates a 16x8 tile, and within each tile,
    # every thread updates a 1x2 vector. The resulting distribution layout
    # is as follows:
    var th_local_res = result_c.tile[16, 8](warp_id, 0).vectorize[
        1, 2
    ]().distribute[Layout.row_major(8, 4)](lan_id)
    th_local_res[0][0] = c_reg[0]
    th_local_res[0][1] = c_reg[1]
    th_local_res[1][0] = c_reg[2]
    th_local_res[1][1] = c_reg[3]


fn wgmma_f16_kernel[
    M: Int,
    N: Int,
    K: Int,
    WMMA_M: Int,
    WMMA_N: Int,
    WMMA_K: Int,
    smem_operand_a_layout: Layout,
    smem_operand_b_layout: Layout,
    a_type: DType,
    b_type: DType,
](
    operand_a: LayoutTensor[a_type, Layout.row_major(M, K), MutableAnyOrigin],
    operand_b: LayoutTensor[b_type, Layout.row_major(K, N), MutableAnyOrigin],
    result_c: LayoutTensor[
        DType.float16, Layout.row_major(M, N), MutableAnyOrigin
    ],
):
    var smem_operand_a = LayoutTensor[
        a_type,
        smem_operand_a_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var smem_operand_b = LayoutTensor[
        b_type,
        smem_operand_b_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var c_reg = SIMD[DType.uint32, 2](0)

    for k_i in range(K // WMMA_K):
        var operand_a_tile = operand_a.tile[M, WMMA_K](0, k_i)
        var operand_b_tile = operand_b.tile[WMMA_K, N](k_i, 0)
        var operand_a_sm_tile = smem_operand_a.tile[M, WMMA_K](0, k_i)
        var operand_b_sm_tile = smem_operand_b.tile[WMMA_K, N](k_i, 0)

        if thread_idx.x == 0:
            operand_a_sm_tile.copy_from(operand_a_tile)
            operand_b_sm_tile.copy_from(operand_b_tile)

        barrier()

        var mat_a_desc = WGMMADescriptor.create[8, 64](operand_a_sm_tile.ptr)
        var mat_b_desc = WGMMADescriptor.create[1, 8](operand_b_sm_tile.ptr)

        wgmma_fence_aligned()

        c_reg = wgmma_async[
            WMMA_M,
            WMMA_N,
            WMMA_K,
            a_type=a_type,
            b_type=b_type,
            accum_type = DType.float16,
        ](mat_a_desc, mat_b_desc, c_reg)
        wgmma_commit_group_sync()
        wgmma_wait_group_sync()
        threadfence()
        wgmma_fence_aligned()

    var warp_id = thread_idx.x // 32
    var lan_id = thread_idx.x % 32
    # Refer to this layout:
    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-D.png
    # Each warp updates a 16x8 tile, and within each tile,
    # every thread updates a 1x2 vector. The resulting distribution layout
    # is as follows:
    c0 = bitcast[DType.float16, 4](c_reg)
    var th_local_res = result_c.tile[16, 8](warp_id, 0).vectorize[
        1, 2
    ]().distribute[Layout.row_major(8, 4)](lan_id)
    th_local_res[0][0] = c0[0]
    th_local_res[0][1] = c0[1]
    th_local_res[1][0] = c0[2]
    th_local_res[1][1] = c0[3]


# CHECK-LABEL: wgmma_e4m3_e4m3_f32_64x8x32
# CHECK: 10440.0 10928.0 11385.0 11826.0 12366.0 12858.0 13315.0 13884.0
# CHECK: 10928.0 11464.0 11952.0 12409.0 12978.0 13518.0 14010.0 14595.0
# CHECK: 11385.0 11952.0 12487.0 12974.0 13558.0 14126.0 14665.0 15284.0
# CHECK: 11826.0 12409.0 12974.0 13507.0 14120.0 14702.0 15268.0 15933.0
# CHECK: 12366.0 12978.0 13558.0 14120.0 14794.0 15404.0 15983.0 16690.0
# CHECK: 12858.0 13518.0 14126.0 14702.0 15404.0 16074.0 16680.0 17398.0
# CHECK: 13315.0 14010.0 14665.0 15268.0 15983.0 16680.0 17344.0 18090.0
# CHECK: 13884.0 14595.0 15284.0 15933.0 16690.0 17398.0 18090.0 18908.0
# CHECK: 14420.0 15164.0 15868.0 16550.0 17352.0 18102.0 18804.0 19648.0
# CHECK: 14908.0 15700.0 16436.0 17132.0 17966.0 18760.0 19502.0 20356.0
# CHECK: 15365.0 16188.0 16970.0 17698.0 18544.0 19370.0 20154.0 21048.0
# CHECK: 15806.0 16644.0 17458.0 18230.0 19108.0 19944.0 20760.0 21694.0
# CHECK: 16346.0 17214.0 18042.0 18844.0 19782.0 20648.0 21474.0 22454.0
# CHECK: 16838.0 17754.0 18610.0 19426.0 20392.0 21318.0 22172.0 23162.0
# CHECK: 17294.0 18246.0 19148.0 19992.0 20970.0 21924.0 22836.0 23854.0
# CHECK: 17864.0 18830.0 19768.0 20656.0 21678.0 22642.0 23582.0 24672.0
# CHECK: 18400.0 19400.0 20352.0 21274.0 22340.0 23346.0 24296.0 25412.0
# CHECK: 18888.0 19936.0 20920.0 21856.0 22954.0 24004.0 24994.0 26120.0
# CHECK: 19344.0 20424.0 21456.0 22424.0 23536.0 24618.0 25652.0 26818.0
# CHECK: 19786.0 20880.0 21942.0 22956.0 24098.0 25192.0 26256.0 27464.0
# CHECK: 20324.0 21450.0 22524.0 23566.0 24768.0 25890.0 26964.0 28216.0
# CHECK: 20814.0 21988.0 23094.0 24148.0 25378.0 26560.0 27662.0 28924.0
# CHECK: 21272.0 22478.0 23632.0 24718.0 25960.0 27170.0 28332.0 29622.0
# CHECK: 21840.0 23064.0 24248.0 25380.0 26668.0 27888.0 29076.0 30440.0
# CHECK: 22376.0 23632.0 24832.0 25992.0 27324.0 28588.0 29784.0 31172.0
# CHECK: 22864.0 24168.0 25400.0 26576.0 27936.0 29244.0 30484.0 31880.0
# CHECK: 23320.0 24656.0 25936.0 27144.0 28520.0 29856.0 31140.0 32580.0
# CHECK: 23762.0 25112.0 26422.0 27676.0 29082.0 30432.0 31742.0 33224.0
# CHECK: 24300.0 25682.0 27004.0 28286.0 29752.0 31130.0 32452.0 33972.0
# CHECK: 24790.0 26220.0 27574.0 28868.0 30362.0 31800.0 33148.0 34684.0
# CHECK: 25248.0 26710.0 28112.0 29438.0 30944.0 32410.0 33820.0 35380.0
# CHECK: 25816.0 27296.0 28728.0 30100.0 31652.0 33128.0 34564.0 36200.0
# CHECK: 26352.0 27864.0 29312.0 30712.0 32308.0 33828.0 35272.0 36932.0
# CHECK: 26840.0 28400.0 29880.0 31296.0 32920.0 34484.0 35972.0 37640.0
# CHECK: 27296.0 28888.0 30416.0 31864.0 33504.0 35096.0 36628.0 38340.0
# CHECK: 27736.0 29344.0 30904.0 32400.0 34072.0 35680.0 37240.0 38996.0
# CHECK: 28144.0 29784.0 31356.0 32880.0 34596.0 36232.0 37804.0 39584.0
# CHECK: 28512.0 30192.0 31796.0 33332.0 35076.0 36756.0 38356.0 40148.0
# CHECK: 29112.0 30816.0 32460.0 34028.0 35816.0 37524.0 39168.0 41020.0
# CHECK: 29680.0 31416.0 33080.0 34684.0 36500.0 38248.0 39916.0 41808.0
# CHECK: 30208.0 31984.0 33680.0 35304.0 37156.0 38932.0 40640.0 42556.0
# CHECK: 30704.0 32512.0 34248.0 35904.0 37776.0 39588.0 41324.0 43280.0
# CHECK: 31176.0 33008.0 34776.0 36472.0 38376.0 40208.0 41980.0 43964.0
# CHECK: 31616.0 33480.0 35272.0 37000.0 38944.0 40808.0 42600.0 44620.0
# CHECK: 32016.0 33920.0 35740.0 37488.0 39460.0 41360.0 43180.0 45216.0
# CHECK: 32648.0 34576.0 36436.0 38212.0 40236.0 42164.0 44020.0 46116.0
# CHECK: 33248.0 35208.0 37092.0 38908.0 40960.0 42940.0 44824.0 46956.0
# CHECK: 33800.0 35808.0 37720.0 39556.0 41644.0 43648.0 45580.0 47736.0
# CHECK: 34320.0 36360.0 38320.0 40184.0 42292.0 44332.0 46288.0 48492.0
# CHECK: 34824.0 36880.0 38872.0 40784.0 42920.0 44980.0 46972.0 49200.0
# CHECK: 35296.0 37384.0 39392.0 41336.0 43520.0 45608.0 47620.0 49884.0
# CHECK: 35720.0 37856.0 39896.0 41856.0 44072.0 46208.0 48248.0 50532.0
# CHECK: 36116.0 38280.0 40364.0 42352.0 44580.0 46744.0 48828.0 51136.0
# CHECK: 36500.0 38676.0 40788.0 42820.0 45076.0 47252.0 49364.0 51716.0
# CHECK: 37104.0 39316.0 41440.0 43500.0 45832.0 48036.0 50160.0 52572.0
# CHECK: 37660.0 39920.0 42076.0 44144.0 46500.0 48776.0 50924.0 53344.0
# CHECK: 38184.0 40476.0 42680.0 44780.0 47144.0 49444.0 51664.0 54108.0
# CHECK: 38692.0 41000.0 43236.0 45384.0 47780.0 50088.0 52332.0 54848.0
# CHECK: 39168.0 41508.0 43760.0 45940.0 48384.0 50724.0 52976.0 55516.0
# CHECK: 39596.0 41984.0 44268.0 46464.0 48940.0 51328.0 53612.0 56160.0
# CHECK: 39996.0 42412.0 44740.0 46964.0 49452.0 51868.0 54196.0 56772.0
# CHECK: 40632.0 43068.0 45424.0 47692.0 50240.0 52668.0 55024.0 57676.0
# CHECK: 41232.0 43704.0 46080.0 48376.0 50968.0 53456.0 55824.0 58504.0
# CHECK: 41784.0 44304.0 46712.0 49024.0 51640.0 54168.0 56592.0 59280.0
def wgmma_e4m3_e4m3_f32_64x8x32(ctx: DeviceContext):
    print("== wgmma_e4m3_e4m3_f32_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(M, K),
    ](ctx)
    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float32,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias wgmma_e4m3_e4m3_f32_kernel_fn = wgmma_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e4m3fn,
        b_type = DType.float8_e4m3fn,
    ]
    ctx.enqueue_function[wgmma_e4m3_e4m3_f32_kernel_fn](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e5m2_e5m2_f32_64x8x32
# CHECK: 10500.0 10968.0 11421.0 11866.0 12266.0 12642.0 13239.0 13828.0
# CHECK: 10968.0 11524.0 11992.0 12445.0 12890.0 13290.0 13922.0 14519.0
# CHECK: 11421.0 11992.0 12547.0 13014.0 13466.0 13910.0 14565.0 15196.0
# CHECK: 11866.0 12445.0 13014.0 13567.0 14032.0 14482.0 15180.0 15833.0
# CHECK: 12266.0 12890.0 13466.0 14032.0 14582.0 15044.0 15747.0 16442.0
# CHECK: 12642.0 13290.0 13910.0 14482.0 15044.0 15590.0 16304.0 17002.0
# CHECK: 13239.0 13922.0 14565.0 15180.0 15747.0 16304.0 17164.0 17874.0
# CHECK: 13828.0 14519.0 15196.0 15833.0 16442.0 17002.0 17874.0 18728.0
# CHECK: 14368.0 15108.0 15792.0 16462.0 17092.0 17694.0 18568.0 19432.0
# CHECK: 14852.0 15648.0 16380.0 17056.0 17718.0 18340.0 19254.0 20120.0
# CHECK: 15320.0 16132.0 16920.0 17644.0 18312.0 18966.0 19900.0 20806.0
# CHECK: 15750.0 16600.0 17402.0 18180.0 18894.0 19552.0 20516.0 21440.0
# CHECK: 16148.0 17030.0 17868.0 18658.0 19424.0 20126.0 21092.0 22044.0
# CHECK: 16778.0 17684.0 18554.0 19380.0 20158.0 20912.0 21986.0 22940.0
# CHECK: 17392.0 18314.0 19208.0 20066.0 20880.0 21646.0 22772.0 23834.0
# CHECK: 17964.0 18928.0 19836.0 20716.0 21560.0 22360.0 23496.0 24608.0
# CHECK: 18488.0 19500.0 20448.0 21340.0 22204.0 23032.0 24200.0 25320.0
# CHECK: 18988.0 20024.0 21020.0 21952.0 22828.0 23676.0 24872.0 26024.0
# CHECK: 19440.0 20524.0 21544.0 22524.0 23440.0 24300.0 25516.0 26696.0
# CHECK: 19852.0 20976.0 22044.0 23048.0 24012.0 24912.0 26140.0 27340.0
# CHECK: 20248.0 21388.0 22492.0 23540.0 24524.0 25468.0 26732.0 27940.0
# CHECK: 20628.0 21784.0 22904.0 23988.0 25016.0 25980.0 27288.0 28532.0
# CHECK: 21256.0 22420.0 23556.0 24656.0 25720.0 26728.0 28120.0 29408.0
# CHECK: 21828.0 23048.0 24188.0 25300.0 26376.0 27416.0 28848.0 30216.0
# CHECK: 22344.0 23620.0 24816.0 25932.0 27020.0 28072.0 29536.0 30944.0
# CHECK: 22852.0 24136.0 25388.0 26560.0 27652.0 28716.0 30192.0 31632.0
# CHECK: 23320.0 24644.0 25904.0 27132.0 28280.0 29348.0 30836.0 32288.0
# CHECK: 23732.0 25112.0 26412.0 27648.0 28852.0 29976.0 31468.0 32932.0
# CHECK: 24124.0 25524.0 26876.0 28148.0 29356.0 30532.0 32076.0 33540.0
# CHECK: 24768.0 26172.0 27544.0 28868.0 30112.0 31292.0 32952.0 34468.0
# CHECK: 25384.0 26816.0 28192.0 29536.0 30832.0 32048.0 33712.0 35344.0
# CHECK: 25944.0 27432.0 28832.0 30176.0 31488.0 32752.0 34448.0 36080.0
# CHECK: 26464.0 27992.0 29448.0 30816.0 32128.0 33408.0 35152.0 36816.0
# CHECK: 26976.0 28512.0 30008.0 31432.0 32768.0 34048.0 35808.0 37520.0
# CHECK: 27432.0 29024.0 30528.0 31992.0 33384.0 34688.0 36448.0 38176.0
# CHECK: 27832.0 29480.0 31040.0 32512.0 33944.0 35304.0 37088.0 38816.0
# CHECK: 28224.0 29880.0 31496.0 33024.0 34464.0 35864.0 37704.0 39456.0
# CHECK: 28600.0 30272.0 31896.0 33480.0 34976.0 36384.0 38264.0 40072.0
# CHECK: 28968.0 30648.0 32280.0 33864.0 35408.0 36864.0 38744.0 40584.0
# CHECK: 29280.0 31016.0 32656.0 34248.0 35792.0 37296.0 39224.0 41064.0
# CHECK: 29568.0 31328.0 33024.0 34624.0 36176.0 37680.0 39656.0 41544.0
# CHECK: 29848.0 31616.0 33336.0 34992.0 36552.0 38064.0 40040.0 41976.0
# CHECK: 30600.0 32408.0 34136.0 35816.0 37432.0 38952.0 41064.0 43000.0
# CHECK: 31328.0 33160.0 34928.0 36616.0 38256.0 39832.0 41952.0 44024.0
# CHECK: 31968.0 33888.0 35680.0 37408.0 39056.0 40656.0 42832.0 44912.0
# CHECK: 32608.0 34528.0 36400.0 38144.0 39824.0 41424.0 43616.0 45744.0
# CHECK: 33224.0 35168.0 37040.0 38864.0 40560.0 42192.0 44384.0 46528.0
# CHECK: 33752.0 35784.0 37680.0 39504.0 41280.0 42928.0 45152.0 47296.0
# CHECK: 34272.0 36312.0 38296.0 40144.0 41920.0 43648.0 45888.0 48064.0
# CHECK: 34784.0 36832.0 38824.0 40760.0 42560.0 44288.0 46608.0 48800.0
# CHECK: 35272.0 37344.0 39344.0 41288.0 43176.0 44928.0 47248.0 49520.0
# CHECK: 35736.0 37832.0 39856.0 41808.0 43704.0 45544.0 47888.0 50160.0
# CHECK: 36128.0 38296.0 40344.0 42320.0 44224.0 46072.0 48504.0 50800.0
# CHECK: 36504.0 38688.0 40808.0 42808.0 44736.0 46592.0 49032.0 51416.0
# CHECK: 36872.0 39064.0 41192.0 43256.0 45200.0 47072.0 49512.0 51896.0
# CHECK: 37168.0 39432.0 41568.0 43640.0 45648.0 47536.0 49992.0 52376.0
# CHECK: 37456.0 39728.0 41936.0 44016.0 46032.0 47984.0 50456.0 52856.0
# CHECK: 38248.0 40528.0 42744.0 44896.0 46920.0 48880.0 51544.0 53960.0
# CHECK: 39032.0 41320.0 43544.0 45704.0 47800.0 49768.0 52440.0 55048.0
# CHECK: 39712.0 42104.0 44336.0 46504.0 48608.0 50648.0 53328.0 55944.0
# CHECK: 40352.0 42784.0 45120.0 47296.0 49408.0 51456.0 54208.0 56832.0
# CHECK: 40992.0 43424.0 45792.0 48064.0 50176.0 52224.0 54976.0 57664.0
# CHECK: 41568.0 44064.0 46432.0 48736.0 50944.0 52992.0 55744.0 58432.0
# CHECK: 42112.0 44640.0 47072.0 49376.0 51616.0 53760.0 56512.0 59200.0
def wgmma_e5m2_e5m2_f32_64x8x32(ctx: DeviceContext):
    print("== wgmma_e5m2_e5m2_f32_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float32,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e5m2,
        b_type = DType.float8_e5m2,
    ]
    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e4m3_e5m2_f32_64x8x32
# CHECK: 10460.0 10948.0 11405.0 11848.0 12258.0 12628.0 13231.0 13800.0
# CHECK: 10948.0 11484.0 11972.0 12429.0 12872.0 13282.0 13908.0 14511.0
# CHECK: 11404.0 11972.0 12507.0 12994.0 13450.0 13892.0 14557.0 15182.0
# CHECK: 11846.0 12428.0 12994.0 13527.0 14012.0 14466.0 15162.0 15825.0
# CHECK: 12384.0 12998.0 13577.0 14140.0 14670.0 15152.0 15891.0 16584.0
# CHECK: 12878.0 13536.0 14146.0 14721.0 15280.0 15806.0 16572.0 17306.0
# CHECK: 13332.0 14030.0 14683.0 15288.0 15858.0 16412.0 17220.0 17982.0
# CHECK: 13904.0 14612.0 15304.0 15951.0 16550.0 17114.0 17982.0 18784.0
# CHECK: 14452.0 15184.0 15885.0 16570.0 17210.0 17802.0 18678.0 19540.0
# CHECK: 14928.0 15732.0 16456.0 17148.0 17826.0 18458.0 19362.0 20230.0
# CHECK: 15381.0 16208.0 17002.0 17718.0 18402.0 19070.0 20012.0 20908.0
# CHECK: 15826.0 16660.0 17478.0 18262.0 18968.0 19642.0 20620.0 21552.0
# CHECK: 16358.0 17234.0 18058.0 18864.0 19638.0 20332.0 21346.0 22314.0
# CHECK: 16858.0 17766.0 18630.0 19442.0 20236.0 20998.0 22032.0 23034.0
# CHECK: 17310.0 18266.0 19160.0 20012.0 20810.0 21592.0 22692.0 23714.0
# CHECK: 17884.0 18846.0 19788.0 20668.0 21506.0 22290.0 23442.0 24528.0
# CHECK: 18440.0 19420.0 20368.0 21294.0 22160.0 22982.0 24136.0 25272.0
# CHECK: 18908.0 19976.0 20940.0 21872.0 22782.0 23632.0 24822.0 25960.0
# CHECK: 19360.0 20444.0 21496.0 22444.0 23360.0 24254.0 25472.0 26646.0
# CHECK: 19804.0 20896.0 21962.0 22996.0 23926.0 24824.0 26084.0 27284.0
# CHECK: 20332.0 21468.0 22540.0 23586.0 24600.0 25510.0 26804.0 28044.0
# CHECK: 20836.0 21996.0 23112.0 24164.0 25190.0 26184.0 27490.0 28764.0
# CHECK: 21288.0 22500.0 23640.0 24736.0 25768.0 26774.0 28164.0 29450.0
# CHECK: 21860.0 23080.0 24270.0 25388.0 26462.0 27472.0 28904.0 30272.0
# CHECK: 22416.0 23652.0 24848.0 26014.0 27108.0 28158.0 29592.0 31000.0
# CHECK: 22884.0 24208.0 25420.0 26592.0 27734.0 28804.0 30278.0 31688.0
# CHECK: 23336.0 24676.0 25976.0 27164.0 28312.0 29430.0 30924.0 32374.0
# CHECK: 23782.0 25128.0 26442.0 27716.0 28878.0 30000.0 31540.0 33008.0
# CHECK: 24308.0 25702.0 27020.0 28306.0 29552.0 30686.0 32260.0 33772.0
# CHECK: 24810.0 26228.0 27594.0 28884.0 30142.0 31360.0 32944.0 34492.0
# CHECK: 25264.0 26730.0 28120.0 29458.0 30720.0 31950.0 33620.0 35176.0
# CHECK: 25836.0 27312.0 28748.0 30108.0 31416.0 32648.0 34360.0 36000.0
# CHECK: 26392.0 27884.0 29328.0 30732.0 32060.0 33336.0 35048.0 36728.0
# CHECK: 26860.0 28440.0 29900.0 31312.0 32684.0 33980.0 35736.0 37416.0
# CHECK: 27312.0 28908.0 30456.0 31884.0 33264.0 34604.0 36380.0 38104.0
# CHECK: 27756.0 29360.0 30924.0 32440.0 33836.0 35184.0 37004.0 38748.0
# CHECK: 28152.0 29804.0 31372.0 32900.0 34380.0 35740.0 37564.0 39348.0
# CHECK: 28524.0 30200.0 31816.0 33348.0 34840.0 36284.0 38120.0 39908.0
# CHECK: 29120.0 30828.0 32468.0 34048.0 35544.0 37000.0 38984.0 40784.0
# CHECK: 29708.0 31424.0 33092.0 34692.0 36232.0 37688.0 39680.0 41624.0
# CHECK: 30248.0 32012.0 33688.0 35316.0 36876.0 38376.0 40368.0 42320.0
# CHECK: 30732.0 32552.0 34276.0 35912.0 37500.0 39020.0 41056.0 43008.0
# CHECK: 31200.0 33036.0 34816.0 36500.0 38096.0 39644.0 41700.0 43696.0
# CHECK: 31628.0 33504.0 35300.0 37040.0 38684.0 40240.0 42324.0 44340.0
# CHECK: 32024.0 33932.0 35764.0 37516.0 39212.0 40812.0 42900.0 44940.0
# CHECK: 32652.0 34584.0 36448.0 38236.0 39944.0 41596.0 43792.0 45836.0
# CHECK: 33264.0 35212.0 37100.0 38920.0 40664.0 42328.0 44576.0 46728.0
# CHECK: 33836.0 35824.0 37724.0 39564.0 41336.0 43032.0 45288.0 47488.0
# CHECK: 34360.0 36396.0 38336.0 40188.0 41980.0 43704.0 45992.0 48200.0
# CHECK: 34860.0 36920.0 38908.0 40800.0 42604.0 44348.0 46664.0 48904.0
# CHECK: 35312.0 37420.0 39432.0 41372.0 43216.0 44972.0 47308.0 49576.0
# CHECK: 35724.0 37872.0 39932.0 41896.0 43788.0 45584.0 47932.0 50220.0
# CHECK: 36120.0 38284.0 40380.0 42388.0 44300.0 46140.0 48524.0 50820.0
# CHECK: 36500.0 38680.0 40792.0 42836.0 44792.0 46652.0 49080.0 51412.0
# CHECK: 37128.0 39316.0 41444.0 43504.0 45496.0 47400.0 49912.0 52288.0
# CHECK: 37700.0 39944.0 42076.0 44148.0 46152.0 48088.0 50640.0 53096.0
# CHECK: 38216.0 40516.0 42704.0 44780.0 46796.0 48744.0 51328.0 53824.0
# CHECK: 38724.0 41032.0 43276.0 45408.0 47428.0 49388.0 51984.0 54512.0
# CHECK: 39192.0 41540.0 43792.0 45980.0 48056.0 50020.0 52628.0 55168.0
# CHECK: 39604.0 42008.0 44300.0 46496.0 48628.0 50648.0 53260.0 55812.0
# CHECK: 39996.0 42420.0 44764.0 46996.0 49132.0 51204.0 53868.0 56420.0
# CHECK: 40640.0 43068.0 45432.0 47716.0 49888.0 51964.0 54744.0 57348.0
# CHECK: 41256.0 43712.0 46080.0 48384.0 50608.0 52720.0 55504.0 58224.0
# CHECK: 41816.0 44328.0 46720.0 49024.0 51264.0 53424.0 56240.0 58960.0
def wgmma_e4m3_e5m2_f32_64x8x32(ctx: DeviceContext):
    print("== wgmma_e4m3_e5m2_f32_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float32,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e4m3fn,
        b_type = DType.float8_e5m2,
    ]
    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e5m2_e4m3_f32_64x8x32
# CHECK: 10460.0 10948.0 11404.0 11846.0 12384.0 12878.0 13332.0 13904.0
# CHECK: 10948.0 11484.0 11972.0 12428.0 12998.0 13536.0 14030.0 14612.0
# CHECK: 11405.0 11972.0 12507.0 12994.0 13577.0 14146.0 14683.0 15304.0
# CHECK: 11848.0 12429.0 12994.0 13527.0 14140.0 14721.0 15288.0 15951.0
# CHECK: 12258.0 12872.0 13450.0 14012.0 14670.0 15280.0 15858.0 16550.0
# CHECK: 12628.0 13282.0 13892.0 14466.0 15152.0 15806.0 16412.0 17114.0
# CHECK: 13231.0 13908.0 14557.0 15162.0 15891.0 16572.0 17220.0 17982.0
# CHECK: 13800.0 14511.0 15182.0 15825.0 16584.0 17306.0 17982.0 18784.0
# CHECK: 14328.0 15080.0 15784.0 16448.0 17244.0 17996.0 18712.0 19540.0
# CHECK: 14824.0 15608.0 16352.0 17048.0 17864.0 18652.0 19396.0 20264.0
# CHECK: 15296.0 16104.0 16880.0 17616.0 18464.0 19272.0 20052.0 20948.0
# CHECK: 15738.0 16576.0 17374.0 18140.0 19026.0 19864.0 20662.0 21592.0
# CHECK: 16140.0 17018.0 17844.0 18630.0 19544.0 20418.0 21244.0 22190.0
# CHECK: 16774.0 17676.0 18542.0 19356.0 20322.0 21224.0 22086.0 23092.0
# CHECK: 17376.0 18310.0 19200.0 20054.0 21048.0 22002.0 22892.0 23934.0
# CHECK: 17928.0 18912.0 19832.0 20708.0 21740.0 22720.0 23660.0 24728.0
# CHECK: 18448.0 19464.0 20432.0 21336.0 22388.0 23404.0 24368.0 25484.0
# CHECK: 18952.0 19984.0 20984.0 21936.0 23016.0 24052.0 25052.0 26192.0
# CHECK: 19424.0 20488.0 21504.0 22488.0 23616.0 24680.0 25700.0 26876.0
# CHECK: 19848.0 20960.0 22008.0 23008.0 24168.0 25280.0 26328.0 27524.0
# CHECK: 20244.0 21384.0 22476.0 23504.0 24676.0 25816.0 26908.0 28128.0
# CHECK: 20628.0 21780.0 22900.0 23972.0 25172.0 26324.0 27444.0 28708.0
# CHECK: 21232.0 22420.0 23552.0 24652.0 25928.0 27108.0 28240.0 29564.0
# CHECK: 21788.0 23024.0 24188.0 25296.0 26596.0 27848.0 29004.0 30336.0
# CHECK: 22312.0 23580.0 24792.0 25932.0 27240.0 28516.0 29744.0 31100.0
# CHECK: 22820.0 24104.0 25348.0 26536.0 27876.0 29160.0 30412.0 31840.0
# CHECK: 23296.0 24612.0 25872.0 27092.0 28480.0 29796.0 31056.0 32508.0
# CHECK: 23724.0 25088.0 26380.0 27616.0 29036.0 30400.0 31692.0 33152.0
# CHECK: 24124.0 25516.0 26852.0 28116.0 29548.0 30940.0 32276.0 33764.0
# CHECK: 24760.0 26172.0 27536.0 28844.0 30336.0 31740.0 33104.0 34668.0
# CHECK: 25360.0 26808.0 28192.0 29528.0 31064.0 32528.0 33904.0 35496.0
# CHECK: 25912.0 27408.0 28824.0 30176.0 31736.0 33240.0 34672.0 36272.0
# CHECK: 26432.0 27960.0 29424.0 30808.0 32384.0 33912.0 35384.0 37040.0
# CHECK: 26936.0 28480.0 29976.0 31408.0 33016.0 34560.0 36056.0 37752.0
# CHECK: 27408.0 28984.0 30496.0 31960.0 33616.0 35192.0 36704.0 38424.0
# CHECK: 27832.0 29456.0 31000.0 32480.0 34168.0 35792.0 37336.0 39072.0
# CHECK: 28224.0 29880.0 31472.0 32984.0 34688.0 36344.0 37936.0 39704.0
# CHECK: 28600.0 30272.0 31896.0 33456.0 35192.0 36864.0 38488.0 40304.0
# CHECK: 28944.0 30648.0 32280.0 33864.0 35640.0 37336.0 38968.0 40808.0
# CHECK: 29256.0 30992.0 32656.0 34248.0 36048.0 37784.0 39440.0 41288.0
# CHECK: 29552.0 31304.0 33000.0 34624.0 36432.0 38192.0 39888.0 41760.0
# CHECK: 29832.0 31600.0 33312.0 34968.0 36808.0 38576.0 40296.0 42208.0
# CHECK: 30592.0 32392.0 34120.0 35792.0 37728.0 39528.0 41256.0 43256.0
# CHECK: 31288.0 33152.0 34912.0 36600.0 38552.0 40448.0 42208.0 44216.0
# CHECK: 31936.0 33848.0 35672.0 37392.0 39360.0 41272.0 43128.0 45168.0
# CHECK: 32568.0 34496.0 36360.0 38136.0 40128.0 42048.0 43912.0 46040.0
# CHECK: 33184.0 35128.0 37008.0 38824.0 40872.0 42816.0 44688.0 46824.0
# CHECK: 33752.0 35744.0 37640.0 39472.0 41560.0 43560.0 45456.0 47600.0
# CHECK: 34272.0 36312.0 38256.0 40104.0 42208.0 44248.0 46200.0 48368.0
# CHECK: 34776.0 36832.0 38824.0 40720.0 42840.0 44896.0 46888.0 49112.0
# CHECK: 35264.0 37336.0 39344.0 41288.0 43456.0 45528.0 47536.0 49800.0
# CHECK: 35704.0 37824.0 39848.0 41808.0 44024.0 46144.0 48168.0 50448.0
# CHECK: 36096.0 38264.0 40336.0 42312.0 44544.0 46712.0 48784.0 51080.0
# CHECK: 36472.0 38656.0 40776.0 42800.0 45048.0 47232.0 49352.0 51696.0
# CHECK: 36840.0 39032.0 41160.0 43224.0 45512.0 47704.0 49832.0 52216.0
# CHECK: 37168.0 39400.0 41536.0 43608.0 45936.0 48168.0 50304.0 52696.0
# CHECK: 37456.0 39728.0 41904.0 43984.0 46320.0 48592.0 50768.0 53168.0
# CHECK: 38248.0 40528.0 42744.0 44864.0 47272.0 49552.0 51768.0 54272.0
# CHECK: 38984.0 41320.0 43544.0 45704.0 48152.0 50504.0 52728.0 55272.0
# CHECK: 39664.0 42056.0 44336.0 46504.0 48992.0 51384.0 53680.0 56232.0
# CHECK: 40320.0 42736.0 45072.0 47296.0 49792.0 52224.0 54560.0 57184.0
# CHECK: 40960.0 43392.0 45744.0 48016.0 50560.0 52992.0 55360.0 58016.0
# CHECK: 41552.0 44032.0 46400.0 48688.0 51280.0 53760.0 56128.0 58816.0
# CHECK: 42096.0 44624.0 47040.0 49344.0 51952.0 54480.0 56896.0 59584.0
def wgmma_e5m2_e4m3_f32_64x8x32(ctx: DeviceContext):
    print("== wgmma_e5m2_e4m3_f32_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float32,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e5m2,
        b_type = DType.float8_e4m3fn,
    ]
    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e4m3_e4m3_f16_64x8x32
# CHECK: 10440.0 10928.0 11384.0 11824.0 12368.0 12856.0 13312.0 13888.0
# CHECK: 10928.0 11464.0 11952.0 12408.0 12976.0 13520.0 14008.0 14592.0
# CHECK: 11384.0 11952.0 12488.0 12976.0 13560.0 14128.0 14664.0 15280.0
# CHECK: 11824.0 12408.0 12976.0 13504.0 14120.0 14704.0 15264.0 15936.0
# CHECK: 12368.0 12976.0 13560.0 14120.0 14792.0 15408.0 15984.0 16688.0
# CHECK: 12856.0 13520.0 14128.0 14704.0 15408.0 16072.0 16672.0 17392.0
# CHECK: 13312.0 14008.0 14664.0 15264.0 15984.0 16672.0 17344.0 18096.0
# CHECK: 13888.0 14592.0 15280.0 15936.0 16688.0 17392.0 18096.0 18912.0
# CHECK: 14416.0 15168.0 15872.0 16544.0 17344.0 18096.0 18800.0 19648.0
# CHECK: 14912.0 15696.0 16432.0 17136.0 17968.0 18752.0 19504.0 20352.0
# CHECK: 15368.0 16192.0 16976.0 17696.0 18544.0 19376.0 20160.0 21056.0
# CHECK: 15808.0 16640.0 17456.0 18224.0 19104.0 19952.0 20768.0 21696.0
# CHECK: 16344.0 17216.0 18048.0 18848.0 19776.0 20640.0 21472.0 22448.0
# CHECK: 16832.0 17760.0 18608.0 19424.0 20384.0 21312.0 22176.0 23168.0
# CHECK: 17296.0 18240.0 19152.0 20000.0 20976.0 21920.0 22832.0 23856.0
# CHECK: 17856.0 18832.0 19776.0 20656.0 21680.0 22640.0 23584.0 24672.0
# CHECK: 18400.0 19392.0 20352.0 21280.0 22336.0 23344.0 24288.0 25408.0
# CHECK: 18880.0 19936.0 20928.0 21856.0 22960.0 24000.0 24992.0 26112.0
# CHECK: 19344.0 20416.0 21456.0 22432.0 23536.0 24624.0 25648.0 26816.0
# CHECK: 19792.0 20880.0 21936.0 22960.0 24096.0 25184.0 26256.0 27456.0
# CHECK: 20320.0 21456.0 22528.0 23568.0 24768.0 25888.0 26960.0 28224.0
# CHECK: 20816.0 21984.0 23088.0 24144.0 25376.0 26560.0 27664.0 28928.0
# CHECK: 21280.0 22480.0 23632.0 24720.0 25952.0 27168.0 28336.0 29616.0
# CHECK: 21840.0 23072.0 24256.0 25376.0 26672.0 27888.0 29072.0 30432.0
# CHECK: 22368.0 23632.0 24832.0 25984.0 27328.0 28592.0 29792.0 31168.0
# CHECK: 22864.0 24160.0 25408.0 26576.0 27936.0 29248.0 30480.0 31872.0
# CHECK: 23328.0 24656.0 25936.0 27136.0 28512.0 29856.0 31136.0 32576.0
# CHECK: 23760.0 25120.0 26416.0 27680.0 29088.0 30432.0 31744.0 33216.0
# CHECK: 24304.0 25680.0 27008.0 28288.0 29760.0 31136.0 32448.0 33984.0
# CHECK: 24784.0 26224.0 27568.0 28864.0 30368.0 31808.0 33152.0 34688.0
# CHECK: 25248.0 26704.0 28112.0 29440.0 30944.0 32416.0 33824.0 35392.0
# CHECK: 25824.0 27296.0 28736.0 30096.0 31648.0 33120.0 34560.0 36192.0
# CHECK: 26352.0 27872.0 29312.0 30720.0 32304.0 33824.0 35264.0 36928.0
# CHECK: 26848.0 28400.0 29888.0 31296.0 32928.0 34496.0 35968.0 37632.0
# CHECK: 27296.0 28896.0 30416.0 31872.0 33504.0 35104.0 36640.0 38336.0
# CHECK: 27744.0 29344.0 30912.0 32400.0 34080.0 35680.0 37248.0 39008.0
# CHECK: 28144.0 29792.0 31360.0 32896.0 34592.0 36224.0 37792.0 39584.0
# CHECK: 28512.0 30192.0 31792.0 33344.0 35072.0 36768.0 38368.0 40160.0
# CHECK: 29120.0 30816.0 32464.0 34016.0 35808.0 37536.0 39168.0 41024.0
# CHECK: 29680.0 31424.0 33088.0 34688.0 36512.0 38240.0 39904.0 41792.0
# CHECK: 30208.0 31984.0 33664.0 35296.0 37152.0 38944.0 40640.0 42560.0
# CHECK: 30704.0 32512.0 34240.0 35904.0 37760.0 39584.0 41312.0 43264.0
# CHECK: 31168.0 33024.0 34784.0 36480.0 38368.0 40192.0 41984.0 43968.0
# CHECK: 31616.0 33472.0 35264.0 36992.0 38944.0 40800.0 42592.0 44608.0
# CHECK: 32016.0 33920.0 35744.0 37504.0 39456.0 41344.0 43168.0 45216.0
# CHECK: 32640.0 34560.0 36448.0 38208.0 40224.0 42176.0 44032.0 46112.0
# CHECK: 33248.0 35200.0 37088.0 38912.0 40960.0 42944.0 44832.0 46944.0
# CHECK: 33792.0 35808.0 37728.0 39552.0 41632.0 43648.0 45568.0 47744.0
# CHECK: 34304.0 36352.0 38336.0 40192.0 42304.0 44320.0 46272.0 48480.0
# CHECK: 34816.0 36864.0 38880.0 40768.0 42912.0 44992.0 46976.0 49216.0
# CHECK: 35296.0 37376.0 39392.0 41344.0 43520.0 45600.0 47616.0 49888.0
# CHECK: 35712.0 37856.0 39904.0 41856.0 44064.0 46208.0 48256.0 50528.0
# CHECK: 36128.0 38272.0 40352.0 42368.0 44576.0 46752.0 48832.0 51136.0
# CHECK: 36512.0 38688.0 40800.0 42816.0 45088.0 47264.0 49376.0 51712.0
# CHECK: 37120.0 39328.0 41440.0 43488.0 45824.0 48032.0 50176.0 52576.0
# CHECK: 37664.0 39936.0 42080.0 44160.0 46496.0 48768.0 50912.0 53344.0
# CHECK: 38176.0 40480.0 42688.0 44768.0 47136.0 49440.0 51648.0 54112.0
# CHECK: 38688.0 40992.0 43232.0 45376.0 47776.0 50080.0 52320.0 54848.0
# CHECK: 39168.0 41504.0 43776.0 45952.0 48384.0 50720.0 52992.0 55520.0
# CHECK: 39584.0 41984.0 44256.0 46464.0 48928.0 51328.0 53600.0 56160.0
# CHECK: 40000.0 42400.0 44736.0 46976.0 49440.0 51872.0 54208.0 56768.0
# CHECK: 40640.0 43072.0 45440.0 47680.0 50240.0 52672.0 55040.0 57664.0
# CHECK: 41216.0 43712.0 46080.0 48384.0 50976.0 53440.0 55808.0 58496.0
# CHECK: 41792.0 44288.0 46720.0 49024.0 51648.0 54176.0 56576.0 59264.0
def wgmma_e4m3_e4m3_f16_64x8x32(ctx: DeviceContext):
    print("== wgmma_e4m3_e4m3_f16_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float16,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f16_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e4m3fn,
        b_type = DType.float8_e4m3fn,
    ]
    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e5m2_e5m2_f16_64x8x32
# CHECK: 10496.0 10968.0 11424.0 11864.0 12264.0 12640.0 13240.0 13824.0
# CHECK: 10968.0 11520.0 11992.0 12448.0 12888.0 13288.0 13920.0 14520.0
# CHECK: 11424.0 11992.0 12544.0 13016.0 13464.0 13912.0 14568.0 15200.0
# CHECK: 11864.0 12448.0 13016.0 13568.0 14032.0 14480.0 15184.0 15832.0
# CHECK: 12264.0 12888.0 13464.0 14032.0 14584.0 15040.0 15744.0 16448.0
# CHECK: 12640.0 13288.0 13912.0 14480.0 15040.0 15592.0 16304.0 17008.0
# CHECK: 13240.0 13920.0 14568.0 15184.0 15744.0 16304.0 17168.0 17872.0
# CHECK: 13824.0 14520.0 15200.0 15832.0 16448.0 17008.0 17872.0 18736.0
# CHECK: 14368.0 15104.0 15792.0 16464.0 17088.0 17696.0 18560.0 19424.0
# CHECK: 14848.0 15648.0 16384.0 17056.0 17712.0 18336.0 19248.0 20128.0
# CHECK: 15320.0 16128.0 16928.0 17648.0 18304.0 18960.0 19904.0 20800.0
# CHECK: 15752.0 16608.0 17408.0 18176.0 18896.0 19552.0 20512.0 21440.0
# CHECK: 16144.0 17024.0 17872.0 18656.0 19424.0 20128.0 21088.0 22048.0
# CHECK: 16784.0 17680.0 18560.0 19376.0 20160.0 20912.0 21984.0 22944.0
# CHECK: 17392.0 18320.0 19200.0 20064.0 20880.0 21648.0 22768.0 23840.0
# CHECK: 17968.0 18928.0 19840.0 20720.0 21568.0 22368.0 23488.0 24608.0
# CHECK: 18496.0 19504.0 20448.0 21344.0 22208.0 23040.0 24192.0 25312.0
# CHECK: 18992.0 20032.0 21024.0 21952.0 22832.0 23680.0 24864.0 26016.0
# CHECK: 19440.0 20528.0 21536.0 22528.0 23440.0 24304.0 25520.0 26688.0
# CHECK: 19856.0 20976.0 22048.0 23040.0 24016.0 24912.0 26144.0 27344.0
# CHECK: 20256.0 21392.0 22496.0 23536.0 24528.0 25472.0 26736.0 27936.0
# CHECK: 20624.0 21792.0 22912.0 23984.0 25024.0 25984.0 27296.0 28528.0
# CHECK: 21248.0 22416.0 23552.0 24656.0 25728.0 26720.0 28128.0 29408.0
# CHECK: 21824.0 23040.0 24192.0 25296.0 26368.0 27424.0 28848.0 30208.0
# CHECK: 22336.0 23616.0 24816.0 25936.0 27024.0 28064.0 29536.0 30944.0
# CHECK: 22848.0 24128.0 25392.0 26560.0 27648.0 28720.0 30192.0 31632.0
# CHECK: 23328.0 24640.0 25904.0 27136.0 28288.0 29344.0 30832.0 32288.0
# CHECK: 23728.0 25120.0 26416.0 27648.0 28848.0 29984.0 31472.0 32928.0
# CHECK: 24128.0 25520.0 26880.0 28144.0 29360.0 30528.0 32080.0 33536.0
# CHECK: 24768.0 26176.0 27552.0 28864.0 30112.0 31296.0 32960.0 34464.0
# CHECK: 25376.0 26816.0 28192.0 29536.0 30832.0 32048.0 33728.0 35328.0
# CHECK: 25952.0 27424.0 28832.0 30176.0 31488.0 32752.0 34432.0 36096.0
# CHECK: 26464.0 28000.0 29440.0 30816.0 32128.0 33408.0 35136.0 36800.0
# CHECK: 26976.0 28512.0 30016.0 31424.0 32768.0 34048.0 35808.0 37504.0
# CHECK: 27424.0 29024.0 30528.0 32000.0 33376.0 34688.0 36448.0 38176.0
# CHECK: 27840.0 29472.0 31040.0 32512.0 33952.0 35296.0 37088.0 38816.0
# CHECK: 28224.0 29888.0 31488.0 33024.0 34464.0 35872.0 37696.0 39456.0
# CHECK: 28608.0 30272.0 31904.0 33472.0 34976.0 36384.0 38272.0 40064.0
# CHECK: 28960.0 30656.0 32288.0 33856.0 35392.0 36864.0 38752.0 40576.0
# CHECK: 29280.0 31008.0 32656.0 34240.0 35776.0 37312.0 39232.0 41056.0
# CHECK: 29568.0 31328.0 33024.0 34624.0 36160.0 37696.0 39648.0 41536.0
# CHECK: 29856.0 31616.0 33344.0 35008.0 36544.0 38080.0 40032.0 41984.0
# CHECK: 30592.0 32416.0 34144.0 35808.0 37440.0 38944.0 41056.0 43008.0
# CHECK: 31328.0 33152.0 34944.0 36608.0 38272.0 39840.0 41952.0 44032.0
# CHECK: 31968.0 33888.0 35680.0 37408.0 39040.0 40640.0 42816.0 44928.0
# CHECK: 32608.0 34528.0 36416.0 38144.0 39808.0 41408.0 43616.0 45760.0
# CHECK: 33216.0 35168.0 37056.0 38848.0 40576.0 42176.0 44384.0 46528.0
# CHECK: 33760.0 35776.0 37696.0 39488.0 41280.0 42944.0 45152.0 47296.0
# CHECK: 34272.0 36320.0 38304.0 40128.0 41920.0 43648.0 45888.0 48064.0
# CHECK: 34784.0 36832.0 38816.0 40768.0 42560.0 44288.0 46592.0 48800.0
# CHECK: 35264.0 37344.0 39360.0 41280.0 43168.0 44928.0 47232.0 49536.0
# CHECK: 35744.0 37824.0 39872.0 41792.0 43712.0 45536.0 47872.0 50176.0
# CHECK: 36128.0 38304.0 40352.0 42304.0 44224.0 46080.0 48512.0 50816.0
# CHECK: 36512.0 38688.0 40800.0 42816.0 44736.0 46592.0 49024.0 51424.0
# CHECK: 36864.0 39072.0 41184.0 43264.0 45184.0 47072.0 49504.0 51904.0
# CHECK: 37184.0 39424.0 41568.0 43648.0 45632.0 47552.0 49984.0 52384.0
# CHECK: 37440.0 39744.0 41920.0 44032.0 46016.0 48000.0 50464.0 52864.0
# CHECK: 38240.0 40512.0 42752.0 44896.0 46912.0 48896.0 51552.0 53952.0
# CHECK: 39040.0 41312.0 43552.0 45696.0 47808.0 49760.0 52448.0 55040.0
# CHECK: 39712.0 42112.0 44352.0 46496.0 48608.0 50656.0 53312.0 55936.0
# CHECK: 40352.0 42784.0 45120.0 47296.0 49408.0 51456.0 54208.0 56832.0
# CHECK: 40992.0 43424.0 45792.0 48064.0 50176.0 52224.0 54976.0 57664.0
# CHECK: 41568.0 44064.0 46432.0 48736.0 50944.0 52992.0 55744.0 58432.0
# CHECK: 42112.0 44640.0 47072.0 49376.0 51616.0 53760.0 56512.0 59200.0
def wgmma_e5m2_e5m2_f16_64x8x32(ctx: DeviceContext):
    print("== wgmma_e5m2_e5m2_f16_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float16,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f16_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e5m2,
        b_type = DType.float8_e5m2,
    ]

    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e4m3_e5m2_f16_64x8x32
# CHECK: 10464.0 10944.0 11408.0 11848.0 12256.0 12624.0 13232.0 13800.0
# CHECK: 10944.0 11488.0 11968.0 12432.0 12872.0 13280.0 13904.0 14512.0
# CHECK: 11408.0 11968.0 12504.0 12992.0 13448.0 13888.0 14560.0 15184.0
# CHECK: 11848.0 12432.0 12992.0 13528.0 14016.0 14464.0 15160.0 15824.0
# CHECK: 12384.0 13000.0 13576.0 14144.0 14672.0 15152.0 15888.0 16576.0
# CHECK: 12880.0 13536.0 14144.0 14720.0 15280.0 15808.0 16576.0 17312.0
# CHECK: 13328.0 14032.0 14680.0 15288.0 15856.0 16416.0 17216.0 17984.0
# CHECK: 13904.0 14608.0 15304.0 15952.0 16544.0 17120.0 17984.0 18784.0
# CHECK: 14448.0 15184.0 15888.0 16576.0 17216.0 17808.0 18672.0 19536.0
# CHECK: 14928.0 15728.0 16448.0 17152.0 17824.0 18464.0 19360.0 20224.0
# CHECK: 15384.0 16208.0 17008.0 17712.0 18400.0 19072.0 20016.0 20912.0
# CHECK: 15824.0 16656.0 17472.0 18256.0 18976.0 19648.0 20624.0 21552.0
# CHECK: 16360.0 17232.0 18064.0 18864.0 19632.0 20336.0 21344.0 22320.0
# CHECK: 16864.0 17760.0 18624.0 19440.0 20240.0 20992.0 22032.0 23040.0
# CHECK: 17312.0 18272.0 19168.0 20016.0 20816.0 21600.0 22688.0 23712.0
# CHECK: 17888.0 18848.0 19792.0 20672.0 21504.0 22288.0 23440.0 24528.0
# CHECK: 18432.0 19424.0 20368.0 21296.0 22160.0 22976.0 24128.0 25280.0
# CHECK: 18912.0 19968.0 20944.0 21872.0 22784.0 23632.0 24816.0 25952.0
# CHECK: 19360.0 20448.0 21504.0 22448.0 23360.0 24256.0 25472.0 26640.0
# CHECK: 19808.0 20896.0 21968.0 22992.0 23920.0 24832.0 26080.0 27280.0
# CHECK: 20336.0 21472.0 22544.0 23584.0 24608.0 25504.0 26800.0 28048.0
# CHECK: 20832.0 22000.0 23104.0 24160.0 25184.0 26176.0 27488.0 28768.0
# CHECK: 21280.0 22496.0 23648.0 24736.0 25760.0 26768.0 28160.0 29456.0
# CHECK: 21856.0 23072.0 24272.0 25392.0 26464.0 27472.0 28896.0 30272.0
# CHECK: 22416.0 23648.0 24848.0 26016.0 27104.0 28160.0 29600.0 31008.0
# CHECK: 22880.0 24208.0 25424.0 26592.0 27728.0 28800.0 30272.0 31680.0
# CHECK: 23328.0 24672.0 25984.0 27168.0 28320.0 29424.0 30928.0 32368.0
# CHECK: 23776.0 25120.0 26448.0 27712.0 28880.0 30000.0 31536.0 33024.0
# CHECK: 24304.0 25696.0 27024.0 28304.0 29552.0 30688.0 32256.0 33760.0
# CHECK: 24816.0 26224.0 27600.0 28880.0 30144.0 31360.0 32960.0 34496.0
# CHECK: 25264.0 26736.0 28128.0 29456.0 30720.0 31952.0 33632.0 35168.0
# CHECK: 25840.0 27312.0 28752.0 30112.0 31424.0 32640.0 34368.0 36000.0
# CHECK: 26400.0 27888.0 29328.0 30736.0 32064.0 33344.0 35040.0 36736.0
# CHECK: 26864.0 28448.0 29904.0 31312.0 32688.0 33984.0 35744.0 37408.0
# CHECK: 27312.0 28912.0 30464.0 31888.0 33280.0 34592.0 36384.0 38112.0
# CHECK: 27760.0 29360.0 30928.0 32448.0 33824.0 35200.0 36992.0 38752.0
# CHECK: 28160.0 29808.0 31376.0 32896.0 34368.0 35744.0 37568.0 39360.0
# CHECK: 28528.0 30208.0 31808.0 33344.0 34848.0 36288.0 38112.0 39904.0
# CHECK: 29120.0 30832.0 32464.0 34048.0 35552.0 36992.0 38976.0 40768.0
# CHECK: 29712.0 31424.0 33088.0 34688.0 36224.0 37696.0 39680.0 41632.0
# CHECK: 30240.0 32016.0 33696.0 35328.0 36864.0 38368.0 40384.0 42304.0
# CHECK: 30736.0 32544.0 34272.0 35904.0 37504.0 39008.0 41056.0 43008.0
# CHECK: 31200.0 33024.0 34816.0 36512.0 38080.0 39648.0 41696.0 43712.0
# CHECK: 31632.0 33504.0 35296.0 37056.0 38688.0 40256.0 42336.0 44352.0
# CHECK: 32032.0 33920.0 35776.0 37504.0 39200.0 40800.0 42912.0 44928.0
# CHECK: 32656.0 34592.0 36448.0 38240.0 39936.0 41600.0 43776.0 45824.0
# CHECK: 33280.0 35200.0 37088.0 38912.0 40672.0 42336.0 44576.0 46720.0
# CHECK: 33824.0 35840.0 37728.0 39552.0 41344.0 43040.0 45280.0 47488.0
# CHECK: 34368.0 36384.0 38336.0 40192.0 41984.0 43712.0 45984.0 48192.0
# CHECK: 34848.0 36928.0 38912.0 40800.0 42592.0 44352.0 46656.0 48896.0
# CHECK: 35328.0 37408.0 39424.0 41376.0 43200.0 44960.0 47296.0 49568.0
# CHECK: 35712.0 37888.0 39936.0 41888.0 43776.0 45568.0 47936.0 50208.0
# CHECK: 36128.0 38272.0 40384.0 42400.0 44288.0 46144.0 48512.0 50816.0
# CHECK: 36512.0 38688.0 40800.0 42848.0 44800.0 46656.0 49088.0 51424.0
# CHECK: 37120.0 39328.0 41440.0 43520.0 45504.0 47392.0 49920.0 52288.0
# CHECK: 37696.0 39936.0 42080.0 44160.0 46144.0 48096.0 50624.0 53088.0
# CHECK: 38208.0 40512.0 42688.0 44768.0 46784.0 48736.0 51328.0 53824.0
# CHECK: 38720.0 41024.0 43264.0 45408.0 47424.0 49376.0 51968.0 54528.0
# CHECK: 39200.0 41536.0 43776.0 45984.0 48064.0 50016.0 52640.0 55168.0
# CHECK: 39616.0 42016.0 44288.0 46496.0 48640.0 50656.0 53248.0 55808.0
# CHECK: 40000.0 42432.0 44768.0 47008.0 49120.0 51200.0 53856.0 56416.0
# CHECK: 40640.0 43072.0 45440.0 47712.0 49888.0 51968.0 54752.0 57344.0
# CHECK: 41248.0 43712.0 46080.0 48384.0 50624.0 52736.0 55488.0 58240.0
# CHECK: 41824.0 44320.0 46720.0 49024.0 51264.0 53440.0 56256.0 58944.0
def wgmma_e4m3_e5m2_f16_64x8x32(ctx: DeviceContext):
    print("== wgmma_e4m3_e5m2_f16_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float16,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f16_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e4m3fn,
        b_type = DType.float8_e5m2,
    ]

    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e5m2_e4m3_f16_64x8x32
# CHECK: 10464.0 10944.0 11408.0 11848.0 12384.0 12880.0 13328.0 13904.0
# CHECK: 10944.0 11488.0 11968.0 12432.0 13000.0 13536.0 14032.0 14608.0
# CHECK: 11408.0 11968.0 12504.0 12992.0 13576.0 14144.0 14680.0 15304.0
# CHECK: 11848.0 12432.0 12992.0 13528.0 14144.0 14720.0 15288.0 15952.0
# CHECK: 12256.0 12872.0 13448.0 14016.0 14672.0 15280.0 15856.0 16544.0
# CHECK: 12624.0 13280.0 13888.0 14464.0 15152.0 15808.0 16416.0 17120.0
# CHECK: 13232.0 13904.0 14560.0 15160.0 15888.0 16576.0 17216.0 17984.0
# CHECK: 13800.0 14512.0 15184.0 15824.0 16576.0 17312.0 17984.0 18784.0
# CHECK: 14328.0 15080.0 15784.0 16448.0 17248.0 18000.0 18720.0 19536.0
# CHECK: 14824.0 15608.0 16352.0 17056.0 17856.0 18656.0 19392.0 20256.0
# CHECK: 15296.0 16104.0 16880.0 17616.0 18464.0 19264.0 20048.0 20944.0
# CHECK: 15736.0 16576.0 17376.0 18144.0 19024.0 19872.0 20656.0 21600.0
# CHECK: 16144.0 17024.0 17840.0 18624.0 19552.0 20416.0 21248.0 22192.0
# CHECK: 16768.0 17680.0 18544.0 19360.0 20320.0 21216.0 22080.0 23088.0
# CHECK: 17376.0 18304.0 19200.0 20048.0 21056.0 22000.0 22896.0 23936.0
# CHECK: 17920.0 18912.0 19840.0 20704.0 21744.0 22720.0 23664.0 24736.0
# CHECK: 18448.0 19456.0 20432.0 21344.0 22384.0 23408.0 24368.0 25488.0
# CHECK: 18944.0 19984.0 20992.0 21936.0 23008.0 24048.0 25056.0 26192.0
# CHECK: 19424.0 20480.0 21504.0 22496.0 23616.0 24672.0 25696.0 26880.0
# CHECK: 19840.0 20960.0 22016.0 23008.0 24160.0 25280.0 26336.0 27520.0
# CHECK: 20240.0 21376.0 22480.0 23504.0 24672.0 25824.0 26912.0 28128.0
# CHECK: 20624.0 21776.0 22896.0 23968.0 25168.0 26320.0 27440.0 28704.0
# CHECK: 21232.0 22416.0 23552.0 24656.0 25920.0 27104.0 28240.0 29568.0
# CHECK: 21792.0 23024.0 24192.0 25296.0 26592.0 27840.0 29008.0 30336.0
# CHECK: 22304.0 23584.0 24800.0 25936.0 27232.0 28512.0 29744.0 31104.0
# CHECK: 22816.0 24096.0 25344.0 26528.0 27872.0 29152.0 30416.0 31840.0
# CHECK: 23296.0 24608.0 25872.0 27088.0 28480.0 29792.0 31056.0 32512.0
# CHECK: 23728.0 25088.0 26384.0 27616.0 29040.0 30400.0 31696.0 33152.0
# CHECK: 24128.0 25520.0 26848.0 28112.0 29552.0 30944.0 32272.0 33760.0
# CHECK: 24768.0 26176.0 27536.0 28848.0 30336.0 31744.0 33088.0 34656.0
# CHECK: 25360.0 26816.0 28192.0 29536.0 31072.0 32528.0 33920.0 35488.0
# CHECK: 25920.0 27408.0 28832.0 30176.0 31744.0 33248.0 34688.0 36288.0
# CHECK: 26432.0 27968.0 29424.0 30816.0 32384.0 33920.0 35392.0 37056.0
# CHECK: 26944.0 28480.0 29984.0 31408.0 33024.0 34560.0 36064.0 37760.0
# CHECK: 27408.0 28992.0 30496.0 31968.0 33600.0 35200.0 36704.0 38432.0
# CHECK: 27840.0 29456.0 31008.0 32480.0 34176.0 35776.0 37344.0 39072.0
# CHECK: 28224.0 29888.0 31472.0 32992.0 34688.0 36352.0 37952.0 39712.0
# CHECK: 28608.0 30272.0 31904.0 33472.0 35200.0 36864.0 38496.0 40320.0
# CHECK: 28944.0 30656.0 32288.0 33856.0 35648.0 37344.0 38976.0 40800.0
# CHECK: 29248.0 30992.0 32656.0 34240.0 36032.0 37792.0 39424.0 41280.0
# CHECK: 29552.0 31296.0 32992.0 34624.0 36416.0 38208.0 39872.0 41760.0
# CHECK: 29824.0 31600.0 33312.0 34976.0 36800.0 38592.0 40288.0 42208.0
# CHECK: 30592.0 32384.0 34112.0 35776.0 37728.0 39520.0 41248.0 43264.0
# CHECK: 31296.0 33152.0 34912.0 36608.0 38560.0 40448.0 42208.0 44224.0
# CHECK: 31936.0 33856.0 35680.0 37376.0 39360.0 41280.0 43136.0 45184.0
# CHECK: 32576.0 34496.0 36352.0 38144.0 40128.0 42048.0 43904.0 46048.0
# CHECK: 33184.0 35136.0 36992.0 38816.0 40864.0 42816.0 44672.0 46816.0
# CHECK: 33760.0 35744.0 37632.0 39488.0 41568.0 43552.0 45440.0 47616.0
# CHECK: 34272.0 36320.0 38272.0 40096.0 42208.0 44256.0 46208.0 48384.0
# CHECK: 34784.0 36832.0 38816.0 40704.0 42848.0 44896.0 46880.0 49120.0
# CHECK: 35264.0 37344.0 39360.0 41280.0 43456.0 45536.0 47552.0 49792.0
# CHECK: 35712.0 37824.0 39840.0 41792.0 44032.0 46144.0 48160.0 50432.0
# CHECK: 36096.0 38272.0 40320.0 42304.0 44544.0 46720.0 48768.0 51072.0
# CHECK: 36480.0 38656.0 40768.0 42816.0 45056.0 47232.0 49344.0 51712.0
# CHECK: 36832.0 39040.0 41152.0 43232.0 45504.0 47712.0 49824.0 52224.0
# CHECK: 37184.0 39392.0 41536.0 43616.0 45952.0 48160.0 50304.0 52704.0
# CHECK: 37440.0 39744.0 41920.0 43968.0 46336.0 48576.0 50752.0 53184.0
# CHECK: 38240.0 40512.0 42752.0 44864.0 47264.0 49536.0 51776.0 54272.0
# CHECK: 38976.0 41312.0 43552.0 45696.0 48160.0 50496.0 52736.0 55264.0
# CHECK: 39680.0 42048.0 44352.0 46496.0 48992.0 51392.0 53696.0 56224.0
# CHECK: 40320.0 42752.0 45056.0 47296.0 49792.0 52224.0 54560.0 57184.0
# CHECK: 40960.0 43392.0 45760.0 48000.0 50560.0 52992.0 55360.0 58016.0
# CHECK: 41536.0 44032.0 46400.0 48704.0 51264.0 53760.0 56128.0 58816.0
# CHECK: 42112.0 44608.0 47040.0 49344.0 51968.0 54464.0 56896.0 59584.0
def wgmma_e5m2_e4m3_f16_64x8x32(ctx: DeviceContext):
    print("== wgmma_e5m2_e4m3_f16_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float16,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f16_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e5m2,
        b_type = DType.float8_e4m3fn,
    ]

    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


def main():
    with DeviceContext() as ctx:
        wgmma_e4m3_e4m3_f32_64x8x32(ctx)
        wgmma_e5m2_e5m2_f32_64x8x32(ctx)
        wgmma_e4m3_e5m2_f32_64x8x32(ctx)
        wgmma_e5m2_e4m3_f32_64x8x32(ctx)
        wgmma_e4m3_e4m3_f16_64x8x32(ctx)
        wgmma_e5m2_e5m2_f16_64x8x32(ctx)
        wgmma_e4m3_e5m2_f16_64x8x32(ctx)
        wgmma_e5m2_e4m3_f16_64x8x32(ctx)
