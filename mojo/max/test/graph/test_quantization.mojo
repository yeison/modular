# ===----------------------------------------------------------------------=== #
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

# RUN: mojo "%s"

import sys
from collections import InlineArray
from random import randn
from sys import sizeof

from max.graph import Graph, TensorType, Type
from max.graph._testing import (
    assert_tensors_almost_equal,
    assert_tensors_equal,
    execute_unary,
)
from max.graph.quantization import (
    BFloat16Encoding,
    BlockQ4K,
    BlockQ5K,
    BlockQ6K,
    BlockQ40,
    Q4_0Encoding,
    Q4_KEncoding,
    Q5_KEncoding,
    Q6_KEncoding,
)
from max.graph.quantization.encodings import _find_extrema
from max.tensor import Tensor, TensorShape
from memory import memset_zero
from testing import assert_equal


def test_quantize_bfloat16():
    # TODO(KERN-228): Enable for neon too once LLVM bfloat16 emulation matures on ARM.
    @parameter
    if not sys.has_neon():
        # Define hyperparameters.
        alias num_tokens = 32
        alias channels = 16

        # Initialize graph.
        g = Graph(
            "bfloat16_quantization",
            in_types=List[Type](
                TensorType(DType.uint8, num_tokens, 2 * channels)
            ),
            out_types=List[Type](
                TensorType(DType.uint8, num_tokens, 2 * channels)
            ),
        )

        # Generate normally-distributed random tensor to quantize.
        f32_tensor = Tensor[DType.float32](TensorShape(num_tokens, channels))
        randn(f32_tensor.unsafe_ptr(), f32_tensor.num_elements())

        # Quantize the float32 token embeddings to bfloat16.
        bfloat16_symbol = g.quantize[BFloat16Encoding](f32_tensor)

        # Add zeros with the weights as a "pseudo identity" op.
        # Otherwise the API/runtime returns an invalid stack address as output.
        # TODO(GEX-498): Fix issue returning mgp.buffer.constant directly.
        g.output(bfloat16_symbol + g[0])

        zeros = Tensor[DType.uint8](TensorShape(num_tokens, 2 * channels))
        memset_zero(zeros.unsafe_ptr(), zeros.num_elements())

        output = execute_unary(g, zeros)

        # Bitcast output to bfloat16 then cast to float32 to verify.
        output = Tensor(
            TensorShape(num_tokens, channels),
            output._steal_ptr().bitcast[BFloat16](),
        ).astype[DType.float32]()

        # Set rtol based on bfloat16 resolution: 0.01.
        assert_tensors_almost_equal(f32_tensor, output, rtol=1e-2)


def test_quantize_q4_0():
    # Define hyperparameters.
    alias num_rows = 1

    # Initialize graph.
    g = Graph(
        "q4_0_quantization",
        in_types=List[Type](
            TensorType(DType.uint8, num_rows, 2 * sizeof[BlockQ40]())
        ),
        out_types=List[Type](
            TensorType(DType.uint8, num_rows, 2 * sizeof[BlockQ40]())
        ),
    )

    # fmt: off
    f32_tensor = Tensor[DType.float32](
        # Two blocks of row 0 of Llama's `token_embd` parameter.
        TensorShape(num_rows, 2 * BlockQ40.elements_per_block()),
        # Block 0 float32 weights.
        1.2293457984924316e-06, -1.8179416656494141e-06, -4.3511390686035156e-06, 8.0466270446777344e-06,
        1.9222497940063477e-06, -5.6028366088867188e-06, 3.0845403671264648e-06, 1.1995434761047363e-06,
        -6.8247318267822266e-06, -1.6763806343078613e-06, -4.4703483581542969e-06, -4.3809413909912109e-06,
        -7.3388218879699707e-07, -8.4638595581054688e-06, 2.1457672119140625e-06, 1.0251998901367188e-05,
        -4.8801302909851074e-07, -1.5273690223693848e-06, 1.6242265701293945e-06, 1.1324882507324219e-06,
        2.8759241104125977e-06, 9.4771385192871094e-06, 3.3080577850341797e-06, -2.8014183044433594e-06,
        -1.2874603271484375e-05, -2.816319465637207e-06, 5.6326389312744141e-06, -1.1175870895385742e-06,
        -3.3676624298095703e-06, -3.0100345611572266e-06, -2.1886080503463745e-07, 1.4156103134155273e-06,
        # Block 1 float32 weights.
        9.1791152954101562e-06, 2.5779008865356445e-06, 1.9669532775878906e-06, 9.8347663879394531e-07,
        -1.1086463928222656e-05, -5.7220458984375e-06, 3.9637088775634766e-06, -1.1026859283447266e-05,
        7.2121620178222656e-06, 1.8477439880371094e-06, 4.5895576477050781e-06, 2.1904706954956055e-06,
        1.3113021850585938e-06, -2.86102294921875e-06, -1.4841556549072266e-05, -6.4671039581298828e-06,
        2.6524066925048828e-06, 6.7055225372314453e-06, -2.6524066925048828e-06, 8.3446502685546875e-06,
        1.5869736671447754e-06, -1.3053417205810547e-05, 4.6193599700927734e-06, 7.8082084655761719e-06,
        -5.5730342864990234e-06, -6.3180923461914062e-06, -3.7811696529388428e-07, 9.2387199401855469e-06,
        5.3644180297851562e-06, -3.9637088775634766e-06, -2.4437904357910156e-06, -4.6193599700927734e-06,
    )
    expected_output = Tensor[DType.uint8](
        TensorShape(num_rows, 2 * sizeof[BlockQ40]()),
        # Block 0 float16 scale.
        27, 0,
        # Block 0 packed nibble weights.
        137, 119, 149, 157, 169, 229, 170, 105, 4, 103, 197, 117, 104, 99, 137, 158,
        # Block 1 float16 scale.
        31, 0,
        # Block 1 packed nibble weights.
        157, 201, 121, 201, 146, 21, 170, 194, 92, 89, 138, 217, 185, 102, 112, 101,
    )
    # fmt: on

    # Quantize the float32 token embeddings to q4_0.
    q4_0_symbol = g.quantize[Q4_0Encoding](f32_tensor)

    # TODO(GEX-498): Remove pseudo-identity once GEX-498 is fixed.
    g.output(q4_0_symbol + g[0])

    zeros = Tensor[DType.uint8](TensorShape(num_rows, 2 * sizeof[BlockQ40]()))
    memset_zero(zeros.unsafe_ptr(), zeros.num_elements())

    actual_output = execute_unary[outtype = DType.uint8](g, zeros)

    assert_tensors_equal(actual_output, expected_output)


def test_find_extrema():
    array3 = InlineArray[Float32, 4](1.5, -3.2, 0.6, 1.0)
    min, max = _find_extrema[array3.size](array3.unsafe_ptr())
    assert_equal(min, -3.2)
    assert_equal(max, 1.5)

    # fmt: off
    array16 = InlineArray[Float32, 16](
        0.14953765, -0.92757577,  1.6711988 , -0.3135307 , -0.05910513,
       -0.1636202 , -1.4872252 ,  0.50984716, -0.9360155 , -1.1835905 ,
       -1.8056588 ,  1.4484313 , -0.3265503 , -0.80376166, -1.0191611 ,
       -0.71436137
    )
    # fmt: on
    min, max = _find_extrema[array16.size](array16.unsafe_ptr())
    assert_equal(min, -1.8056588)
    assert_equal(max, 1.6711988)


def test_quantize_q4_k():
    # Define hyperparameters.
    alias num_rows = 1

    # Initialize graph.
    g = Graph(
        "q4_k_quantization",
        in_types=List[Type](
            TensorType(DType.uint8, num_rows, 2 * sizeof[BlockQ4K]())
        ),
        out_types=List[Type](
            TensorType(DType.uint8, num_rows, 2 * sizeof[BlockQ4K]())
        ),
    )

    # N.B.: Generating test data: Start with an FP32 Llama3 GGUF model.  Use
    # llama.cpp's llama-quantize tool to convert it to the desired quantized
    # format.  Use the Python gguf.gguf_reader.GGUFReader class (defined in
    # gguf-py subdirectory in llama.cpp repository) on the original and the
    # quantized version.  Pull out the "token_embd.weight" tensor.  Use
    # "tensor.data.flat[:128]" or such to pull out prefixes of the weights.
    # Reformat for insertion into Mojo test as desired.

    # fmt: off
    f32_tensor = Tensor[DType.float32](
        # Two blocks of row 0 of Llama3's `token_embd` parameter.
        # (https://huggingface.co/brendanduke/Llama-3-8B-f32.gguf)
        TensorShape(num_rows, 2 * BlockQ4K.elements_per_superblock()),
        # Superblock 0 subblock 0 float32 weights.
         1.37329102e-03,  5.09643555e-03, -3.03649902e-03, -1.81198120e-05,
        -2.91442871e-03, -3.14712524e-04, -7.55310059e-04, -2.18200684e-03,
        -2.77709961e-03, -3.66210938e-03, -7.82012939e-04, -8.85009766e-03,
         2.33459473e-03,  1.07574463e-03, -8.05664062e-03,  8.60595703e-03,
        -5.64575195e-03, -4.21142578e-03, -5.85937500e-03, -4.74929810e-04,
         1.57165527e-03,  1.13525391e-02, -2.22778320e-03, -2.59399414e-03,
        -2.44140625e-03, -1.25885010e-03,  1.85012817e-04,  4.39453125e-03,
         4.88281250e-03,  1.73950195e-03, -7.75146484e-03, -1.44195557e-03,
        # Superblock 0 subblock 1 float32 weights.
        -8.85009766e-03, -7.93457031e-03, -3.68118286e-04,  1.34277344e-03,
        -1.01318359e-02, -4.82177734e-03, -1.37939453e-02,  2.07519531e-03,
         8.11767578e-03, -6.86645508e-03,  1.57165527e-03,  2.38037109e-03,
         2.86865234e-03, -3.26538086e-03, -1.19018555e-03,  2.79541016e-02,
         1.61743164e-03,  1.35040283e-03,  3.90625000e-03,  2.18200684e-03,
        -3.43322754e-03, -3.78417969e-03, -3.33786011e-04,  4.99725342e-04,
        -4.88281250e-03, -4.39453125e-03, -1.40991211e-02,  2.27355957e-03,
        -7.05718994e-04, -4.21142578e-03,  6.22558594e-03, -8.91113281e-03,
        # Superblock 0 subblock 2 float32 weights.
        -1.86157227e-03,  1.41906738e-03, -4.99725342e-04, -7.53784180e-03,
        -5.73730469e-03, -6.19506836e-03,  2.47192383e-03, -3.34167480e-03,
         3.40270996e-03,  4.05883789e-03, -1.60980225e-03,  1.80816650e-03,
         7.40051270e-04, -5.27954102e-03, -6.62231445e-03,  1.22070312e-02,
        -1.77001953e-03, -3.89099121e-03, -4.02832031e-03, -7.01904297e-03,
        -1.62506104e-03, -3.09753418e-03,  7.29370117e-03,  1.68457031e-02,
        -7.05718994e-04, -1.26953125e-02, -4.30297852e-03, -4.10079956e-04,
        -1.66320801e-03,  1.26342773e-02,  5.03540039e-03,  1.25885010e-03,
        # Superblock 0 subblock 3 float32 weights.
        -9.58251953e-03, -1.94549561e-03,  2.34985352e-03,  9.15527344e-03,
         3.50952148e-03, -1.63269043e-03, -1.77001953e-03,  5.76782227e-03,
         6.10351562e-03, -3.90625000e-03,  7.38525391e-03, -7.93457031e-04,
        -1.13677979e-03, -1.73950195e-03, -3.11279297e-03, -5.24902344e-03,
        -3.67736816e-03,  5.88989258e-03,  3.29589844e-03,  1.54876709e-03,
         1.58691406e-03,  3.79943848e-03,  9.44137573e-05, -4.42504883e-03,
         4.66918945e-03,  3.11279297e-03,  3.93676758e-03, -2.25830078e-03,
        -3.93676758e-03,  1.05590820e-02, -3.63159180e-03,  3.31115723e-03,
        # Superblock 0 subblock 4 float32 weights.
         2.01416016e-03,  3.46374512e-03,  5.88989258e-03, -2.27355957e-03,
        -1.83105469e-03, -8.23974609e-03, -2.91442871e-03, -3.32641602e-03,
         1.43432617e-03,  3.69262695e-03, -4.05883789e-03,  1.20239258e-02,
        -2.87294388e-05,  9.53674316e-04, -9.82666016e-03, -5.34057617e-04,
        -2.36511230e-03,  9.11712646e-04, -1.73339844e-02,  6.43920898e-03,
        -6.19506836e-03,  6.59179688e-03,  6.86645508e-03,  5.92041016e-03,
         1.00097656e-02, -4.08935547e-03, -2.36511230e-03,  2.39562988e-03,
        -6.77490234e-03, -2.36511230e-03,  2.25830078e-03, -2.56347656e-03,
        # Superblock 0 subblock 5 float32 weights.
         9.76562500e-03,  1.80053711e-03, -4.48226929e-04, -5.92041016e-03,
         7.65991211e-03,  3.96728516e-03,  1.18408203e-02, -2.96020508e-03,
        -3.64303589e-04,  5.06591797e-03,  1.48010254e-03,  2.39562988e-03,
        -9.11712646e-04,  7.72094727e-03,  6.56127930e-03,  5.31005859e-03,
        -2.76184082e-03, -1.46484375e-03, -6.19506836e-03,  1.60217285e-03,
         6.59179688e-03,  3.55529785e-03, -3.37219238e-03, -7.62939453e-03,
        -1.67236328e-02,  9.64355469e-03, -1.76239014e-03, -2.92968750e-03,
        -5.76782227e-03, -1.48773193e-03,  8.72802734e-03,  1.56402588e-03,
        # Superblock 0 subblock 6 float32 weights.
        -3.38745117e-03,  5.88989258e-03, -1.00708008e-02, -4.88281250e-03,
         1.31835938e-02,  3.90625000e-03, -2.19726562e-03,  4.76074219e-03,
         1.37329102e-03, -1.22070312e-03,  8.66699219e-03, -4.45556641e-03,
         8.54492188e-03, -1.19018555e-03, -9.48905945e-05,  7.26318359e-03,
        -5.70678711e-03, -1.19781494e-03, -7.56835938e-03, -3.43322754e-03,
         4.57763672e-03,  3.15856934e-03, -3.28063965e-04,  1.43432617e-03,
         4.73022461e-03,  7.93457031e-03, -1.78527832e-03,  1.57928467e-03,
        -5.09643555e-03,  8.48388672e-03,  6.82830811e-04,  1.43432617e-02,
        # Superblock 0 subblock 7 float32 weights.
        -5.34057617e-03,  7.75146484e-03, -3.96728516e-03, -5.53131104e-04,
         2.07519531e-03,  1.94549561e-03, -8.23974609e-04, -1.71661377e-04,
        -7.99560547e-03, -5.73730469e-03, -2.89916992e-03, -2.92968750e-03,
         1.07421875e-02, -3.79943848e-03, -2.55584717e-04,  1.69677734e-02,
         6.59179688e-03, -6.50024414e-03, -2.05993652e-04, -3.26538086e-03,
        -5.46264648e-03,  6.68334961e-03,  2.07519531e-03,  1.39160156e-02,
        -3.55529785e-03, -5.22613525e-04,  5.67626953e-03, -1.55639648e-03,
        -9.38415527e-04, -1.73568726e-04, -4.51660156e-03,  8.69750977e-04,
        # Superblock 1 subblock 0 float32 weights.
         5.11169434e-04, -5.40161133e-03, -9.64355469e-03,  3.82995605e-03,
        -3.23486328e-03,  5.12695312e-03, -1.57928467e-03, -1.82342529e-03,
        -3.35693359e-03, -1.95312500e-03,  5.92041016e-03, -3.11279297e-03,
         3.29589844e-03,  1.52587891e-02, -1.92871094e-02, -7.93457031e-03,
        -9.23156738e-04, -6.80541992e-03, -4.91333008e-03,  2.12097168e-03,
         9.70458984e-03, -7.47680664e-03, -8.20159912e-04,  4.97436523e-03,
        -3.15856934e-03,  1.19628906e-02, -1.41143799e-03, -3.46374512e-03,
         7.87353516e-03, -2.41088867e-03,  1.11083984e-02,  5.24902344e-03,
        # Superblock 1 subblock 1 float32 weights.
         7.23266602e-03, -8.91113281e-03, -3.69262695e-03,  3.05175781e-03,
        -1.50146484e-02,  9.09423828e-03, -1.33056641e-02, -7.65991211e-03,
         1.87683105e-03, -5.24902344e-03, -7.75146484e-03, -1.20849609e-02,
         6.10351562e-03, -1.42822266e-02,  6.86645508e-03,  4.48608398e-03,
         5.92041016e-03, -3.47900391e-03, -2.21252441e-03, -3.86047363e-03,
         2.31933594e-03, -7.55310059e-04, -6.01196289e-03, -5.64575195e-03,
         6.06536865e-04,  6.34765625e-03,  1.46484375e-03, -7.01904297e-03,
         6.22558594e-03,  7.26318359e-03,  1.06811523e-02, -2.79235840e-03,
        # Superblock 1 subblock 2 float32 weights.
        -3.05175781e-04,  7.81250000e-03,  1.27563477e-02,  7.66754150e-04,
        -3.11279297e-03,  5.03540039e-03,  3.67736816e-03, -4.79125977e-03,
        -2.06947327e-04,  4.24194336e-03,  1.03759766e-02, -4.55379486e-05,
         6.40869141e-03,  5.43212891e-03,  3.32641602e-03,  2.47192383e-03,
        -3.96728516e-03,  2.67028809e-03,  4.21142578e-03,  7.89642334e-04,
         1.80053711e-03,  1.10473633e-02, -8.27789307e-04, -3.28063965e-03,
         3.03649902e-03, -4.05883789e-03, -8.05664062e-03,  8.11767578e-03,
         5.61523438e-03, -1.02539062e-02, -1.61743164e-03,  9.21630859e-03,
        # Superblock 1 subblock 3 float32 weights.
         8.66699219e-03,  6.89697266e-03, -7.20214844e-03,  6.43920898e-03,
         2.80380249e-04, -2.99453735e-04,  9.94873047e-03,  3.02124023e-03,
         3.35693359e-03, -3.06701660e-03,  5.03540039e-04, -9.82666016e-03,
        -1.00708008e-03, -1.54876709e-03,  5.79833984e-03, -1.02996826e-03,
         7.23266602e-03, -3.57055664e-03, -2.57873535e-03, -2.68554688e-03,
         1.35040283e-03, -3.55529785e-03, -5.06591797e-03, -3.20434570e-03,
        -6.04248047e-03, -3.82995605e-03,  2.79235840e-03, -9.46044922e-03,
        -7.32421875e-03, -5.91278076e-04, -3.18908691e-03, -2.57492065e-04,
        # Superblock 1 subblock 4 float32 weights.
         6.37817383e-03, -7.20977783e-04,  1.15966797e-02,  6.19506836e-03,
        -7.38525391e-03,  1.21307373e-03, -1.02539062e-02,  5.82885742e-03,
         4.85229492e-03,  4.63867188e-03, -1.53350830e-03, -3.38745117e-03,
         6.50024414e-03,  4.05883789e-03, -5.00488281e-03,  3.05175781e-03,
         1.19018555e-02, -1.18255615e-03,  1.45874023e-02, -1.72615051e-04,
         3.17382812e-03,  2.41088867e-03, -1.25122070e-02,  4.57763672e-03,
         2.51770020e-03, -4.94384766e-03, -1.17187500e-02, -4.24194336e-03,
         5.30242920e-04, -1.10473633e-02,  9.53674316e-04, -2.77709961e-03,
        # Superblock 1 subblock 5 float32 weights.
         6.25610352e-03, -1.50756836e-02,  2.55584717e-04,  6.19506836e-03,
         6.01196289e-03, -1.36566162e-03,  3.76892090e-03, -2.04467773e-03,
        -2.45666504e-03,  1.86157227e-03, -4.39453125e-03, -1.64031982e-03,
        -2.45666504e-03,  3.20434570e-03, -6.25610352e-03,  3.29589844e-03,
         2.62451172e-03,  6.02722168e-04, -6.19506836e-03,  8.23974609e-03,
         9.82666016e-03,  5.79833984e-04, -6.40869141e-03,  3.29589844e-03,
        -1.72424316e-03,  1.73187256e-03,  3.96728516e-03,  9.04083252e-04,
         1.30004883e-02,  5.88989258e-03,  1.34468079e-04,  5.03540039e-03,
        # Superblock 1 subblock 6 float32 weights.
         6.37817383e-03,  9.52148438e-03, -9.21630859e-03,  4.21142578e-03,
        -1.53350830e-03, -5.52368164e-03, -2.45666504e-03, -1.55639648e-03,
        -1.88827515e-04, -1.32751465e-03,  1.53350830e-03,  5.64575195e-03,
         6.98852539e-03, -4.05883789e-03, -1.09252930e-02,  1.82342529e-03,
        -1.80664062e-02,  3.38745117e-03, -4.48608398e-03, -8.17871094e-03,
         2.28881836e-03, -7.87353516e-03, -1.10473633e-02, -9.03320312e-03,
         2.15148926e-03, -9.03320312e-03,  3.05175781e-03,  3.93676758e-03,
         3.50952148e-04, -1.29699707e-03,  2.54821777e-03,  2.13623047e-03,
        # Superblock 1 subblock 7 float32 weights.
        -2.48718262e-03,  2.92968750e-03,  1.44958496e-03,  2.13623047e-03,
         2.88391113e-03, -2.44140625e-03,  2.16674805e-03,  7.26318359e-03,
         9.39941406e-03,  3.31115723e-03,  1.43432617e-03, -4.36401367e-03,
         6.65283203e-03, -8.72802734e-03,  1.28784180e-02,  4.91333008e-03,
         6.46972656e-03,  3.96728516e-03,  1.93786621e-03, -6.98089600e-04,
        -6.07299805e-03,  2.93731689e-04, -3.03268433e-04,  7.23266602e-03,
         3.87573242e-03, -2.44140625e-03,  3.20434570e-03,  3.77655029e-04,
         8.30078125e-03, -4.33349609e-03,  4.18090820e-03,  1.71661377e-03,
    )
    expected_output = Tensor[DType.uint8](
        TensorShape(num_rows, 2 * sizeof[BlockQ4K]()),
        # Superblock 0 float16 scale for quantized scales.
        235, 2,
        # Superblock 0 float16 scale for quantized mins.
        164, 12,
        # Superblock 0 packed scales and mins.
        158, 191, 172, 160, 223, 243, 174, 97, 252, 186, 36, 165,
        # Superblock 0 packed nibble weights.
         40,  42,  84, 103,  36,  54,   6, 101,  # Subblocks 0-1.
        132,  52, 102,  96, 104,  71,  81, 253,  # (subblk 0 in low 4 bits,
         98,  99, 114, 102,  72,  79,  85,  85,  #  subblk 1 in high 4)
         53,  70,   7, 106,  90,  72, 113,  37,
          6,  87, 134, 211, 148,  83,  88, 181,  # Subblocks 2-3.
        184,  73, 198, 104, 103,  84,  67,  61,
         70, 181, 149, 131, 134, 149, 122,  63,
        166, 144, 148,  86,  70, 237,  73, 151,
        234, 171, 156, 104, 216, 181, 248, 119,  # Subblocks 4-5.
        154, 203, 167, 175, 137, 218, 196, 201,
        120, 138,  96, 172, 198, 188, 125,  92,
         14, 231, 136, 122, 102, 136, 234, 168,
         20, 154,  32,  67, 110, 104,  69,  73,  # Subblocks 6-7.
          7,  21,  59,  51, 187,  37,  70, 251,
        130,  21,  65,  36,  25, 152, 102, 215,
         41,  75, 133,  71,  67,  75,  38,  95,
        # Superblock 1 float16 scale for quantized scales.
        85, 2,
        # Superblock 1 float16 scale for quantized mins.
        238, 12,
        # Superblock 1 packed scales and mins.
        255, 242, 237, 165, 191, 241, 225,  97, 149,  54, 196, 236,
        # Superblock 1 packed nibble weights.
        201,  54, 100, 170,   7, 219,  24,  72,  # Subblocks 0-1.
        151,  88,  75,  23, 202,  15, 192, 181,
        200, 101, 118, 105, 173, 133,  88,  91,
        151, 206, 152,  71, 204, 199, 237, 123,
        230, 219,  46, 199, 132, 121, 248, 163,  # Subblocks 2-3.
        166,  89, 141,   6, 122, 106, 200, 120,
        212,  88, 105, 103, 151,  93,  70,  84,
         56,  84, 161,  11,  42, 112,  85, 124,
        186,   6, 141, 186, 179, 119, 161, 122,  # Subblocks 4-5.
        121, 153, 102, 117, 122, 169,  84, 168,
        157, 134,  94, 198, 216, 136,  80, 169,
        120, 148, 160, 132, 247, 177, 135, 181,
         77, 143, 117, 124, 137,  71, 120, 169,  # Subblocks 6-7.
        202, 137, 123,  61, 174,   8, 228, 155,
        160, 140, 119,  85,  43, 102, 100, 165,
        139,  69, 139, 108, 186,  57, 139, 123,
    )
    # fmt: on

    # Quantize the float32 token embeddings to q4_k.
    q4_k_symbol = g.quantize[Q4_KEncoding](f32_tensor)

    # TODO(GEX-498): Remove pseudo-identity once GEX-498 is fixed.
    g.output(q4_k_symbol + g[0])

    zeros = Tensor[DType.uint8](TensorShape(num_rows, 2 * sizeof[BlockQ4K]()))
    memset_zero(zeros.unsafe_ptr(), zeros.num_elements())

    actual_output = execute_unary[outtype = DType.uint8](g, zeros)

    assert_tensors_equal(actual_output, expected_output)


def test_quantize_q5_k():
    # Define hyperparameters.
    alias num_rows = 1

    # Initialize graph.
    g = Graph(
        "q5_k_quantization",
        in_types=List[Type](
            TensorType(DType.uint8, num_rows, 2 * sizeof[BlockQ5K]())
        ),
        out_types=List[Type](
            TensorType(DType.uint8, num_rows, 2 * sizeof[BlockQ5K]())
        ),
    )

    # fmt: off
    f32_tensor = Tensor[DType.float32](
        # Two blocks of row 0 of Llama3's `token_embd` parameter.
        # (https://huggingface.co/brendanduke/Llama-3-8B-f32.gguf)
        TensorShape(num_rows, 2 * BlockQ5K.elements_per_superblock()),
        # Superblock 0 subblock 0 float32 weights.
         1.37329102e-03,  5.09643555e-03, -3.03649902e-03, -1.81198120e-05,
        -2.91442871e-03, -3.14712524e-04, -7.55310059e-04, -2.18200684e-03,
        -2.77709961e-03, -3.66210938e-03, -7.82012939e-04, -8.85009766e-03,
         2.33459473e-03,  1.07574463e-03, -8.05664062e-03,  8.60595703e-03,
        -5.64575195e-03, -4.21142578e-03, -5.85937500e-03, -4.74929810e-04,
         1.57165527e-03,  1.13525391e-02, -2.22778320e-03, -2.59399414e-03,
        -2.44140625e-03, -1.25885010e-03,  1.85012817e-04,  4.39453125e-03,
         4.88281250e-03,  1.73950195e-03, -7.75146484e-03, -1.44195557e-03,
        # Superblock 0 subblock 1 float32 weights.
        -8.85009766e-03, -7.93457031e-03, -3.68118286e-04,  1.34277344e-03,
        -1.01318359e-02, -4.82177734e-03, -1.37939453e-02,  2.07519531e-03,
         8.11767578e-03, -6.86645508e-03,  1.57165527e-03,  2.38037109e-03,
         2.86865234e-03, -3.26538086e-03, -1.19018555e-03,  2.79541016e-02,
         1.61743164e-03,  1.35040283e-03,  3.90625000e-03,  2.18200684e-03,
        -3.43322754e-03, -3.78417969e-03, -3.33786011e-04,  4.99725342e-04,
        -4.88281250e-03, -4.39453125e-03, -1.40991211e-02,  2.27355957e-03,
        -7.05718994e-04, -4.21142578e-03,  6.22558594e-03, -8.91113281e-03,
        # Superblock 0 subblock 2 float32 weights.
        -1.86157227e-03,  1.41906738e-03, -4.99725342e-04, -7.53784180e-03,
        -5.73730469e-03, -6.19506836e-03,  2.47192383e-03, -3.34167480e-03,
         3.40270996e-03,  4.05883789e-03, -1.60980225e-03,  1.80816650e-03,
         7.40051270e-04, -5.27954102e-03, -6.62231445e-03,  1.22070312e-02,
        -1.77001953e-03, -3.89099121e-03, -4.02832031e-03, -7.01904297e-03,
        -1.62506104e-03, -3.09753418e-03,  7.29370117e-03,  1.68457031e-02,
        -7.05718994e-04, -1.26953125e-02, -4.30297852e-03, -4.10079956e-04,
        -1.66320801e-03,  1.26342773e-02,  5.03540039e-03,  1.25885010e-03,
        # Superblock 0 subblock 3 float32 weights.
        -9.58251953e-03, -1.94549561e-03,  2.34985352e-03,  9.15527344e-03,
         3.50952148e-03, -1.63269043e-03, -1.77001953e-03,  5.76782227e-03,
         6.10351562e-03, -3.90625000e-03,  7.38525391e-03, -7.93457031e-04,
        -1.13677979e-03, -1.73950195e-03, -3.11279297e-03, -5.24902344e-03,
        -3.67736816e-03,  5.88989258e-03,  3.29589844e-03,  1.54876709e-03,
         1.58691406e-03,  3.79943848e-03,  9.44137573e-05, -4.42504883e-03,
         4.66918945e-03,  3.11279297e-03,  3.93676758e-03, -2.25830078e-03,
        -3.93676758e-03,  1.05590820e-02, -3.63159180e-03,  3.31115723e-03,
        # Superblock 0 subblock 4 float32 weights.
         2.01416016e-03,  3.46374512e-03,  5.88989258e-03, -2.27355957e-03,
        -1.83105469e-03, -8.23974609e-03, -2.91442871e-03, -3.32641602e-03,
         1.43432617e-03,  3.69262695e-03, -4.05883789e-03,  1.20239258e-02,
        -2.87294388e-05,  9.53674316e-04, -9.82666016e-03, -5.34057617e-04,
        -2.36511230e-03,  9.11712646e-04, -1.73339844e-02,  6.43920898e-03,
        -6.19506836e-03,  6.59179688e-03,  6.86645508e-03,  5.92041016e-03,
         1.00097656e-02, -4.08935547e-03, -2.36511230e-03,  2.39562988e-03,
        -6.77490234e-03, -2.36511230e-03,  2.25830078e-03, -2.56347656e-03,
        # Superblock 0 subblock 5 float32 weights.
         9.76562500e-03,  1.80053711e-03, -4.48226929e-04, -5.92041016e-03,
         7.65991211e-03,  3.96728516e-03,  1.18408203e-02, -2.96020508e-03,
        -3.64303589e-04,  5.06591797e-03,  1.48010254e-03,  2.39562988e-03,
        -9.11712646e-04,  7.72094727e-03,  6.56127930e-03,  5.31005859e-03,
        -2.76184082e-03, -1.46484375e-03, -6.19506836e-03,  1.60217285e-03,
         6.59179688e-03,  3.55529785e-03, -3.37219238e-03, -7.62939453e-03,
        -1.67236328e-02,  9.64355469e-03, -1.76239014e-03, -2.92968750e-03,
        -5.76782227e-03, -1.48773193e-03,  8.72802734e-03,  1.56402588e-03,
        # Superblock 0 subblock 6 float32 weights.
        -3.38745117e-03,  5.88989258e-03, -1.00708008e-02, -4.88281250e-03,
         1.31835938e-02,  3.90625000e-03, -2.19726562e-03,  4.76074219e-03,
         1.37329102e-03, -1.22070312e-03,  8.66699219e-03, -4.45556641e-03,
         8.54492188e-03, -1.19018555e-03, -9.48905945e-05,  7.26318359e-03,
        -5.70678711e-03, -1.19781494e-03, -7.56835938e-03, -3.43322754e-03,
         4.57763672e-03,  3.15856934e-03, -3.28063965e-04,  1.43432617e-03,
         4.73022461e-03,  7.93457031e-03, -1.78527832e-03,  1.57928467e-03,
        -5.09643555e-03,  8.48388672e-03,  6.82830811e-04,  1.43432617e-02,
        # Superblock 0 subblock 7 float32 weights.
        -5.34057617e-03,  7.75146484e-03, -3.96728516e-03, -5.53131104e-04,
         2.07519531e-03,  1.94549561e-03, -8.23974609e-04, -1.71661377e-04,
        -7.99560547e-03, -5.73730469e-03, -2.89916992e-03, -2.92968750e-03,
         1.07421875e-02, -3.79943848e-03, -2.55584717e-04,  1.69677734e-02,
         6.59179688e-03, -6.50024414e-03, -2.05993652e-04, -3.26538086e-03,
        -5.46264648e-03,  6.68334961e-03,  2.07519531e-03,  1.39160156e-02,
        -3.55529785e-03, -5.22613525e-04,  5.67626953e-03, -1.55639648e-03,
        -9.38415527e-04, -1.73568726e-04, -4.51660156e-03,  8.69750977e-04,
        # Superblock 1 subblock 0 float32 weights.
         5.11169434e-04, -5.40161133e-03, -9.64355469e-03,  3.82995605e-03,
        -3.23486328e-03,  5.12695312e-03, -1.57928467e-03, -1.82342529e-03,
        -3.35693359e-03, -1.95312500e-03,  5.92041016e-03, -3.11279297e-03,
         3.29589844e-03,  1.52587891e-02, -1.92871094e-02, -7.93457031e-03,
        -9.23156738e-04, -6.80541992e-03, -4.91333008e-03,  2.12097168e-03,
         9.70458984e-03, -7.47680664e-03, -8.20159912e-04,  4.97436523e-03,
        -3.15856934e-03,  1.19628906e-02, -1.41143799e-03, -3.46374512e-03,
         7.87353516e-03, -2.41088867e-03,  1.11083984e-02,  5.24902344e-03,
        # Superblock 1 subblock 1 float32 weights.
         7.23266602e-03, -8.91113281e-03, -3.69262695e-03,  3.05175781e-03,
        -1.50146484e-02,  9.09423828e-03, -1.33056641e-02, -7.65991211e-03,
         1.87683105e-03, -5.24902344e-03, -7.75146484e-03, -1.20849609e-02,
         6.10351562e-03, -1.42822266e-02,  6.86645508e-03,  4.48608398e-03,
         5.92041016e-03, -3.47900391e-03, -2.21252441e-03, -3.86047363e-03,
         2.31933594e-03, -7.55310059e-04, -6.01196289e-03, -5.64575195e-03,
         6.06536865e-04,  6.34765625e-03,  1.46484375e-03, -7.01904297e-03,
         6.22558594e-03,  7.26318359e-03,  1.06811523e-02, -2.79235840e-03,
        # Superblock 1 subblock 2 float32 weights.
        -3.05175781e-04,  7.81250000e-03,  1.27563477e-02,  7.66754150e-04,
        -3.11279297e-03,  5.03540039e-03,  3.67736816e-03, -4.79125977e-03,
        -2.06947327e-04,  4.24194336e-03,  1.03759766e-02, -4.55379486e-05,
         6.40869141e-03,  5.43212891e-03,  3.32641602e-03,  2.47192383e-03,
        -3.96728516e-03,  2.67028809e-03,  4.21142578e-03,  7.89642334e-04,
         1.80053711e-03,  1.10473633e-02, -8.27789307e-04, -3.28063965e-03,
         3.03649902e-03, -4.05883789e-03, -8.05664062e-03,  8.11767578e-03,
         5.61523438e-03, -1.02539062e-02, -1.61743164e-03,  9.21630859e-03,
        # Superblock 1 subblock 3 float32 weights.
         8.66699219e-03,  6.89697266e-03, -7.20214844e-03,  6.43920898e-03,
         2.80380249e-04, -2.99453735e-04,  9.94873047e-03,  3.02124023e-03,
         3.35693359e-03, -3.06701660e-03,  5.03540039e-04, -9.82666016e-03,
        -1.00708008e-03, -1.54876709e-03,  5.79833984e-03, -1.02996826e-03,
         7.23266602e-03, -3.57055664e-03, -2.57873535e-03, -2.68554688e-03,
         1.35040283e-03, -3.55529785e-03, -5.06591797e-03, -3.20434570e-03,
        -6.04248047e-03, -3.82995605e-03,  2.79235840e-03, -9.46044922e-03,
        -7.32421875e-03, -5.91278076e-04, -3.18908691e-03, -2.57492065e-04,
        # Superblock 1 subblock 4 float32 weights.
         6.37817383e-03, -7.20977783e-04,  1.15966797e-02,  6.19506836e-03,
        -7.38525391e-03,  1.21307373e-03, -1.02539062e-02,  5.82885742e-03,
         4.85229492e-03,  4.63867188e-03, -1.53350830e-03, -3.38745117e-03,
         6.50024414e-03,  4.05883789e-03, -5.00488281e-03,  3.05175781e-03,
         1.19018555e-02, -1.18255615e-03,  1.45874023e-02, -1.72615051e-04,
         3.17382812e-03,  2.41088867e-03, -1.25122070e-02,  4.57763672e-03,
         2.51770020e-03, -4.94384766e-03, -1.17187500e-02, -4.24194336e-03,
         5.30242920e-04, -1.10473633e-02,  9.53674316e-04, -2.77709961e-03,
        # Superblock 1 subblock 5 float32 weights.
         6.25610352e-03, -1.50756836e-02,  2.55584717e-04,  6.19506836e-03,
         6.01196289e-03, -1.36566162e-03,  3.76892090e-03, -2.04467773e-03,
        -2.45666504e-03,  1.86157227e-03, -4.39453125e-03, -1.64031982e-03,
        -2.45666504e-03,  3.20434570e-03, -6.25610352e-03,  3.29589844e-03,
         2.62451172e-03,  6.02722168e-04, -6.19506836e-03,  8.23974609e-03,
         9.82666016e-03,  5.79833984e-04, -6.40869141e-03,  3.29589844e-03,
        -1.72424316e-03,  1.73187256e-03,  3.96728516e-03,  9.04083252e-04,
         1.30004883e-02,  5.88989258e-03,  1.34468079e-04,  5.03540039e-03,
        # Superblock 1 subblock 6 float32 weights.
         6.37817383e-03,  9.52148438e-03, -9.21630859e-03,  4.21142578e-03,
        -1.53350830e-03, -5.52368164e-03, -2.45666504e-03, -1.55639648e-03,
        -1.88827515e-04, -1.32751465e-03,  1.53350830e-03,  5.64575195e-03,
         6.98852539e-03, -4.05883789e-03, -1.09252930e-02,  1.82342529e-03,
        -1.80664062e-02,  3.38745117e-03, -4.48608398e-03, -8.17871094e-03,
         2.28881836e-03, -7.87353516e-03, -1.10473633e-02, -9.03320312e-03,
         2.15148926e-03, -9.03320312e-03,  3.05175781e-03,  3.93676758e-03,
         3.50952148e-04, -1.29699707e-03,  2.54821777e-03,  2.13623047e-03,
        # Superblock 1 subblock 7 float32 weights.
        -2.48718262e-03,  2.92968750e-03,  1.44958496e-03,  2.13623047e-03,
         2.88391113e-03, -2.44140625e-03,  2.16674805e-03,  7.26318359e-03,
         9.39941406e-03,  3.31115723e-03,  1.43432617e-03, -4.36401367e-03,
         6.65283203e-03, -8.72802734e-03,  1.28784180e-02,  4.91333008e-03,
         6.46972656e-03,  3.96728516e-03,  1.93786621e-03, -6.98089600e-04,
        -6.07299805e-03,  2.93731689e-04, -3.03268433e-04,  7.23266602e-03,
         3.87573242e-03, -2.44140625e-03,  3.20434570e-03,  3.77655029e-04,
         8.30078125e-03, -4.33349609e-03,  4.18090820e-03,  1.71661377e-03,
    )
    expected_output = Tensor[DType.uint8](
        TensorShape(num_rows, 2 * sizeof[BlockQ5K]()),
        # Superblock 0 float16 scale for quantized scales.
        105, 1,
        # Superblock 0 float16 scale for quantized mins.
        133, 12,
        # Superblock 0 packed scales and mins.
        158, 191, 172, 158, 224, 242, 174,  99, 252, 218,  68, 214,
        # Superblock 0 packed weight high-bits.
         49, 241,  56,  24, 120,  96,  36,  72,
         62,  52, 104,  48, 241,  48,  32, 247,
        176,  56,   8,  56, 105, 249,  20, 148,
         88, 104, 184,  17,   1, 125,  52, 120,
        # Superblock 0 packed nibble weights.
         64,  70, 169, 190,  57, 125,  13, 202,  # Subblocks 0-1.
          9,  88, 188, 192, 193, 143, 145, 251,  # (subblk 0 in 4 lower bits,
        181, 183, 213, 205, 128, 127, 170, 186,  #  subblk 1 in 4 higher bits)
        122, 124,  14, 196, 165, 112, 242,  75,
         11, 207,  61, 213,  71, 199, 192, 138,  # Subblocks 2-3.
        129, 146, 172, 239, 222, 200, 166, 122,
        156, 137,  73,  22,  28,  90, 245, 143,
        109,  64,  89, 189, 156, 251, 147,  79,
        212,  86,  41, 192, 176, 122, 255, 255,  # Subblocks 4-5.
         36, 134,  78,  95,  34, 179, 168, 130,
          0,  19, 192,  73, 172, 121, 250, 169,
         13, 222,  16, 245, 203,  16, 197,  64,
         56,  52,  80, 151, 206, 194, 154, 163,  # Subblocks 6-7.
         15,  59, 104, 103, 120,  91, 157, 246,
         37,  43, 163, 104,  51,  33, 204, 191,
         83, 151,  27, 143, 150, 168,  78, 191,
        # Superblock 1 float16 scale for quantized scales.
        38, 1,
        # Superblock 1 float16 scale for quantized mins.
        234, 12,
        # Superblock 1 packed scales and mins.
        255, 239, 235, 164, 191, 243, 226,  97, 162,  51, 194, 215,
        # Superblock 1 packed weight high-bits.
        123, 204,  52, 251, 232,  55, 237, 216, 218, 244,  77,  64, 215,
        117, 142, 246, 187, 228, 150,  33, 127,  54,   1, 177, 214,  35,
        235, 100, 231,  98, 227, 101,
        # Superblock 1 packed nibble weights.
        178, 140, 232, 101,  14, 230,  32, 159,  # Subblocks 0-1.
         94, 207, 150,  78, 164,  31, 176, 138,
        160, 235,  13, 227,  90,  42, 176, 198,
         62, 172,  64, 174, 168, 191, 251, 246,
        221, 184,  78, 175,   9, 244, 242,  71,  # Subblocks 2-3.
         93, 179,  11,  13, 230, 213, 146, 225,
        184, 161, 195, 191,  32, 172, 140, 185,
         98, 168,  67,  24,  69, 240, 187, 250,
        134,  14,  28, 133, 134,   0,  83, 245,  # Subblocks 4-5.
        228,  52, 205, 251, 230,  83, 169,  82,
         76,  45, 175, 174, 194,  33, 160,  84,
        241,  57,  97,  42, 255, 130,  31, 123,
        156,  31, 250,   9,  19, 158,   2, 115,  # Subblocks 6-7.
        164,  35, 246, 107, 109,   0, 248,  71,
         96,  56,  15, 203,  71, 220, 200, 122,
         39, 154,  24, 217, 149,  99,  55, 247,
    )
    # fmt: on

    # Quantize the float32 token embeddings to q5_k.
    q5_k_symbol = g.quantize[Q5_KEncoding](f32_tensor)

    # TODO(GEX-498): Remove pseudo-identity once GEX-498 is fixed.
    g.output(q5_k_symbol + g[0])

    zeros = Tensor[DType.uint8](TensorShape(num_rows, 2 * sizeof[BlockQ5K]()))
    memset_zero(zeros.unsafe_ptr(), zeros.num_elements())

    actual_output = execute_unary[outtype = DType.uint8](g, zeros)

    assert_tensors_equal(actual_output, expected_output)


def test_quantize_q6_k():
    # Define hyperparameters.
    alias num_rows = 1

    # Initialize graph.
    g = Graph(
        "q6_k_quantization",
        in_types=List[Type](
            TensorType(DType.uint8, num_rows, 2 * sizeof[BlockQ6K]())
        ),
        out_types=List[Type](
            TensorType(DType.uint8, num_rows, 2 * sizeof[BlockQ6K]())
        ),
    )

    # fmt: off
    f32_tensor = Tensor[DType.float32](
        # Two blocks of row 0 of Llama3's `token_embd` parameter.
        # (https://huggingface.co/brendanduke/Llama-3-8B-f32.gguf)
        TensorShape(num_rows, 2 * BlockQ6K.elements_per_superblock()),
        # Superblock 0 subblock 0 float32 weights.
         1.37329102e-03,  5.09643555e-03, -3.03649902e-03, -1.81198120e-05,
        -2.91442871e-03, -3.14712524e-04, -7.55310059e-04, -2.18200684e-03,
        -2.77709961e-03, -3.66210938e-03, -7.82012939e-04, -8.85009766e-03,
         2.33459473e-03,  1.07574463e-03, -8.05664062e-03,  8.60595703e-03,
        # Superblock 0 subblock 1 float32 weights.
        -5.64575195e-03, -4.21142578e-03, -5.85937500e-03, -4.74929810e-04,
         1.57165527e-03,  1.13525391e-02, -2.22778320e-03, -2.59399414e-03,
        -2.44140625e-03, -1.25885010e-03,  1.85012817e-04,  4.39453125e-03,
         4.88281250e-03,  1.73950195e-03, -7.75146484e-03, -1.44195557e-03,
        # Superblock 0 subblock 2 float32 weights.
        -8.85009766e-03, -7.93457031e-03, -3.68118286e-04,  1.34277344e-03,
        -1.01318359e-02, -4.82177734e-03, -1.37939453e-02,  2.07519531e-03,
         8.11767578e-03, -6.86645508e-03,  1.57165527e-03,  2.38037109e-03,
         2.86865234e-03, -3.26538086e-03, -1.19018555e-03,  2.79541016e-02,
        # Superblock 0 subblock 3 float32 weights.
         1.61743164e-03,  1.35040283e-03,  3.90625000e-03,  2.18200684e-03,
        -3.43322754e-03, -3.78417969e-03, -3.33786011e-04,  4.99725342e-04,
        -4.88281250e-03, -4.39453125e-03, -1.40991211e-02,  2.27355957e-03,
        -7.05718994e-04, -4.21142578e-03,  6.22558594e-03, -8.91113281e-03,
        # Superblock 0 subblock 4 float32 weights.
        -1.86157227e-03,  1.41906738e-03, -4.99725342e-04, -7.53784180e-03,
        -5.73730469e-03, -6.19506836e-03,  2.47192383e-03, -3.34167480e-03,
         3.40270996e-03,  4.05883789e-03, -1.60980225e-03,  1.80816650e-03,
         7.40051270e-04, -5.27954102e-03, -6.62231445e-03,  1.22070312e-02,
        # Superblock 0 subblock 5 float32 weights.
        -1.77001953e-03, -3.89099121e-03, -4.02832031e-03, -7.01904297e-03,
        -1.62506104e-03, -3.09753418e-03,  7.29370117e-03,  1.68457031e-02,
        -7.05718994e-04, -1.26953125e-02, -4.30297852e-03, -4.10079956e-04,
        -1.66320801e-03,  1.26342773e-02,  5.03540039e-03,  1.25885010e-03,
        # Superblock 0 subblock 6 float32 weights.
        -9.58251953e-03, -1.94549561e-03,  2.34985352e-03,  9.15527344e-03,
         3.50952148e-03, -1.63269043e-03, -1.77001953e-03,  5.76782227e-03,
         6.10351562e-03, -3.90625000e-03,  7.38525391e-03, -7.93457031e-04,
        -1.13677979e-03, -1.73950195e-03, -3.11279297e-03, -5.24902344e-03,
        # Superblock 0 subblock 7 float32 weights.
        -3.67736816e-03,  5.88989258e-03,  3.29589844e-03,  1.54876709e-03,
         1.58691406e-03,  3.79943848e-03,  9.44137573e-05, -4.42504883e-03,
         4.66918945e-03,  3.11279297e-03,  3.93676758e-03, -2.25830078e-03,
        -3.93676758e-03,  1.05590820e-02, -3.63159180e-03,  3.31115723e-03,
        # Superblock 0 subblock 8 float32 weights.
         2.01416016e-03,  3.46374512e-03,  5.88989258e-03, -2.27355957e-03,
        -1.83105469e-03, -8.23974609e-03, -2.91442871e-03, -3.32641602e-03,
         1.43432617e-03,  3.69262695e-03, -4.05883789e-03,  1.20239258e-02,
        -2.87294388e-05,  9.53674316e-04, -9.82666016e-03, -5.34057617e-04,
        # Superblock 0 subblock 9 float32 weights.
        -2.36511230e-03,  9.11712646e-04, -1.73339844e-02,  6.43920898e-03,
        -6.19506836e-03,  6.59179688e-03,  6.86645508e-03,  5.92041016e-03,
         1.00097656e-02, -4.08935547e-03, -2.36511230e-03,  2.39562988e-03,
        -6.77490234e-03, -2.36511230e-03,  2.25830078e-03, -2.56347656e-03,
        # Superblock 0 subblock 10 float32 weights.
         9.76562500e-03,  1.80053711e-03, -4.48226929e-04, -5.92041016e-03,
         7.65991211e-03,  3.96728516e-03,  1.18408203e-02, -2.96020508e-03,
        -3.64303589e-04,  5.06591797e-03,  1.48010254e-03,  2.39562988e-03,
        -9.11712646e-04,  7.72094727e-03,  6.56127930e-03,  5.31005859e-03,
        # Superblock 0 subblock 11 float32 weights.
        -2.76184082e-03, -1.46484375e-03, -6.19506836e-03,  1.60217285e-03,
         6.59179688e-03,  3.55529785e-03, -3.37219238e-03, -7.62939453e-03,
        -1.67236328e-02,  9.64355469e-03, -1.76239014e-03, -2.92968750e-03,
        -5.76782227e-03, -1.48773193e-03,  8.72802734e-03,  1.56402588e-03,
        # Superblock 0 subblock 12 float32 weights.
        -3.38745117e-03,  5.88989258e-03, -1.00708008e-02, -4.88281250e-03,
         1.31835938e-02,  3.90625000e-03, -2.19726562e-03,  4.76074219e-03,
         1.37329102e-03, -1.22070312e-03,  8.66699219e-03, -4.45556641e-03,
         8.54492188e-03, -1.19018555e-03, -9.48905945e-05,  7.26318359e-03,
        # Superblock 0 subblock 13 float32 weights.
        -5.70678711e-03, -1.19781494e-03, -7.56835938e-03, -3.43322754e-03,
         4.57763672e-03,  3.15856934e-03, -3.28063965e-04,  1.43432617e-03,
         4.73022461e-03,  7.93457031e-03, -1.78527832e-03,  1.57928467e-03,
        -5.09643555e-03,  8.48388672e-03,  6.82830811e-04,  1.43432617e-02,
        # Superblock 0 subblock 14 float32 weights.
        -5.34057617e-03,  7.75146484e-03, -3.96728516e-03, -5.53131104e-04,
         2.07519531e-03,  1.94549561e-03, -8.23974609e-04, -1.71661377e-04,
        -7.99560547e-03, -5.73730469e-03, -2.89916992e-03, -2.92968750e-03,
         1.07421875e-02, -3.79943848e-03, -2.55584717e-04,  1.69677734e-02,
        # Superblock 0 subblock 15 float32 weights.
         6.59179688e-03, -6.50024414e-03, -2.05993652e-04, -3.26538086e-03,
        -5.46264648e-03,  6.68334961e-03,  2.07519531e-03,  1.39160156e-02,
        -3.55529785e-03, -5.22613525e-04,  5.67626953e-03, -1.55639648e-03,
        -9.38415527e-04, -1.73568726e-04, -4.51660156e-03,  8.69750977e-04,
        # Superblock 1 subblock 0 float32 weights.
         5.11169434e-04, -5.40161133e-03, -9.64355469e-03,  3.82995605e-03,
        -3.23486328e-03,  5.12695312e-03, -1.57928467e-03, -1.82342529e-03,
        -3.35693359e-03, -1.95312500e-03,  5.92041016e-03, -3.11279297e-03,
         3.29589844e-03,  1.52587891e-02, -1.92871094e-02, -7.93457031e-03,
        # Superblock 1 subblock 1 float32 weights.
        -9.23156738e-04, -6.80541992e-03, -4.91333008e-03,  2.12097168e-03,
         9.70458984e-03, -7.47680664e-03, -8.20159912e-04,  4.97436523e-03,
        -3.15856934e-03,  1.19628906e-02, -1.41143799e-03, -3.46374512e-03,
         7.87353516e-03, -2.41088867e-03,  1.11083984e-02,  5.24902344e-03,
        # Superblock 1 subblock 2 float32 weights.
         7.23266602e-03, -8.91113281e-03, -3.69262695e-03,  3.05175781e-03,
        -1.50146484e-02,  9.09423828e-03, -1.33056641e-02, -7.65991211e-03,
         1.87683105e-03, -5.24902344e-03, -7.75146484e-03, -1.20849609e-02,
         6.10351562e-03, -1.42822266e-02,  6.86645508e-03,  4.48608398e-03,
        # Superblock 1 subblock 3 float32 weights.
         5.92041016e-03, -3.47900391e-03, -2.21252441e-03, -3.86047363e-03,
         2.31933594e-03, -7.55310059e-04, -6.01196289e-03, -5.64575195e-03,
         6.06536865e-04,  6.34765625e-03,  1.46484375e-03, -7.01904297e-03,
         6.22558594e-03,  7.26318359e-03,  1.06811523e-02, -2.79235840e-03,
        # Superblock 1 subblock 4 float32 weights.
        -3.05175781e-04,  7.81250000e-03,  1.27563477e-02,  7.66754150e-04,
        -3.11279297e-03,  5.03540039e-03,  3.67736816e-03, -4.79125977e-03,
        -2.06947327e-04,  4.24194336e-03,  1.03759766e-02, -4.55379486e-05,
         6.40869141e-03,  5.43212891e-03,  3.32641602e-03,  2.47192383e-03,
        # Superblock 1 subblock 5 float32 weights.
        -3.96728516e-03,  2.67028809e-03,  4.21142578e-03,  7.89642334e-04,
         1.80053711e-03,  1.10473633e-02, -8.27789307e-04, -3.28063965e-03,
         3.03649902e-03, -4.05883789e-03, -8.05664062e-03,  8.11767578e-03,
         5.61523438e-03, -1.02539062e-02, -1.61743164e-03,  9.21630859e-03,
        # Superblock 1 subblock 6 float32 weights.
         8.66699219e-03,  6.89697266e-03, -7.20214844e-03,  6.43920898e-03,
         2.80380249e-04, -2.99453735e-04,  9.94873047e-03,  3.02124023e-03,
         3.35693359e-03, -3.06701660e-03,  5.03540039e-04, -9.82666016e-03,
        -1.00708008e-03, -1.54876709e-03,  5.79833984e-03, -1.02996826e-03,
        # Superblock 1 subblock 7 float32 weights.
         7.23266602e-03, -3.57055664e-03, -2.57873535e-03, -2.68554688e-03,
         1.35040283e-03, -3.55529785e-03, -5.06591797e-03, -3.20434570e-03,
        -6.04248047e-03, -3.82995605e-03,  2.79235840e-03, -9.46044922e-03,
        -7.32421875e-03, -5.91278076e-04, -3.18908691e-03, -2.57492065e-04,
        # Superblock 1 subblock 8 float32 weights.
         6.37817383e-03, -7.20977783e-04,  1.15966797e-02,  6.19506836e-03,
        -7.38525391e-03,  1.21307373e-03, -1.02539062e-02,  5.82885742e-03,
         4.85229492e-03,  4.63867188e-03, -1.53350830e-03, -3.38745117e-03,
         6.50024414e-03,  4.05883789e-03, -5.00488281e-03,  3.05175781e-03,
        # Superblock 1 subblock 9 float32 weights.
         1.19018555e-02, -1.18255615e-03,  1.45874023e-02, -1.72615051e-04,
         3.17382812e-03,  2.41088867e-03, -1.25122070e-02,  4.57763672e-03,
         2.51770020e-03, -4.94384766e-03, -1.17187500e-02, -4.24194336e-03,
         5.30242920e-04, -1.10473633e-02,  9.53674316e-04, -2.77709961e-03,
        # Superblock 1 subblock 10 float32 weights.
         6.25610352e-03, -1.50756836e-02,  2.55584717e-04,  6.19506836e-03,
         6.01196289e-03, -1.36566162e-03,  3.76892090e-03, -2.04467773e-03,
        -2.45666504e-03,  1.86157227e-03, -4.39453125e-03, -1.64031982e-03,
        -2.45666504e-03,  3.20434570e-03, -6.25610352e-03,  3.29589844e-03,
        # Superblock 1 subblock 11 float32 weights.
         2.62451172e-03,  6.02722168e-04, -6.19506836e-03,  8.23974609e-03,
         9.82666016e-03,  5.79833984e-04, -6.40869141e-03,  3.29589844e-03,
        -1.72424316e-03,  1.73187256e-03,  3.96728516e-03,  9.04083252e-04,
         1.30004883e-02,  5.88989258e-03,  1.34468079e-04,  5.03540039e-03,
        # Superblock 1 subblock 12 float32 weights.
         6.37817383e-03,  9.52148438e-03, -9.21630859e-03,  4.21142578e-03,
        -1.53350830e-03, -5.52368164e-03, -2.45666504e-03, -1.55639648e-03,
        -1.88827515e-04, -1.32751465e-03,  1.53350830e-03,  5.64575195e-03,
         6.98852539e-03, -4.05883789e-03, -1.09252930e-02,  1.82342529e-03,
        # Superblock 1 subblock 13 float32 weights.
        -1.80664062e-02,  3.38745117e-03, -4.48608398e-03, -8.17871094e-03,
         2.28881836e-03, -7.87353516e-03, -1.10473633e-02, -9.03320312e-03,
         2.15148926e-03, -9.03320312e-03,  3.05175781e-03,  3.93676758e-03,
         3.50952148e-04, -1.29699707e-03,  2.54821777e-03,  2.13623047e-03,
        # Superblock 1 subblock 14 float32 weights.
        -2.48718262e-03,  2.92968750e-03,  1.44958496e-03,  2.13623047e-03,
         2.88391113e-03, -2.44140625e-03,  2.16674805e-03,  7.26318359e-03,
         9.39941406e-03,  3.31115723e-03,  1.43432617e-03, -4.36401367e-03,
         6.65283203e-03, -8.72802734e-03,  1.28784180e-02,  4.91333008e-03,
        # Superblock 1 subblock 15 float32 weights.
         6.46972656e-03,  3.96728516e-03,  1.93786621e-03, -6.98089600e-04,
        -6.07299805e-03,  2.93731689e-04, -3.03268433e-04,  7.23266602e-03,
         3.87573242e-03, -2.44140625e-03,  3.20434570e-03,  3.77655029e-04,
         8.30078125e-03, -4.33349609e-03,  4.18090820e-03,  1.71661377e-03,
    )
    expected_output = Tensor[DType.uint8](
        TensorShape(num_rows, 2 * sizeof[BlockQ6K]()),
        # Superblock 0 packed low-nibble weights.
         85, 194,  21,  64, 246,  15, 173, 152,  # <- LSBs = subblock  0,
        118,  83,  77, 177, 232, 228,  20,  14,  #    MSBs = subblock  2.
         48, 124, 129, 209,  60,  96,  38,   7,  # <- LSBs = subblock  1,
         23, 132, 143,  20,  50, 139, 102, 228,  #    MSBs = subblock  3.
         26, 169, 128, 238, 188, 182, 160,  62,  # <- LSBs = subblock  4,
         71,  56, 142, 221, 205, 164,  97, 240,  #    MSBs = subblock  6.
        180, 227, 105, 181, 184,  71,  15, 225,  # <- LSBs = subblock  5,
         37, 102,  64, 117, 206,   6, 190, 108,  #    MSBs = subblock  7.
        139,  39, 128, 198,  21, 118,  88,  89,  # <- LSBs = subblock  8,
        220,  54, 187, 176, 192,  61,  10, 241,  #    MSBs = subblock 10.
        220,  50,  17, 140, 101, 156,  28, 219,  # <- LSBs = subblock  9,
         82, 233,  76, 196, 180, 220, 228,  11,  #    MSBs = subblock 11.
        165,  27, 113,  16, 203, 197,  32,   8,  # <- LSBs = subblock 12,
        241, 178,  92, 105, 194, 123,  14,   2,  #    MSBs = subblock 14.
         27, 253,   4, 131, 220,  23, 186,   2,  # <- LSBs = subblock 13,
        129,  18,  61,  75,  37,  13, 160, 227,  #    MSBs = subblock 15.
        # Superblock 0 packed weight high-bits.
         42,  91, 169, 246, 169, 121,  93, 229,  # Subblocks 0,  2,  4,  6.
        213,  89, 229,  84,  86, 106, 120,   3,
        171,  42, 107, 106, 101, 100, 150, 138,  # Subblocks 1,  3,  5,  7.
        102, 118,  97, 169, 165,   5, 155,  82,
        161,  85, 185, 174,  66,  87, 162, 154,  # Subblocks 8, 10, 12, 14.
        153, 165, 134, 164,  10, 161, 163,   6,
        101, 166, 180, 170, 153,  90, 102,  22,  # Subblocks 9, 11, 13, 15.
        147, 141, 101, 150, 165, 133, 158,  73,
        # Superblock 0 quantized scales.
         42, 204, 128,  65, 200, 179,  45, 208,
        201,  82, 202,  79, 194, 190, 178, 192,
        # Superblock 0 float16 scale for quantized scales.
        114, 0,
        # Superblock 1 packed low-nibble weights.
         17, 199,   0, 230, 139,  57, 125, 205,  # <- LSBs = subblock  0,
         26,  93, 106,  11,   5,  41, 128, 163,  #    MSBs = subblock  2.
        178, 130,  77, 234, 182,  20,  34, 147,  # <- LSBs = subblock  1,
        120, 176, 116, 153,  11, 214,  82,  98,  #    MSBs = subblock  3.
         95, 173, 120, 198, 240,  19,   4,  96,  # <- LSBs = subblock  4,
         84, 165, 224, 247,  61,  82, 238,  57,  #    MSBs = subblock  6.
        142,  75, 119, 124,  73,  66, 242,  81,  # <- LSBs = subblock  5,
        206,  61, 156,  21, 141, 234,  80, 248,  #    MSBs = subblock  7.
         46, 178,  96, 207, 196,  13, 156, 192,  # <- LSBs = subblock  8,
        243, 195,  68,   9,  78,  85,  30,  88,  #    MSBs = subblock 10.
          6,  99, 128,  16,  73,  43, 203,   6,  # <- LSBs = subblock  9,
         74,  11,  90, 121,  31, 232,  94,  70,  #    MSBs = subblock 11.
        109, 144, 193, 189, 157, 109, 184, 236,  # <- LSBs = subblock 12,
        155, 132, 199, 189,  11,  87,  19,  71,  #    MSBs = subblock 14.
        122,  31, 159,  60, 120, 255,  16,  72,  # <- LSBs = subblock 13,
         20, 156,  70, 254,   0,  17,   0, 148,  #    MSBs = subblock 15.
        # Superblock 1 packed weight high-bits.
         42,   1, 197,  26,  97, 158,  17, 101,  # Subblocks 0,  2,  4,  6.
        105, 149,  70, 225, 154, 147,  24, 153,
        226,  91,  90,  89, 148,  75,  46, 109,  # Subblocks 1,  3,  5,  7.
         22,  96, 182,  14,  16, 114,  96,  73,
        184, 114,  72, 104,  91, 149,  91,  21,  # Subblocks 8, 10, 12, 14.
         21,  89, 102, 182, 116, 217,   6, 105,
          4, 102,  88, 146, 225,  85, 143,  21,  # Subblocks 9, 11, 13, 15.
        105, 150, 103, 102,  33, 215, 105, 102,
        # Superblock 1 quantized scales.
        128,  79, 155,  70,  84,  75,  67, 192,
         77,  97, 156,  86, 181, 137,  87,  55,
        # Superblock 1 float16 scale for quantized scales.
        79, 128,
    )
    # fmt: on

    # Quantize the float32 token embeddings to q6_k.
    q6_k_symbol = g.quantize[Q6_KEncoding](f32_tensor)

    # TODO(GEX-498): Remove pseudo-identity once GEX-498 is fixed.
    g.output(q6_k_symbol + g[0])

    zeros = Tensor[DType.uint8](TensorShape(num_rows, 2 * sizeof[BlockQ6K]()))
    memset_zero(zeros.unsafe_ptr(), zeros.num_elements())

    actual_output = execute_unary[outtype = DType.uint8](g, zeros)

    assert_tensors_equal(actual_output, expected_output)


def main():
    test_quantize_bfloat16()
    test_quantize_q4_0()
    test_find_extrema()
    test_quantize_q4_k()
    test_quantize_q5_k()
    test_quantize_q6_k()
