##===----------------------------------------------------------------------===##
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##===----------------------------------------------------------------------===##

name: bench_kv_cache_ragged_flash_attention
file: $KERNEL_BENCHMARKS_ROOT/gpu/bench_kv_cache_ragged_flash_attention_paged.mojo

params:

# llama3.1 shapes
# token gen
- $seq_len: 1
  $cache_len: [215, 1024]
  $batch_size: [20, 250, 500]
  num_q_heads: 32
  num_kv_heads: 8
  head_dim: 128
  $use_random_seq_lengths: False
  $use_random_cache_lengths: False
  metadata:
    MODEL: llama3.1

- $seq_len: [215, 1024]
  $cache_len: 0
  $batch_size: [20, 80]
  num_q_heads: 32
  num_kv_heads: 8
  head_dim: 128
  $use_random_seq_lengths: False
  $use_random_cache_lengths: False
  metadata:
    MODEL: llama3.1

# llama 3.2 shape
- $seq_len: 1
  $cache_len: [215, 1024]
  $batch_size: [20, 250, 500]
  num_q_heads: 24
  num_kv_heads: 8
  head_dim: 128
  $use_random_seq_lengths: False
  $use_random_cache_lengths: False
  metadata:
    MODEL: llama3.2

- $seq_len: [215, 1024]
  $cache_len: 0
  $batch_size: [20, 80]
  num_q_heads: 24
  num_kv_heads: 8
  head_dim: 128
  $use_random_seq_lengths: False
  $use_random_cache_lengths: False
  metadata:
    MODEL: llama3.2
