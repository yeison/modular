# ===----------------------------------------------------------------------=== #
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

from builtin.io import _printf
from gpu import barrier
from gpu.host import DeviceContext
from gpu.host.compile import get_gpu_target
from gpu.id import thread_idx
from gpu.intrinsics import threadfence
from gpu.memory import AddressSpace
from gpu.mma import (
    WGMMADescriptor,
    wgmma_async,
    wgmma_commit_group_sync,
    wgmma_fence_aligned,
    wgmma_wait_group_sync,
)
from layout import IntTuple, Layout, LayoutTensor
from layout._fillers import arange
from layout._utils import ManagedLayoutTensor
from layout.layout import print_layout
from layout.tensor_core_async import (
    _lhs_descriptor,
    _rhs_descriptor,
    tile_layout_k_major,
    tile_layout_mn_major,
)
from memory import bitcast

from utils.index import Index


# We have a hard code 2D path in `arange` and it's row-major.
# Add the col-major version as a work-around. Generalizing the above
# may touch too many places.
fn _arange_2d_col_major_tensor(t: LayoutTensor[mut=True, **_]):
    alias layout = t.layout
    alias size = layout.size()

    for i in range(size):
        idx = layout(i)
        t.ptr[idx] = Scalar[t.dtype](i)


fn wgmma_tf32_tf32_f32_kernel[
    M: Int,
    N: Int,
    K: Int,
    WMMA_M: Int,
    WMMA_N: Int,
    WMMA_K: Int,
    a_smem_layout: Layout,
    b_smem_layout: Layout,
](
    a_gmem: LayoutTensor[
        DType.float32, Layout.row_major(M, K), MutableAnyOrigin
    ],
    b_gmem: LayoutTensor[
        DType.float32, Layout.row_major(K, N), MutableAnyOrigin
    ],
    result_c: LayoutTensor[
        DType.float32, Layout.row_major(M, N), MutableAnyOrigin
    ],
):
    var a_smem_tile = LayoutTensor[
        DType.float32,
        a_smem_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var b_smem_tile = LayoutTensor[
        DType.float32,
        b_smem_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var c_reg = SIMD[DType.float32, 4](0)

    for k_i in range(K // WMMA_K):
        var a_gmem_tile = a_gmem.tile[M, WMMA_K](0, k_i)
        var b_gmem_tile = b_gmem.tile[WMMA_K, N](k_i, 0)

        if thread_idx.x == 0:
            a_smem_tile.copy_from(a_gmem_tile)
            b_smem_tile.copy_from(b_gmem_tile)

        barrier()

        var mat_a_desc = _lhs_descriptor(a_smem_tile)
        var mat_b_desc = WGMMADescriptor.create[1, 8](b_smem_tile.ptr)

        wgmma_fence_aligned()

        c_reg = wgmma_async[
            WMMA_M,
            WMMA_N,
            WMMA_K,
            a_type = DType.tensor_float32,
            b_type = DType.tensor_float32,
        ](mat_a_desc, mat_b_desc, c_reg)
        wgmma_commit_group_sync()
        wgmma_wait_group_sync()

    var warp_id = thread_idx.x // 32
    var lan_id = thread_idx.x % 32
    # Refer to this layout:
    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N8-D.png
    # Each warp updates a 16x8 tile, and within each tile,
    # every thread updates a 1x2 vector. The resulting distribution layout
    # is as follows:
    var th_local_res = (
        result_c.tile[16, 8](warp_id, 0)
        .vectorize[1, 2]()
        .distribute[Layout.row_major(8, 4)](lan_id)
    )
    th_local_res[0][0] = c_reg[0]
    th_local_res[0][1] = c_reg[1]
    th_local_res[1][0] = c_reg[2]
    th_local_res[1][1] = c_reg[3]


# CHECK-LABEL: wgmma_tf32_tf32_f32_64x8x8
# CHECK: 1120.0 1148.0 1176.0 1204.0 1232.0 1260.0 1288.0 1316.0
# CHECK: 2912.0 3004.0 3096.0 3188.0 3280.0 3372.0 3464.0 3556.0
# CHECK: 4704.0 4860.0 5016.0 5172.0 5328.0 5484.0 5640.0 5796.0
# CHECK: 6496.0 6716.0 6936.0 7156.0 7376.0 7596.0 7816.0 8036.0
# CHECK: 8288.0 8572.0 8856.0 9140.0 9424.0 9708.0 9992.0 10276.0
# CHECK: 10080.0 10428.0 10776.0 11124.0 11472.0 11820.0 12168.0 12516.0
# CHECK: 11872.0 12284.0 12696.0 13108.0 13520.0 13932.0 14344.0 14756.0
# CHECK: 13664.0 14140.0 14616.0 15092.0 15568.0 16044.0 16520.0 16996.0
# CHECK: 15456.0 15996.0 16536.0 17076.0 17616.0 18156.0 18696.0 19236.0
# CHECK: 17248.0 17852.0 18456.0 19060.0 19664.0 20268.0 20872.0 21476.0
# CHECK: 19040.0 19708.0 20376.0 21044.0 21712.0 22380.0 23048.0 23716.0
# CHECK: 20832.0 21564.0 22296.0 23028.0 23760.0 24492.0 25224.0 25956.0
# CHECK: 22624.0 23420.0 24216.0 25012.0 25808.0 26604.0 27400.0 28196.0
# CHECK: 24416.0 25276.0 26136.0 26996.0 27856.0 28716.0 29576.0 30436.0
# CHECK: 26208.0 27132.0 28056.0 28980.0 29904.0 30828.0 31752.0 32676.0
# CHECK: 28000.0 28988.0 29976.0 30964.0 31952.0 32940.0 33928.0 34916.0
# CHECK: 29792.0 30844.0 31896.0 32948.0 34000.0 35052.0 36104.0 37156.0
# CHECK: 31584.0 32700.0 33816.0 34932.0 36048.0 37164.0 38280.0 39396.0
# CHECK: 33376.0 34556.0 35736.0 36916.0 38096.0 39276.0 40456.0 41636.0
# CHECK: 35168.0 36412.0 37656.0 38900.0 40144.0 41388.0 42632.0 43876.0
# CHECK: 36960.0 38268.0 39576.0 40884.0 42192.0 43500.0 44808.0 46116.0
# CHECK: 38752.0 40124.0 41496.0 42868.0 44240.0 45612.0 46984.0 48356.0
# CHECK: 40544.0 41980.0 43416.0 44852.0 46288.0 47724.0 49160.0 50596.0
# CHECK: 42336.0 43836.0 45336.0 46836.0 48336.0 49836.0 51336.0 52836.0
# CHECK: 44128.0 45692.0 47256.0 48820.0 50384.0 51948.0 53512.0 55076.0
# CHECK: 45920.0 47548.0 49176.0 50804.0 52432.0 54060.0 55688.0 57316.0
# CHECK: 47712.0 49404.0 51096.0 52788.0 54480.0 56172.0 57864.0 59556.0
# CHECK: 49504.0 51260.0 53016.0 54772.0 56528.0 58284.0 60040.0 61796.0
# CHECK: 51296.0 53116.0 54936.0 56756.0 58576.0 60396.0 62216.0 64036.0
# CHECK: 53088.0 54972.0 56856.0 58740.0 60624.0 62508.0 64392.0 66276.0
# CHECK: 54880.0 56828.0 58776.0 60724.0 62672.0 64620.0 66568.0 68516.0
# CHECK: 56672.0 58684.0 60696.0 62708.0 64720.0 66732.0 68744.0 70756.0
# CHECK: 58464.0 60540.0 62616.0 64692.0 66768.0 68844.0 70920.0 72996.0
# CHECK: 60256.0 62396.0 64536.0 66676.0 68816.0 70956.0 73096.0 75236.0
# CHECK: 62048.0 64252.0 66456.0 68660.0 70864.0 73068.0 75272.0 77476.0
# CHECK: 63840.0 66108.0 68376.0 70644.0 72912.0 75180.0 77448.0 79716.0
# CHECK: 65632.0 67964.0 70296.0 72628.0 74960.0 77292.0 79624.0 81956.0
# CHECK: 67424.0 69820.0 72216.0 74612.0 77008.0 79404.0 81800.0 84196.0
# CHECK: 69216.0 71676.0 74136.0 76596.0 79056.0 81516.0 83976.0 86436.0
# CHECK: 71008.0 73532.0 76056.0 78580.0 81104.0 83628.0 86152.0 88676.0
# CHECK: 72800.0 75388.0 77976.0 80564.0 83152.0 85740.0 88328.0 90916.0
# CHECK: 74592.0 77244.0 79896.0 82548.0 85200.0 87852.0 90504.0 93156.0
# CHECK: 76384.0 79100.0 81816.0 84532.0 87248.0 89964.0 92680.0 95396.0
# CHECK: 78176.0 80956.0 83736.0 86516.0 89296.0 92076.0 94856.0 97636.0
# CHECK: 79968.0 82812.0 85656.0 88500.0 91344.0 94188.0 97032.0 99876.0
# CHECK: 81760.0 84668.0 87576.0 90484.0 93392.0 96300.0 99208.0 102116.0
# CHECK: 83552.0 86524.0 89496.0 92468.0 95440.0 98412.0 101384.0 104356.0
# CHECK: 85344.0 88380.0 91416.0 94452.0 97488.0 100524.0 103560.0 106596.0
# CHECK: 87136.0 90236.0 93336.0 96436.0 99536.0 102636.0 105736.0 108836.0
# CHECK: 88928.0 92092.0 95256.0 98420.0 101584.0 104748.0 107912.0 111076.0
# CHECK: 90720.0 93948.0 97176.0 100404.0 103632.0 106860.0 110088.0 113316.0
# CHECK: 92512.0 95804.0 99096.0 102388.0 105680.0 108972.0 112264.0 115556.0
# CHECK: 94304.0 97660.0 101016.0 104372.0 107728.0 111084.0 114440.0 117796.0
# CHECK: 96096.0 99516.0 102936.0 106356.0 109776.0 113196.0 116616.0 120036.0
# CHECK: 97888.0 101372.0 104856.0 108340.0 111824.0 115308.0 118792.0 122276.0
# CHECK: 99680.0 103228.0 106776.0 110324.0 113872.0 117420.0 120968.0 124516.0
# CHECK: 101472.0 105084.0 108696.0 112308.0 115920.0 119532.0 123144.0 126756.0
# CHECK: 103264.0 106940.0 110616.0 114292.0 117968.0 121644.0 125320.0 128996.0
# CHECK: 105056.0 108796.0 112536.0 116276.0 120016.0 123756.0 127496.0 131236.0
# CHECK: 106848.0 110652.0 114456.0 118260.0 122064.0 125868.0 129672.0 133476.0
# CHECK: 108640.0 112508.0 116376.0 120244.0 124112.0 127980.0 131848.0 135716.0
# CHECK: 110432.0 114364.0 118296.0 122228.0 126160.0 130092.0 134024.0 137956.0
# CHECK: 112224.0 116220.0 120216.0 124212.0 128208.0 132204.0 136200.0 140196.0
# CHECK: 114016.0 118076.0 122136.0 126196.0 130256.0 134316.0 138376.0 142436.0
def wgmma_tf32_tf32_f32_64x8x8(ctx: DeviceContext):
    print("== wgmma_tf32_tf32_f32_64x8x8")
    alias M = 64
    alias N = 8
    alias K = 8

    var lhs = ManagedLayoutTensor[DType.float32, Layout.row_major(M, K)](ctx)
    arange(lhs.tensor())

    var rhs = ManagedLayoutTensor[DType.float32, Layout.row_major(K, N)](ctx)
    arange(rhs.tensor())

    var res = ManagedLayoutTensor[DType.float32, Layout.row_major(M, N)](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N8-core-matrices-A.png
    alias a_smem_layout = tile_layout_k_major[DType.float32, M, BK=8]()

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N8-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(4, 2), 8), IntTuple(IntTuple(1, 32), 4)
    )
    alias kernel = wgmma_tf32_tf32_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        8,
        a_smem_layout,
        b_smem_layout,
    ]

    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = lhs^
    _ = rhs^
    _ = res^


# CHECK-LABEL: wgmma_tf32_tf32_f32_64x8x8_inst_64x8x16
# CHECK: 9920.0 10040.0 10160.0 10280.0 10400.0 10520.0 10640.0 10760.0
# CHECK: 25280.0 25656.0 26032.0 26408.0 26784.0 27160.0 27536.0 27912.0
# CHECK: 40640.0 41272.0 41904.0 42536.0 43168.0 43800.0 44432.0 45064.0
# CHECK: 56000.0 56888.0 57776.0 58664.0 59552.0 60440.0 61328.0 62216.0
# CHECK: 71360.0 72504.0 73648.0 74792.0 75936.0 77080.0 78224.0 79368.0
# CHECK: 86720.0 88120.0 89520.0 90920.0 92320.0 93720.0 95120.0 96520.0
# CHECK: 102080.0 103736.0 105392.0 107048.0 108704.0 110360.0 112016.0 113672.0
# CHECK: 117440.0 119352.0 121264.0 123176.0 125088.0 127000.0 128912.0 130824.0
# CHECK: 132800.0 134968.0 137136.0 139304.0 141472.0 143640.0 145808.0 147976.0
# CHECK: 148160.0 150584.0 153008.0 155432.0 157856.0 160280.0 162704.0 165128.0
# CHECK: 163520.0 166200.0 168880.0 171560.0 174240.0 176920.0 179600.0 182280.0
# CHECK: 178880.0 181816.0 184752.0 187688.0 190624.0 193560.0 196496.0 199432.0
# CHECK: 194240.0 197432.0 200624.0 203816.0 207008.0 210200.0 213392.0 216584.0
# CHECK: 209600.0 213048.0 216496.0 219944.0 223392.0 226840.0 230288.0 233736.0
# CHECK: 224960.0 228664.0 232368.0 236072.0 239776.0 243480.0 247184.0 250888.0
# CHECK: 240320.0 244280.0 248240.0 252200.0 256160.0 260120.0 264080.0 268040.0
# CHECK: 255680.0 259896.0 264112.0 268328.0 272544.0 276760.0 280976.0 285192.0
# CHECK: 271040.0 275512.0 279984.0 284456.0 288928.0 293400.0 297872.0 302344.0
# CHECK: 286400.0 291128.0 295856.0 300584.0 305312.0 310040.0 314768.0 319496.0
# CHECK: 301760.0 306744.0 311728.0 316712.0 321696.0 326680.0 331664.0 336648.0
# CHECK: 317120.0 322360.0 327600.0 332840.0 338080.0 343320.0 348560.0 353800.0
# CHECK: 332480.0 337976.0 343472.0 348968.0 354464.0 359960.0 365456.0 370952.0
# CHECK: 347840.0 353592.0 359344.0 365096.0 370848.0 376600.0 382352.0 388104.0
# CHECK: 363200.0 369208.0 375216.0 381224.0 387232.0 393240.0 399248.0 405256.0
# CHECK: 378560.0 384824.0 391088.0 397352.0 403616.0 409880.0 416144.0 422408.0
# CHECK: 393920.0 400440.0 406960.0 413480.0 420000.0 426520.0 433040.0 439560.0
# CHECK: 409280.0 416056.0 422832.0 429608.0 436384.0 443160.0 449936.0 456712.0
# CHECK: 424640.0 431672.0 438704.0 445736.0 452768.0 459800.0 466832.0 473864.0
# CHECK: 440000.0 447288.0 454576.0 461864.0 469152.0 476440.0 483728.0 491016.0
# CHECK: 455360.0 462904.0 470448.0 477992.0 485536.0 493080.0 500624.0 508168.0
# CHECK: 470720.0 478520.0 486320.0 494120.0 501920.0 509720.0 517520.0 525320.0
# CHECK: 486080.0 494136.0 502192.0 510248.0 518304.0 526360.0 534416.0 542472.0
# CHECK: 501440.0 509752.0 518064.0 526376.0 534688.0 543000.0 551312.0 559624.0
# CHECK: 516800.0 525368.0 533936.0 542504.0 551072.0 559640.0 568208.0 576776.0
# CHECK: 532160.0 540984.0 549808.0 558632.0 567456.0 576280.0 585104.0 593928.0
# CHECK: 547520.0 556600.0 565680.0 574760.0 583840.0 592920.0 602000.0 611080.0
# CHECK: 562880.0 572216.0 581552.0 590888.0 600224.0 609560.0 618896.0 628232.0
# CHECK: 578240.0 587832.0 597424.0 607016.0 616608.0 626200.0 635792.0 645384.0
# CHECK: 593600.0 603448.0 613296.0 623144.0 632992.0 642840.0 652688.0 662536.0
# CHECK: 608960.0 619064.0 629168.0 639272.0 649376.0 659480.0 669584.0 679688.0
# CHECK: 624320.0 634680.0 645040.0 655400.0 665760.0 676120.0 686480.0 696840.0
# CHECK: 639680.0 650296.0 660912.0 671528.0 682144.0 692760.0 703376.0 713992.0
# CHECK: 655040.0 665912.0 676784.0 687656.0 698528.0 709400.0 720272.0 731144.0
# CHECK: 670400.0 681528.0 692656.0 703784.0 714912.0 726040.0 737168.0 748296.0
# CHECK: 685760.0 697144.0 708528.0 719912.0 731296.0 742680.0 754064.0 765448.0
# CHECK: 701120.0 712760.0 724400.0 736040.0 747680.0 759320.0 770960.0 782600.0
# CHECK: 716480.0 728376.0 740272.0 752168.0 764064.0 775960.0 787856.0 799752.0
# CHECK: 731840.0 743992.0 756144.0 768296.0 780448.0 792600.0 804752.0 816904.0
# CHECK: 747200.0 759608.0 772016.0 784424.0 796832.0 809240.0 821648.0 834056.0
# CHECK: 762560.0 775224.0 787888.0 800552.0 813216.0 825880.0 838544.0 851208.0
# CHECK: 777920.0 790840.0 803760.0 816680.0 829600.0 842520.0 855440.0 868360.0
# CHECK: 793280.0 806456.0 819632.0 832808.0 845984.0 859160.0 872336.0 885512.0
# CHECK: 808640.0 822072.0 835504.0 848936.0 862368.0 875800.0 889232.0 902664.0
# CHECK: 824000.0 837688.0 851376.0 865064.0 878752.0 892440.0 906128.0 919816.0
# CHECK: 839360.0 853304.0 867248.0 881192.0 895136.0 909080.0 923024.0 936968.0
# CHECK: 854720.0 868920.0 883120.0 897320.0 911520.0 925720.0 939920.0 954120.0
# CHECK: 870080.0 884536.0 898992.0 913448.0 927904.0 942360.0 956816.0 971272.0
# CHECK: 885440.0 900152.0 914864.0 929576.0 944288.0 959000.0 973712.0 988424.0
# CHECK: 900800.0 915768.0 930736.0 945704.0 960672.0 975640.0 990608.0 1005576.0
# CHECK: 916160.0 931384.0 946608.0 961832.0 977056.0 992280.0 1007504.0 1022728.0
# CHECK: 931520.0 947000.0 962480.0 977960.0 993440.0 1008920.0 1024400.0 1039880.0
# CHECK: 946880.0 962616.0 978352.0 994088.0 1009824.0 1025560.0 1041296.0 1057032.0
# CHECK: 962240.0 978232.0 994224.0 1010216.0 1026208.0 1042200.0 1058192.0 1074184.0
# CHECK: 977600.0 993848.0 1010096.0 1026344.0 1042592.0 1058840.0 1075088.0 1091336.0


def wgmma_tf32_tf32_f32_64x8x8_inst_64x8x16(ctx: DeviceContext):
    print("== wgmma_tf32_tf32_f32_64x8x8_inst_64x8x16")
    alias M = 64
    alias N = 8
    alias K = 16

    var lhs = ManagedLayoutTensor[DType.float32, Layout.row_major(M, K)](ctx)
    arange(lhs.tensor())

    var rhs = ManagedLayoutTensor[DType.float32, Layout.row_major(K, N)](ctx)
    arange(rhs.tensor())

    var res = ManagedLayoutTensor[DType.float32, Layout.row_major(M, N)](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N8-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(4, 2)),
        IntTuple(IntTuple(4, 32), IntTuple(1, 256)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N8-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(4, 2), 8), IntTuple(IntTuple(1, 32), 4)
    )
    alias kernel = wgmma_tf32_tf32_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        8,
        a_smem_layout,
        b_smem_layout,
    ]
    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = lhs^
    _ = rhs^
    _ = res^


fn wgmma_bf16_bf16_f32_kernel[
    M: Int,
    N: Int,
    K: Int,
    WMMA_M: Int,
    WMMA_N: Int,
    WMMA_K: Int,
    a_smem_layout: Layout,
    b_smem_layout: Layout,
](
    a_gmem: LayoutTensor[
        DType.bfloat16, Layout.row_major(M, K), MutableAnyOrigin
    ],
    b_gmem: LayoutTensor[
        DType.bfloat16, Layout.col_major(N, K), MutableAnyOrigin
    ],
    result_c: LayoutTensor[
        DType.float32, Layout.row_major(M, N), MutableAnyOrigin
    ],
):
    var a_smem_tile = LayoutTensor[
        DType.bfloat16,
        a_smem_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var b_smem_tile = LayoutTensor[
        DType.bfloat16,
        b_smem_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var c_reg = SIMD[DType.float32, 4](0)

    for k_i in range(K // WMMA_K):
        var a_gmem_tile = a_gmem.tile[M, WMMA_K](0, k_i)
        var b_gmem_tile = b_gmem.tile[N, WMMA_K](0, k_i)

        if thread_idx.x == 0:
            a_smem_tile.copy_from(a_gmem_tile)
            b_smem_tile.copy_from(b_gmem_tile)

        barrier()

        var mat_a_desc = _lhs_descriptor(a_smem_tile)
        var mat_b_desc = _rhs_descriptor[transposed=False](b_smem_tile)

        wgmma_fence_aligned()

        c_reg = wgmma_async[
            WMMA_M,
            WMMA_N,
            WMMA_K,
            a_type = DType.bfloat16,
            b_type = DType.bfloat16,
            layout_b="row",
        ](mat_a_desc, mat_b_desc, c_reg)
        wgmma_commit_group_sync()
        wgmma_wait_group_sync()

    var warp_id = thread_idx.x // 32
    var lan_id = thread_idx.x % 32
    # Refer to this layout:
    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N16-D.png
    # Each warp updates a 16x8 tile, and within each tile,
    # every thread updates a 1x2 vector. The resulting distribution layout
    # is as follows:
    var th_local_res = (
        result_c.tile[16, 8](warp_id, 0)
        .vectorize[1, 2]()
        .distribute[Layout.row_major(8, 4)](lan_id)
    )
    th_local_res[0][0] = c_reg[0]
    th_local_res[0][1] = c_reg[1]
    th_local_res[1][0] = c_reg[2]
    th_local_res[1][1] = c_reg[3]


# CHECK-LABEL: wgmma_bf16_bf16_f32_64x8x16
# CHECK: 9920.0 10040.0 10160.0 10280.0 10400.0 10520.0 10640.0 10760.0
# CHECK: 25280.0 25656.0 26032.0 26408.0 26784.0 27160.0 27536.0 27912.0
# CHECK: 40640.0 41272.0 41904.0 42536.0 43168.0 43800.0 44432.0 45064.0
# CHECK: 56000.0 56888.0 57776.0 58664.0 59552.0 60440.0 61328.0 62216.0
# CHECK: 71360.0 72504.0 73648.0 74792.0 75936.0 77080.0 78224.0 79368.0
# CHECK: 86720.0 88120.0 89520.0 90920.0 92320.0 93720.0 95120.0 96520.0
# CHECK: 102080.0 103736.0 105392.0 107048.0 108704.0 110360.0 112016.0 113672.0
# CHECK: 117440.0 119352.0 121264.0 123176.0 125088.0 127000.0 128912.0 130824.0
# CHECK: 132800.0 134968.0 137136.0 139304.0 141472.0 143640.0 145808.0 147976.0
# CHECK: 148160.0 150584.0 153008.0 155432.0 157856.0 160280.0 162704.0 165128.0
# CHECK: 163520.0 166200.0 168880.0 171560.0 174240.0 176920.0 179600.0 182280.0
# CHECK: 178880.0 181816.0 184752.0 187688.0 190624.0 193560.0 196496.0 199432.0
# CHECK: 194240.0 197432.0 200624.0 203816.0 207008.0 210200.0 213392.0 216584.0
# CHECK: 209600.0 213048.0 216496.0 219944.0 223392.0 226840.0 230288.0 233736.0
# CHECK: 224960.0 228664.0 232368.0 236072.0 239776.0 243480.0 247184.0 250888.0
# CHECK: 240320.0 244280.0 248240.0 252200.0 256160.0 260120.0 264080.0 268040.0
# CHECK: 255744.0 259960.0 264176.0 268392.0 272608.0 276824.0 281040.0 285256.0
# CHECK: 271104.0 275576.0 280048.0 284520.0 288992.0 293464.0 297936.0 302408.0
# CHECK: 286464.0 291192.0 295920.0 300648.0 305376.0 310104.0 314832.0 319560.0
# CHECK: 301824.0 306808.0 311792.0 316776.0 321760.0 326744.0 331728.0 336712.0
# CHECK: 317184.0 322424.0 327664.0 332904.0 338144.0 343384.0 348624.0 353864.0
# CHECK: 332544.0 338040.0 343536.0 349032.0 354528.0 360024.0 365520.0 371016.0
# CHECK: 347904.0 353656.0 359408.0 365160.0 370912.0 376664.0 382416.0 388168.0
# CHECK: 363264.0 369272.0 375280.0 381288.0 387296.0 393304.0 399312.0 405320.0
# CHECK: 378624.0 384888.0 391152.0 397416.0 403680.0 409944.0 416208.0 422472.0
# CHECK: 393984.0 400504.0 407024.0 413544.0 420064.0 426584.0 433104.0 439624.0
# CHECK: 409344.0 416120.0 422896.0 429672.0 436448.0 443224.0 450000.0 456776.0
# CHECK: 424704.0 431736.0 438768.0 445800.0 452832.0 459864.0 466896.0 473928.0
# CHECK: 440064.0 447352.0 454640.0 461928.0 469216.0 476504.0 483792.0 491080.0
# CHECK: 455424.0 462968.0 470512.0 478056.0 485600.0 493144.0 500688.0 508232.0
# CHECK: 470784.0 478584.0 486384.0 494184.0 501984.0 509784.0 517584.0 525384.0
# CHECK: 486144.0 494200.0 502256.0 510312.0 518368.0 526424.0 534480.0 542536.0
# CHECK: 501632.0 509944.0 518256.0 526568.0 534880.0 543192.0 551504.0 559816.0
# CHECK: 516992.0 525560.0 534128.0 542696.0 551264.0 559832.0 568400.0 576968.0
# CHECK: 532352.0 541176.0 550000.0 558824.0 567648.0 576472.0 585296.0 594120.0
# CHECK: 547712.0 556792.0 565872.0 574952.0 584032.0 593112.0 602192.0 611272.0
# CHECK: 563072.0 572408.0 581744.0 591080.0 600416.0 609752.0 619088.0 628424.0
# CHECK: 578432.0 588024.0 597616.0 607208.0 616800.0 626392.0 635984.0 645576.0
# CHECK: 593792.0 603640.0 613488.0 623336.0 633184.0 643032.0 652880.0 662728.0
# CHECK: 609152.0 619256.0 629360.0 639464.0 649568.0 659672.0 669776.0 679880.0
# CHECK: 624512.0 634872.0 645232.0 655592.0 665952.0 676312.0 686672.0 697032.0
# CHECK: 639872.0 650488.0 661104.0 671720.0 682336.0 692952.0 703568.0 714184.0
# CHECK: 655232.0 666104.0 676976.0 687848.0 698720.0 709592.0 720464.0 731336.0
# CHECK: 670592.0 681720.0 692848.0 703976.0 715104.0 726232.0 737360.0 748488.0
# CHECK: 685952.0 697336.0 708720.0 720104.0 731488.0 742872.0 754256.0 765640.0
# CHECK: 701312.0 712952.0 724592.0 736232.0 747872.0 759512.0 771152.0 782792.0
# CHECK: 716672.0 728568.0 740464.0 752360.0 764256.0 776152.0 788048.0 799944.0
# CHECK: 732032.0 744184.0 756336.0 768488.0 780640.0 792792.0 804944.0 817096.0
# CHECK: 747392.0 759800.0 772208.0 784616.0 797024.0 809432.0 821840.0 834248.0
# CHECK: 762752.0 775416.0 788080.0 800744.0 813408.0 826072.0 838736.0 851400.0
# CHECK: 778112.0 791032.0 803952.0 816872.0 829792.0 842712.0 855632.0 868552.0
# CHECK: 793472.0 806648.0 819824.0 833000.0 846176.0 859352.0 872528.0 885704.0
# CHECK: 808832.0 822264.0 835696.0 849128.0 862560.0 875992.0 889424.0 902856.0
# CHECK: 824192.0 837880.0 851568.0 865256.0 878944.0 892632.0 906320.0 920008.0
# CHECK: 839552.0 853496.0 867440.0 881384.0 895328.0 909272.0 923216.0 937160.0
# CHECK: 854912.0 869112.0 883312.0 897512.0 911712.0 925912.0 940112.0 954312.0
# CHECK: 870272.0 884728.0 899184.0 913640.0 928096.0 942552.0 957008.0 971464.0
# CHECK: 885632.0 900344.0 915056.0 929768.0 944480.0 959192.0 973904.0 988616.0
# CHECK: 900992.0 915960.0 930928.0 945896.0 960864.0 975832.0 990800.0 1005768.0
# CHECK: 916352.0 931576.0 946800.0 962024.0 977248.0 992472.0 1007696.0 1022920.0
# CHECK: 931712.0 947192.0 962672.0 978152.0 993632.0 1009112.0 1024592.0 1040072.0
# CHECK: 947072.0 962808.0 978544.0 994280.0 1010016.0 1025752.0 1041488.0 1057224.0
# CHECK: 962432.0 978424.0 994416.0 1010408.0 1026400.0 1042392.0 1058384.0 1074376.0
# CHECK: 977792.0 994040.0 1010288.0 1026536.0 1042784.0 1059032.0 1075280.0 1091528.0
def wgmma_bf16_bf16_f32_64x8x16(ctx: DeviceContext):
    print("== wgmma_bf16_bf16_f32_64x8x16")
    alias M = 64
    alias N = 8
    alias K = 16

    var lhs = ManagedLayoutTensor[DType.bfloat16, Layout.row_major(M, K)](ctx)
    arange(lhs.tensor())

    var rhs = ManagedLayoutTensor[DType.bfloat16, Layout.col_major(N, K)](ctx)
    _arange_2d_col_major_tensor(rhs.tensor[update=False]())

    var res = ManagedLayoutTensor[DType.float32, Layout.row_major(M, N)](ctx)

    alias a_smem_layout = tile_layout_k_major[DType.bfloat16, BM=M, BK=16]()
    alias b_smem_layout = tile_layout_mn_major[
        DType.bfloat16, mn_dim=N, k_dim=K
    ]()

    alias kernel = wgmma_bf16_bf16_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        16,
        a_smem_layout,
        b_smem_layout,
    ]

    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = lhs^
    _ = rhs^
    _ = res^


# CHECK-LABEL: wgmma_bf16_bf16_f32_64x8x16_inst_64x8x32
# CHECK: 83328.0 83824.0 84320.0 84816.0 85312.0 85808.0 86304.0 86800.0
# CHECK: 210304.0 211824.0 213344.0 214864.0 216384.0 217904.0 219424.0 220944.0
# CHECK: 337280.0 339824.0 342368.0 344912.0 347456.0 350000.0 352544.0 355088.0
# CHECK: 464256.0 467824.0 471392.0 474960.0 478528.0 482096.0 485664.0 489232.0
# CHECK: 591232.0 595824.0 600416.0 605008.0 609600.0 614192.0 618784.0 623376.0
# CHECK: 718208.0 723824.0 729440.0 735056.0 740672.0 746288.0 751904.0 757520.0
# CHECK: 845184.0 851824.0 858464.0 865104.0 871744.0 878384.0 885024.0 891664.0
# CHECK: 972160.0 979824.0 987488.0 995152.0 1002816.0 1010480.0 1018144.0 1025808.0
# CHECK: 1099264.0 1107952.0 1116640.0 1125328.0 1134016.0 1142704.0 1151392.0 1160080.0
# CHECK: 1226240.0 1235952.0 1245664.0 1255376.0 1265088.0 1274800.0 1284512.0 1294224.0
# CHECK: 1353216.0 1363952.0 1374688.0 1385424.0 1396160.0 1406896.0 1417632.0 1428368.0
# CHECK: 1480192.0 1491952.0 1503712.0 1515472.0 1527232.0 1538992.0 1550752.0 1562512.0
# CHECK: 1607168.0 1619952.0 1632736.0 1645520.0 1658304.0 1671088.0 1683872.0 1696656.0
# CHECK: 1734144.0 1747952.0 1761760.0 1775568.0 1789376.0 1803184.0 1816992.0 1830800.0
# CHECK: 1861120.0 1875952.0 1890784.0 1905616.0 1920448.0 1935280.0 1950112.0 1964944.0
# CHECK: 1988096.0 2003952.0 2019808.0 2035664.0 2051520.0 2067376.0 2083232.0 2099088.0
# CHECK: 2115328.0 2132208.0 2149088.0 2165968.0 2182848.0 2199728.0 2216608.0 2233488.0
# CHECK: 2242304.0 2260208.0 2278112.0 2296016.0 2313920.0 2331824.0 2349728.0 2367632.0
# CHECK: 2369280.0 2388208.0 2407136.0 2426064.0 2444992.0 2463920.0 2482848.0 2501776.0
# CHECK: 2496256.0 2516208.0 2536160.0 2556112.0 2576064.0 2596016.0 2615968.0 2635920.0
# CHECK: 2623232.0 2644208.0 2665184.0 2686160.0 2707136.0 2728112.0 2749088.0 2770064.0
# CHECK: 2750208.0 2772208.0 2794208.0 2816208.0 2838208.0 2860208.0 2882208.0 2904208.0
# CHECK: 2877184.0 2900208.0 2923232.0 2946256.0 2969280.0 2992304.0 3015328.0 3038352.0
# CHECK: 3004160.0 3028208.0 3052256.0 3076304.0 3100352.0 3124400.0 3148448.0 3172496.0
# CHECK: 3131136.0 3156208.0 3181280.0 3206352.0 3231424.0 3256496.0 3281568.0 3306640.0
# CHECK: 3258112.0 3284208.0 3310304.0 3336400.0 3362496.0 3388592.0 3414688.0 3440784.0
# CHECK: 3385088.0 3412208.0 3439328.0 3466448.0 3493568.0 3520688.0 3547808.0 3574928.0
# CHECK: 3512064.0 3540208.0 3568352.0 3596496.0 3624640.0 3652784.0 3680928.0 3709072.0
# CHECK: 3639040.0 3668208.0 3697376.0 3726544.0 3755712.0 3784880.0 3814048.0 3843216.0
# CHECK: 3766016.0 3796208.0 3826400.0 3856592.0 3886784.0 3916976.0 3947168.0 3977360.0
# CHECK: 3892992.0 3924208.0 3955424.0 3986640.0 4017856.0 4049072.0 4080288.0 4111504.0
# CHECK: 4019968.0 4052208.0 4084448.0 4116688.0 4148928.0 4181168.0 4213408.0 4245648.0
# CHECK: 4147712.0 4180976.0 4214240.0 4247504.0 4280768.0 4314032.0 4347296.0 4380560.0
# CHECK: 4274688.0 4308976.0 4343264.0 4377552.0 4411840.0 4446128.0 4480416.0 4514704.0
# CHECK: 4401664.0 4436976.0 4472288.0 4507600.0 4542912.0 4578224.0 4613536.0 4648848.0
# CHECK: 4528640.0 4564976.0 4601312.0 4637648.0 4673984.0 4710320.0 4746656.0 4782992.0
# CHECK: 4655616.0 4692976.0 4730336.0 4767696.0 4805056.0 4842416.0 4879776.0 4917136.0
# CHECK: 4782592.0 4820976.0 4859360.0 4897744.0 4936128.0 4974512.0 5012896.0 5051280.0
# CHECK: 4909568.0 4948976.0 4988384.0 5027792.0 5067200.0 5106608.0 5146016.0 5185424.0
# CHECK: 5036544.0 5076976.0 5117408.0 5157840.0 5198272.0 5238704.0 5279136.0 5319568.0
# CHECK: 5163520.0 5204976.0 5246432.0 5287888.0 5329344.0 5370800.0 5412256.0 5453712.0
# CHECK: 5290496.0 5332976.0 5375456.0 5417936.0 5460416.0 5502896.0 5545376.0 5587856.0
# CHECK: 5417472.0 5460976.0 5504480.0 5547984.0 5591488.0 5634992.0 5678496.0 5722000.0
# CHECK: 5544448.0 5588976.0 5633504.0 5678032.0 5722560.0 5767088.0 5811616.0 5856144.0
# CHECK: 5671424.0 5716976.0 5762528.0 5808080.0 5853632.0 5899184.0 5944736.0 5990288.0
# CHECK: 5798400.0 5844976.0 5891552.0 5938128.0 5984704.0 6031280.0 6077856.0 6124432.0
# CHECK: 5925376.0 5972976.0 6020576.0 6068176.0 6115776.0 6163376.0 6210976.0 6258576.0
# CHECK: 6052352.0 6100976.0 6149600.0 6198224.0 6246848.0 6295472.0 6344096.0 6392720.0
# CHECK: 6179328.0 6228976.0 6278624.0 6328272.0 6377920.0 6427568.0 6477216.0 6526864.0
# CHECK: 6306304.0 6356976.0 6407648.0 6458320.0 6508992.0 6559664.0 6610336.0 6661008.0
# CHECK: 6433280.0 6484976.0 6536672.0 6588368.0 6640064.0 6691760.0 6743456.0 6795152.0
# CHECK: 6560256.0 6612976.0 6665696.0 6718416.0 6771136.0 6823856.0 6876576.0 6929296.0
# CHECK: 6687232.0 6740976.0 6794720.0 6848464.0 6902208.0 6955952.0 7009696.0 7063440.0
# CHECK: 6814208.0 6868976.0 6923744.0 6978512.0 7033280.0 7088048.0 7142816.0 7197584.0
# CHECK: 6941184.0 6996976.0 7052768.0 7108560.0 7164352.0 7220144.0 7275936.0 7331728.0
# CHECK: 7068160.0 7124976.0 7181792.0 7238608.0 7295424.0 7352240.0 7409056.0 7465872.0
# CHECK: 7195136.0 7252976.0 7310816.0 7368656.0 7426496.0 7484336.0 7542176.0 7600016.0
# CHECK: 7322112.0 7380976.0 7439840.0 7498704.0 7557568.0 7616432.0 7675296.0 7734160.0
# CHECK: 7449088.0 7508976.0 7568864.0 7628752.0 7688640.0 7748528.0 7808416.0 7868304.0
# CHECK: 7576064.0 7636976.0 7697888.0 7758800.0 7819712.0 7880624.0 7941536.0 8002448.0
# CHECK: 7703040.0 7764976.0 7826912.0 7888848.0 7950784.0 8012720.0 8074656.0 8136592.0
# CHECK: 7830016.0 7892976.0 7955936.0 8018896.0 8081856.0 8144816.0 8207776.0 8270736.0
# CHECK: 7956992.0 8020976.0 8084960.0 8148944.0 8212928.0 8276912.0 8340896.0 8404880.0
# CHECK: 8083968.0 8148976.0 8213984.0 8278992.0 8344000.0 8409008.0 8474016.0 8539024.0
def wgmma_bf16_bf16_f32_64x8x16_inst_64x8x32(ctx: DeviceContext):
    print("== wgmma_bf16_bf16_f32_64x8x16_inst_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[DType.bfloat16, Layout.row_major(M, K)](ctx)
    arange(lhs.tensor())

    var rhs = ManagedLayoutTensor[DType.bfloat16, Layout.col_major(N, K)](ctx)
    _arange_2d_col_major_tensor(rhs.tensor[update=False]())

    var res = ManagedLayoutTensor[DType.float32, Layout.row_major(M, N)](ctx)

    alias a_smem_layout = tile_layout_k_major[DType.bfloat16, BM=M, BK=16]()
    alias b_smem_layout = tile_layout_mn_major[
        DType.bfloat16, mn_dim=N, k_dim=16
    ]()

    alias kernel = wgmma_bf16_bf16_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        16,
        a_smem_layout,
        b_smem_layout,
    ]

    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = lhs^
    _ = rhs^
    _ = res^


fn wgmma_f16_f16_f32_kernel[
    M: Int,
    N: Int,
    K: Int,
    WMMA_M: Int,
    WMMA_N: Int,
    WMMA_K: Int,
    a_smem_layout: Layout,
    b_smem_layout: Layout,
](
    a_gmem: LayoutTensor[
        DType.float16, Layout.row_major(M, K), MutableAnyOrigin
    ],
    b_gmem: LayoutTensor[
        DType.float16, Layout.col_major(N, K), MutableAnyOrigin
    ],
    result_c: LayoutTensor[
        DType.float32, Layout.row_major(M, N), MutableAnyOrigin
    ],
):
    var a_smem_tile = LayoutTensor[
        DType.float16,
        a_smem_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var b_smem_tile = LayoutTensor[
        DType.float16,
        b_smem_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var c_reg = SIMD[DType.float32, 4](0)

    for k_i in range(K // WMMA_K):
        var a_gmem_tile = a_gmem.tile[M, WMMA_K](0, k_i)
        var b_gmem_tile = b_gmem.tile[N, WMMA_K](0, k_i)

        if thread_idx.x == 0:
            a_smem_tile.copy_from(a_gmem_tile)
            b_smem_tile.copy_from(b_gmem_tile)

        barrier()

        var mat_a_desc = _lhs_descriptor(a_smem_tile)
        var mat_b_desc = _rhs_descriptor[transposed=False](b_smem_tile)

        wgmma_fence_aligned()

        c_reg = wgmma_async[
            WMMA_M,
            WMMA_N,
            WMMA_K,
            a_type = DType.float16,
            b_type = DType.float16,
            layout_b="row",
        ](mat_a_desc, mat_b_desc, c_reg)
        wgmma_commit_group_sync()
        wgmma_wait_group_sync()

    var warp_id = thread_idx.x // 32
    var lan_id = thread_idx.x % 32
    # Refer to this layout:
    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N16-D.png
    # Each warp updates a 16x8 tile, and within each tile,
    # every thread updates a 1x2 vector. The resulting distribution layout
    # is as follows:
    var th_local_res = (
        result_c.tile[16, 8](warp_id, 0)
        .vectorize[1, 2]()
        .distribute[Layout.row_major(8, 4)](lan_id)
    )
    th_local_res[0][0] = c_reg[0]
    th_local_res[0][1] = c_reg[1]
    th_local_res[1][0] = c_reg[2]
    th_local_res[1][1] = c_reg[3]


# CHECK-LABEL: wgmma_f16_f16_f32_64x8x16
# CHECK: 9920.0 10040.0 10160.0 10280.0 10400.0 10520.0 10640.0 10760.0
# CHECK: 25280.0 25656.0 26032.0 26408.0 26784.0 27160.0 27536.0 27912.0
# CHECK: 40640.0 41272.0 41904.0 42536.0 43168.0 43800.0 44432.0 45064.0
# CHECK: 56000.0 56888.0 57776.0 58664.0 59552.0 60440.0 61328.0 62216.0
# CHECK: 71360.0 72504.0 73648.0 74792.0 75936.0 77080.0 78224.0 79368.0
# CHECK: 86720.0 88120.0 89520.0 90920.0 92320.0 93720.0 95120.0 96520.0
# CHECK: 102080.0 103736.0 105392.0 107048.0 108704.0 110360.0 112016.0 113672.0
# CHECK: 117440.0 119352.0 121264.0 123176.0 125088.0 127000.0 128912.0 130824.0
# CHECK: 132800.0 134968.0 137136.0 139304.0 141472.0 143640.0 145808.0 147976.0
# CHECK: 148160.0 150584.0 153008.0 155432.0 157856.0 160280.0 162704.0 165128.0
# CHECK: 163520.0 166200.0 168880.0 171560.0 174240.0 176920.0 179600.0 182280.0
# CHECK: 178880.0 181816.0 184752.0 187688.0 190624.0 193560.0 196496.0 199432.0
# CHECK: 194240.0 197432.0 200624.0 203816.0 207008.0 210200.0 213392.0 216584.0
# CHECK: 209600.0 213048.0 216496.0 219944.0 223392.0 226840.0 230288.0 233736.0
# CHECK: 224960.0 228664.0 232368.0 236072.0 239776.0 243480.0 247184.0 250888.0
# CHECK: 240320.0 244280.0 248240.0 252200.0 256160.0 260120.0 264080.0 268040.0
# CHECK: 255680.0 259896.0 264112.0 268328.0 272544.0 276760.0 280976.0 285192.0
# CHECK: 271040.0 275512.0 279984.0 284456.0 288928.0 293400.0 297872.0 302344.0
# CHECK: 286400.0 291128.0 295856.0 300584.0 305312.0 310040.0 314768.0 319496.0
# CHECK: 301760.0 306744.0 311728.0 316712.0 321696.0 326680.0 331664.0 336648.0
# CHECK: 317120.0 322360.0 327600.0 332840.0 338080.0 343320.0 348560.0 353800.0
# CHECK: 332480.0 337976.0 343472.0 348968.0 354464.0 359960.0 365456.0 370952.0
# CHECK: 347840.0 353592.0 359344.0 365096.0 370848.0 376600.0 382352.0 388104.0
# CHECK: 363200.0 369208.0 375216.0 381224.0 387232.0 393240.0 399248.0 405256.0
# CHECK: 378560.0 384824.0 391088.0 397352.0 403616.0 409880.0 416144.0 422408.0
# CHECK: 393920.0 400440.0 406960.0 413480.0 420000.0 426520.0 433040.0 439560.0
# CHECK: 409280.0 416056.0 422832.0 429608.0 436384.0 443160.0 449936.0 456712.0
# CHECK: 424640.0 431672.0 438704.0 445736.0 452768.0 459800.0 466832.0 473864.0
# CHECK: 440000.0 447288.0 454576.0 461864.0 469152.0 476440.0 483728.0 491016.0
# CHECK: 455360.0 462904.0 470448.0 477992.0 485536.0 493080.0 500624.0 508168.0
# CHECK: 470720.0 478520.0 486320.0 494120.0 501920.0 509720.0 517520.0 525320.0
# CHECK: 486080.0 494136.0 502192.0 510248.0 518304.0 526360.0 534416.0 542472.0
# CHECK: 501440.0 509752.0 518064.0 526376.0 534688.0 543000.0 551312.0 559624.0
# CHECK: 516800.0 525368.0 533936.0 542504.0 551072.0 559640.0 568208.0 576776.0
# CHECK: 532160.0 540984.0 549808.0 558632.0 567456.0 576280.0 585104.0 593928.0
# CHECK: 547520.0 556600.0 565680.0 574760.0 583840.0 592920.0 602000.0 611080.0
# CHECK: 562880.0 572216.0 581552.0 590888.0 600224.0 609560.0 618896.0 628232.0
# CHECK: 578240.0 587832.0 597424.0 607016.0 616608.0 626200.0 635792.0 645384.0
# CHECK: 593600.0 603448.0 613296.0 623144.0 632992.0 642840.0 652688.0 662536.0
# CHECK: 608960.0 619064.0 629168.0 639272.0 649376.0 659480.0 669584.0 679688.0
# CHECK: 624320.0 634680.0 645040.0 655400.0 665760.0 676120.0 686480.0 696840.0
# CHECK: 639680.0 650296.0 660912.0 671528.0 682144.0 692760.0 703376.0 713992.0
# CHECK: 655040.0 665912.0 676784.0 687656.0 698528.0 709400.0 720272.0 731144.0
# CHECK: 670400.0 681528.0 692656.0 703784.0 714912.0 726040.0 737168.0 748296.0
# CHECK: 685760.0 697144.0 708528.0 719912.0 731296.0 742680.0 754064.0 765448.0
# CHECK: 701120.0 712760.0 724400.0 736040.0 747680.0 759320.0 770960.0 782600.0
# CHECK: 716480.0 728376.0 740272.0 752168.0 764064.0 775960.0 787856.0 799752.0
# CHECK: 731840.0 743992.0 756144.0 768296.0 780448.0 792600.0 804752.0 816904.0
# CHECK: 747200.0 759608.0 772016.0 784424.0 796832.0 809240.0 821648.0 834056.0
# CHECK: 762560.0 775224.0 787888.0 800552.0 813216.0 825880.0 838544.0 851208.0
# CHECK: 777920.0 790840.0 803760.0 816680.0 829600.0 842520.0 855440.0 868360.0
# CHECK: 793280.0 806456.0 819632.0 832808.0 845984.0 859160.0 872336.0 885512.0
# CHECK: 808640.0 822072.0 835504.0 848936.0 862368.0 875800.0 889232.0 902664.0
# CHECK: 824000.0 837688.0 851376.0 865064.0 878752.0 892440.0 906128.0 919816.0
# CHECK: 839360.0 853304.0 867248.0 881192.0 895136.0 909080.0 923024.0 936968.0
# CHECK: 854720.0 868920.0 883120.0 897320.0 911520.0 925720.0 939920.0 954120.0
# CHECK: 870080.0 884536.0 898992.0 913448.0 927904.0 942360.0 956816.0 971272.0
# CHECK: 885440.0 900152.0 914864.0 929576.0 944288.0 959000.0 973712.0 988424.0
# CHECK: 900800.0 915768.0 930736.0 945704.0 960672.0 975640.0 990608.0 1005576.0
# CHECK: 916160.0 931384.0 946608.0 961832.0 977056.0 992280.0 1007504.0 1022728.0
# CHECK: 931520.0 947000.0 962480.0 977960.0 993440.0 1008920.0 1024400.0 1039880.0
# CHECK: 946880.0 962616.0 978352.0 994088.0 1009824.0 1025560.0 1041296.0 1057032.0
# CHECK: 962240.0 978232.0 994224.0 1010216.0 1026208.0 1042200.0 1058192.0 1074184.0
# CHECK: 977600.0 993848.0 1010096.0 1026344.0 1042592.0 1058840.0 1075088.0 1091336.0
def wgmma_f16_f16_f32_64x8x16(ctx: DeviceContext):
    print("== wgmma_f16_f16_f32_64x8x16")
    alias M = 64
    alias N = 8
    alias K = 16

    var lhs = ManagedLayoutTensor[DType.float16, Layout.row_major(M, K)](ctx)
    arange(lhs.tensor())

    var rhs = ManagedLayoutTensor[DType.float16, Layout.col_major(N, K)](ctx)
    _arange_2d_col_major_tensor(rhs.tensor[update=False]())

    var res = ManagedLayoutTensor[DType.float32, Layout.row_major(M, N)](ctx)

    alias a_smem_layout = tile_layout_k_major[DType.float16, BM=64, BK=16]()
    alias b_smem_layout = tile_layout_mn_major[
        DType.float16, mn_dim=N, k_dim=16
    ]()

    alias kernel = wgmma_f16_f16_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        16,
        a_smem_layout,
        b_smem_layout,
    ]

    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = lhs^
    _ = rhs^
    _ = res^


# CHECK-LABEL: wgmma_f16_f16_f32_64x8x16_inst_64x8x32
# CHECK: 83328.0 83824.0 84320.0 84816.0 85312.0 85808.0 86304.0 86800.0
# CHECK: 210304.0 211824.0 213344.0 214864.0 216384.0 217904.0 219424.0 220944.0
# CHECK: 337280.0 339824.0 342368.0 344912.0 347456.0 350000.0 352544.0 355088.0
# CHECK: 464256.0 467824.0 471392.0 474960.0 478528.0 482096.0 485664.0 489232.0
# CHECK: 591232.0 595824.0 600416.0 605008.0 609600.0 614192.0 618784.0 623376.0
# CHECK: 718208.0 723824.0 729440.0 735056.0 740672.0 746288.0 751904.0 757520.0
# CHECK: 845184.0 851824.0 858464.0 865104.0 871744.0 878384.0 885024.0 891664.0
# CHECK: 972160.0 979824.0 987488.0 995152.0 1002816.0 1010480.0 1018144.0 1025808.0
# CHECK: 1099136.0 1107824.0 1116512.0 1125200.0 1133888.0 1142576.0 1151264.0 1159952.0
# CHECK: 1226112.0 1235824.0 1245536.0 1255248.0 1264960.0 1274672.0 1284384.0 1294096.0
# CHECK: 1353088.0 1363824.0 1374560.0 1385296.0 1396032.0 1406768.0 1417504.0 1428240.0
# CHECK: 1480064.0 1491824.0 1503584.0 1515344.0 1527104.0 1538864.0 1550624.0 1562384.0
# CHECK: 1607040.0 1619824.0 1632608.0 1645392.0 1658176.0 1670960.0 1683744.0 1696528.0
# CHECK: 1734016.0 1747824.0 1761632.0 1775440.0 1789248.0 1803056.0 1816864.0 1830672.0
# CHECK: 1860992.0 1875824.0 1890656.0 1905488.0 1920320.0 1935152.0 1949984.0 1964816.0
# CHECK: 1987968.0 2003824.0 2019680.0 2035536.0 2051392.0 2067248.0 2083104.0 2098960.0
# CHECK: 2114944.0 2131824.0 2148704.0 2165584.0 2182464.0 2199344.0 2216224.0 2233104.0
# CHECK: 2241920.0 2259824.0 2277728.0 2295632.0 2313536.0 2331440.0 2349344.0 2367248.0
# CHECK: 2368896.0 2387824.0 2406752.0 2425680.0 2444608.0 2463536.0 2482464.0 2501392.0
# CHECK: 2495872.0 2515824.0 2535776.0 2555728.0 2575680.0 2595632.0 2615584.0 2635536.0
# CHECK: 2622848.0 2643824.0 2664800.0 2685776.0 2706752.0 2727728.0 2748704.0 2769680.0
# CHECK: 2749824.0 2771824.0 2793824.0 2815824.0 2837824.0 2859824.0 2881824.0 2903824.0
# CHECK: 2876800.0 2899824.0 2922848.0 2945872.0 2968896.0 2991920.0 3014944.0 3037968.0
# CHECK: 3003776.0 3027824.0 3051872.0 3075920.0 3099968.0 3124016.0 3148064.0 3172112.0
# CHECK: 3130752.0 3155824.0 3180896.0 3205968.0 3231040.0 3256112.0 3281184.0 3306256.0
# CHECK: 3257728.0 3283824.0 3309920.0 3336016.0 3362112.0 3388208.0 3414304.0 3440400.0
# CHECK: 3384704.0 3411824.0 3438944.0 3466064.0 3493184.0 3520304.0 3547424.0 3574544.0
# CHECK: 3511680.0 3539824.0 3567968.0 3596112.0 3624256.0 3652400.0 3680544.0 3708688.0
# CHECK: 3638656.0 3667824.0 3696992.0 3726160.0 3755328.0 3784496.0 3813664.0 3842832.0
# CHECK: 3765632.0 3795824.0 3826016.0 3856208.0 3886400.0 3916592.0 3946784.0 3976976.0
# CHECK: 3892608.0 3923824.0 3955040.0 3986256.0 4017472.0 4048688.0 4079904.0 4111120.0
# CHECK: 4019584.0 4051824.0 4084064.0 4116304.0 4148544.0 4180784.0 4213024.0 4245264.0
# CHECK: 4146560.0 4179824.0 4213088.0 4246352.0 4279616.0 4312880.0 4346144.0 4379408.0
# CHECK: 4273536.0 4307824.0 4342112.0 4376400.0 4410688.0 4444976.0 4479264.0 4513552.0
# CHECK: 4400512.0 4435824.0 4471136.0 4506448.0 4541760.0 4577072.0 4612384.0 4647696.0
# CHECK: 4527488.0 4563824.0 4600160.0 4636496.0 4672832.0 4709168.0 4745504.0 4781840.0
# CHECK: 4654464.0 4691824.0 4729184.0 4766544.0 4803904.0 4841264.0 4878624.0 4915984.0
# CHECK: 4781440.0 4819824.0 4858208.0 4896592.0 4934976.0 4973360.0 5011744.0 5050128.0
# CHECK: 4908416.0 4947824.0 4987232.0 5026640.0 5066048.0 5105456.0 5144864.0 5184272.0
# CHECK: 5035392.0 5075824.0 5116256.0 5156688.0 5197120.0 5237552.0 5277984.0 5318416.0
# CHECK: 5162368.0 5203824.0 5245280.0 5286736.0 5328192.0 5369648.0 5411104.0 5452560.0
# CHECK: 5289344.0 5331824.0 5374304.0 5416784.0 5459264.0 5501744.0 5544224.0 5586704.0
# CHECK: 5416320.0 5459824.0 5503328.0 5546832.0 5590336.0 5633840.0 5677344.0 5720848.0
# CHECK: 5543296.0 5587824.0 5632352.0 5676880.0 5721408.0 5765936.0 5810464.0 5854992.0
# CHECK: 5670272.0 5715824.0 5761376.0 5806928.0 5852480.0 5898032.0 5943584.0 5989136.0
# CHECK: 5797248.0 5843824.0 5890400.0 5936976.0 5983552.0 6030128.0 6076704.0 6123280.0
# CHECK: 5924224.0 5971824.0 6019424.0 6067024.0 6114624.0 6162224.0 6209824.0 6257424.0
# CHECK: 6051200.0 6099824.0 6148448.0 6197072.0 6245696.0 6294320.0 6342944.0 6391568.0
# CHECK: 6178176.0 6227824.0 6277472.0 6327120.0 6376768.0 6426416.0 6476064.0 6525712.0
# CHECK: 6305152.0 6355824.0 6406496.0 6457168.0 6507840.0 6558512.0 6609184.0 6659856.0
# CHECK: 6432128.0 6483824.0 6535520.0 6587216.0 6638912.0 6690608.0 6742304.0 6794000.0
# CHECK: 6559104.0 6611824.0 6664544.0 6717264.0 6769984.0 6822704.0 6875424.0 6928144.0
# CHECK: 6686080.0 6739824.0 6793568.0 6847312.0 6901056.0 6954800.0 7008544.0 7062288.0
# CHECK: 6813056.0 6867824.0 6922592.0 6977360.0 7032128.0 7086896.0 7141664.0 7196432.0
# CHECK: 6940032.0 6995824.0 7051616.0 7107408.0 7163200.0 7218992.0 7274784.0 7330576.0
# CHECK: 7067008.0 7123824.0 7180640.0 7237456.0 7294272.0 7351088.0 7407904.0 7464720.0
# CHECK: 7193984.0 7251824.0 7309664.0 7367504.0 7425344.0 7483184.0 7541024.0 7598864.0
# CHECK: 7320960.0 7379824.0 7438688.0 7497552.0 7556416.0 7615280.0 7674144.0 7733008.0
# CHECK: 7447936.0 7507824.0 7567712.0 7627600.0 7687488.0 7747376.0 7807264.0 7867152.0
# CHECK: 7574912.0 7635824.0 7696736.0 7757648.0 7818560.0 7879472.0 7940384.0 8001296.0
# CHECK: 7701888.0 7763824.0 7825760.0 7887696.0 7949632.0 8011568.0 8073504.0 8135440.0
# CHECK: 7828864.0 7891824.0 7954784.0 8017744.0 8080704.0 8143664.0 8206624.0 8269584.0
# CHECK: 7955840.0 8019824.0 8083808.0 8147792.0 8211776.0 8275760.0 8339744.0 8403728.0
# CHECK: 8082816.0 8147824.0 8212832.0 8277840.0 8342848.0 8407856.0 8472864.0 8537872.0
def wgmma_f16_f16_f32_64x8x16_inst_64x8x32(ctx: DeviceContext):
    print("== wgmma_f16_f16_f32_64x8x16_inst_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[DType.float16, Layout.row_major(M, K)](ctx)
    arange(lhs.tensor())

    var rhs = ManagedLayoutTensor[DType.float16, Layout.col_major(N, K)](ctx)
    _arange_2d_col_major_tensor(rhs.tensor[update=False]())

    var res = ManagedLayoutTensor[DType.float32, Layout.row_major(M, N)](ctx)

    alias a_smem_layout = tile_layout_k_major[DType.float16, BM=64, BK=16]()
    alias b_smem_layout = tile_layout_mn_major[
        DType.float16, mn_dim=N, k_dim=16
    ]()

    alias kernel = wgmma_f16_f16_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        16,
        a_smem_layout,
        b_smem_layout,
    ]

    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = lhs^
    _ = rhs^
    _ = res^


fn wgmma_f16_f16_f16_kernel[
    M: Int,
    N: Int,
    K: Int,
    WMMA_M: Int,
    WMMA_N: Int,
    WMMA_K: Int,
    a_smem_layout: Layout,
    b_smem_layout: Layout,
](
    a_gmem: LayoutTensor[
        DType.float16, Layout.row_major(M, K), MutableAnyOrigin
    ],
    b_gmem: LayoutTensor[
        DType.float16, Layout.row_major(K, N), MutableAnyOrigin
    ],
    result_c: LayoutTensor[
        DType.float16, Layout.row_major(M, N), MutableAnyOrigin
    ],
):
    var a_smem_tile = LayoutTensor[
        DType.float16,
        a_smem_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var b_smem_tile = LayoutTensor[
        DType.float16,
        b_smem_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var c_reg = SIMD[DType.uint32, 2](0)

    for k_i in range(K // WMMA_K):
        var a_gmem_tile = a_gmem.tile[M, WMMA_K](0, k_i)
        var b_gmem_tile = b_gmem.tile[WMMA_K, N](k_i, 0)

        if thread_idx.x == 0:
            a_smem_tile.copy_from(a_gmem_tile)
            b_smem_tile.copy_from(b_gmem_tile)

        barrier()

        var mat_a_desc = _lhs_descriptor(a_smem_tile)
        var mat_b_desc = WGMMADescriptor.create[1, 8](b_smem_tile.ptr)

        wgmma_fence_aligned()

        c_reg = wgmma_async[
            WMMA_M,
            WMMA_N,
            WMMA_K,
            a_type = DType.float16,
            b_type = DType.float16,
            accum_type = DType.float16,
        ](mat_a_desc, mat_b_desc, c_reg)
        wgmma_commit_group_sync()
        wgmma_wait_group_sync()

    var warp_id = thread_idx.x // 32
    var lan_id = thread_idx.x % 32
    # Refer to this layout:
    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N16-D.png
    # Each warp updates a 16x8 tile, and within each tile,
    # every thread updates a 1x2 vector. The resulting distribution layout
    # is as follows:
    c0 = bitcast[DType.float16, 4](c_reg)
    var th_local_res = (
        result_c.tile[16, 8](warp_id, 0)
        .vectorize[1, 2]()
        .distribute[Layout.row_major(8, 4)](lan_id)
    )
    th_local_res[0][0] = c0[0]
    th_local_res[0][1] = c0[1]
    th_local_res[1][0] = c0[2]
    th_local_res[1][1] = c0[3]


# CHECK-LABEL: wgmma_f16_f16_f16_64x8x16
# CHECK: 378.0 339.0 365.0 430.0 417.0 430.0 378.0 391.0
# CHECK: 449.0 513.0 499.0 524.0 614.0 522.0 573.0 546.0
# CHECK: 399.0 379.0 424.0 365.0 371.0 416.0 383.0 415.0
# CHECK: 459.0 531.0 525.0 558.0 513.0 442.0 501.0 482.0
# CHECK: 453.0 452.0 516.0 476.0 501.0 435.0 421.0 472.0
# CHECK: 414.0 494.0 496.0 537.0 500.0 450.0 517.0 506.0
# CHECK: 397.0 415.0 498.0 477.0 521.0 487.0 492.0 562.0
# CHECK: 457.0 402.0 412.0 461.0 432.0 403.0 478.0 475.0
# CHECK: 517.0 554.0 513.0 511.0 574.0 429.0 453.0 542.0
# CHECK: 445.0 398.0 416.0 473.0 452.0 444.0 384.0 389.0
# CHECK: 527.0 583.0 561.0 578.0 660.0 547.0 590.0 555.0
# CHECK: 378.0 339.0 365.0 430.0 417.0 430.0 378.0 391.0
# CHECK: 449.0 513.0 499.0 524.0 614.0 522.0 573.0 546.0
# CHECK: 399.0 379.0 424.0 365.0 371.0 416.0 383.0 415.0
# CHECK: 459.0 531.0 525.0 558.0 513.0 442.0 501.0 482.0
# CHECK: 453.0 452.0 516.0 476.0 501.0 435.0 421.0 472.0
# CHECK: 414.0 494.0 496.0 537.0 500.0 450.0 517.0 506.0
# CHECK: 397.0 415.0 498.0 477.0 521.0 487.0 492.0 562.0
# CHECK: 457.0 402.0 412.0 461.0 432.0 403.0 478.0 475.0
# CHECK: 517.0 554.0 513.0 511.0 574.0 429.0 453.0 542.0
# CHECK: 445.0 398.0 416.0 473.0 452.0 444.0 384.0 389.0
# CHECK: 527.0 583.0 561.0 578.0 660.0 547.0 590.0 555.0
# CHECK: 378.0 339.0 365.0 430.0 417.0 430.0 378.0 391.0
# CHECK: 449.0 513.0 499.0 524.0 614.0 522.0 573.0 546.0
# CHECK: 399.0 379.0 424.0 365.0 371.0 416.0 383.0 415.0
# CHECK: 459.0 531.0 525.0 558.0 513.0 442.0 501.0 482.0
# CHECK: 453.0 452.0 516.0 476.0 501.0 435.0 421.0 472.0
# CHECK: 414.0 494.0 496.0 537.0 500.0 450.0 517.0 506.0
# CHECK: 397.0 415.0 498.0 477.0 521.0 487.0 492.0 562.0
# CHECK: 457.0 402.0 412.0 461.0 432.0 403.0 478.0 475.0
# CHECK: 517.0 554.0 513.0 511.0 574.0 429.0 453.0 542.0
# CHECK: 445.0 398.0 416.0 473.0 452.0 444.0 384.0 389.0
# CHECK: 527.0 583.0 561.0 578.0 660.0 547.0 590.0 555.0
# CHECK: 378.0 339.0 365.0 430.0 417.0 430.0 378.0 391.0
# CHECK: 449.0 513.0 499.0 524.0 614.0 522.0 573.0 546.0
# CHECK: 399.0 379.0 424.0 365.0 371.0 416.0 383.0 415.0
# CHECK: 459.0 531.0 525.0 558.0 513.0 442.0 501.0 482.0
# CHECK: 453.0 452.0 516.0 476.0 501.0 435.0 421.0 472.0
# CHECK: 414.0 494.0 496.0 537.0 500.0 450.0 517.0 506.0
# CHECK: 397.0 415.0 498.0 477.0 521.0 487.0 492.0 562.0
# CHECK: 457.0 402.0 412.0 461.0 432.0 403.0 478.0 475.0
# CHECK: 517.0 554.0 513.0 511.0 574.0 429.0 453.0 542.0
# CHECK: 445.0 398.0 416.0 473.0 452.0 444.0 384.0 389.0
# CHECK: 527.0 583.0 561.0 578.0 660.0 547.0 590.0 555.0
# CHECK: 378.0 339.0 365.0 430.0 417.0 430.0 378.0 391.0
# CHECK: 449.0 513.0 499.0 524.0 614.0 522.0 573.0 546.0
# CHECK: 399.0 379.0 424.0 365.0 371.0 416.0 383.0 415.0
# CHECK: 459.0 531.0 525.0 558.0 513.0 442.0 501.0 482.0
# CHECK: 453.0 452.0 516.0 476.0 501.0 435.0 421.0 472.0
# CHECK: 414.0 494.0 496.0 537.0 500.0 450.0 517.0 506.0
# CHECK: 397.0 415.0 498.0 477.0 521.0 487.0 492.0 562.0
# CHECK: 457.0 402.0 412.0 461.0 432.0 403.0 478.0 475.0
# CHECK: 517.0 554.0 513.0 511.0 574.0 429.0 453.0 542.0
# CHECK: 445.0 398.0 416.0 473.0 452.0 444.0 384.0 389.0
# CHECK: 527.0 583.0 561.0 578.0 660.0 547.0 590.0 555.0
# CHECK: 378.0 339.0 365.0 430.0 417.0 430.0 378.0 391.0
# CHECK: 449.0 513.0 499.0 524.0 614.0 522.0 573.0 546.0
# CHECK: 399.0 379.0 424.0 365.0 371.0 416.0 383.0 415.0
# CHECK: 459.0 531.0 525.0 558.0 513.0 442.0 501.0 482.0
# CHECK: 453.0 452.0 516.0 476.0 501.0 435.0 421.0 472.0
# CHECK: 414.0 494.0 496.0 537.0 500.0 450.0 517.0 506.0
# CHECK: 397.0 415.0 498.0 477.0 521.0 487.0 492.0 562.0
# CHECK: 457.0 402.0 412.0 461.0 432.0 403.0 478.0 475.0
# CHECK: 517.0 554.0 513.0 511.0 574.0 429.0 453.0 542.0
def wgmma_f16_f16_f16_64x8x16(ctx: DeviceContext):
    print("== wgmma_f16_f16_f16_64x8x16")
    alias M = 64
    alias N = 8
    alias K = 16

    var lhs = ManagedLayoutTensor[DType.float16, Layout.row_major(M, K)](ctx)
    arange(lhs.tensor(), end=11)

    var rhs = ManagedLayoutTensor[DType.float16, Layout.row_major(K, N)](ctx)
    arange(rhs.tensor(), end=13)

    var res = ManagedLayoutTensor[DType.float16, Layout.row_major(M, N)](ctx)

    alias a_smem_layout = tile_layout_k_major[DType.float16, BM=64, BK=16]()

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N16-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(8, 2), 8), IntTuple(IntTuple(1, 64), 8)
    )

    alias kernel = wgmma_f16_f16_f16_kernel[
        M,
        N,
        K,
        64,
        8,
        16,
        a_smem_layout,
        b_smem_layout,
    ]

    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = lhs^
    _ = rhs^
    _ = res^


# CHECK-LABEL: wgmma_f16_f16_f16_64x8x16_inst_64x8x32
# CHECK: 1436.0 1537.0 1391.0 1414.0 1567.0 1499.0 1574.0 1389.0
# CHECK: 1560.0 1547.0 1339.0 1482.0 1521.0 1391.0 1586.0 1573.0
# CHECK: 1612.0 1719.0 1683.0 1712.0 1637.0 1679.0 1760.0 1685.0
# CHECK: 1826.0 1819.0 1721.0 1636.0 1681.0 1661.0 1628.0 1725.0
# CHECK: 1500.0 1577.0 1615.0 1614.0 1743.0 1625.0 1676.0 1675.0
# CHECK: 1390.0 1551.0 1491.0 1574.0 1553.0 1337.0 1472.0 1373.0
# CHECK: 1478.0 1525.0 1403.0 1606.0 1705.0 1661.0 1682.0 1521.0
# CHECK: 1728.0 1661.0 1711.0 1800.0 1785.0 1679.0 1586.0 1597.0
# CHECK: 1672.0 1725.0 1713.0 1688.0 1793.0 1859.0 1886.0 1835.0
# CHECK: 1436.0 1537.0 1391.0 1414.0 1567.0 1499.0 1574.0 1389.0
# CHECK: 1560.0 1547.0 1339.0 1482.0 1521.0 1391.0 1586.0 1573.0
# CHECK: 1612.0 1719.0 1683.0 1712.0 1637.0 1679.0 1760.0 1685.0
# CHECK: 1826.0 1819.0 1721.0 1636.0 1681.0 1661.0 1628.0 1725.0
# CHECK: 1500.0 1577.0 1615.0 1614.0 1743.0 1625.0 1676.0 1675.0
# CHECK: 1390.0 1551.0 1491.0 1574.0 1553.0 1337.0 1472.0 1373.0
# CHECK: 1478.0 1525.0 1403.0 1606.0 1705.0 1661.0 1682.0 1521.0
# CHECK: 1728.0 1661.0 1711.0 1800.0 1785.0 1679.0 1586.0 1597.0
# CHECK: 1672.0 1725.0 1713.0 1688.0 1793.0 1859.0 1886.0 1835.0
# CHECK: 1436.0 1537.0 1391.0 1414.0 1567.0 1499.0 1574.0 1389.0
# CHECK: 1560.0 1547.0 1339.0 1482.0 1521.0 1391.0 1586.0 1573.0
# CHECK: 1612.0 1719.0 1683.0 1712.0 1637.0 1679.0 1760.0 1685.0
# CHECK: 1826.0 1819.0 1721.0 1636.0 1681.0 1661.0 1628.0 1725.0
# CHECK: 1500.0 1577.0 1615.0 1614.0 1743.0 1625.0 1676.0 1675.0
# CHECK: 1390.0 1551.0 1491.0 1574.0 1553.0 1337.0 1472.0 1373.0
# CHECK: 1478.0 1525.0 1403.0 1606.0 1705.0 1661.0 1682.0 1521.0
# CHECK: 1728.0 1661.0 1711.0 1800.0 1785.0 1679.0 1586.0 1597.0
# CHECK: 1672.0 1725.0 1713.0 1688.0 1793.0 1859.0 1886.0 1835.0
# CHECK: 1436.0 1537.0 1391.0 1414.0 1567.0 1499.0 1574.0 1389.0
# CHECK: 1560.0 1547.0 1339.0 1482.0 1521.0 1391.0 1586.0 1573.0
# CHECK: 1612.0 1719.0 1683.0 1712.0 1637.0 1679.0 1760.0 1685.0
# CHECK: 1826.0 1819.0 1721.0 1636.0 1681.0 1661.0 1628.0 1725.0
# CHECK: 1500.0 1577.0 1615.0 1614.0 1743.0 1625.0 1676.0 1675.0
# CHECK: 1390.0 1551.0 1491.0 1574.0 1553.0 1337.0 1472.0 1373.0
# CHECK: 1478.0 1525.0 1403.0 1606.0 1705.0 1661.0 1682.0 1521.0
# CHECK: 1728.0 1661.0 1711.0 1800.0 1785.0 1679.0 1586.0 1597.0
# CHECK: 1672.0 1725.0 1713.0 1688.0 1793.0 1859.0 1886.0 1835.0
# CHECK: 1436.0 1537.0 1391.0 1414.0 1567.0 1499.0 1574.0 1389.0
# CHECK: 1560.0 1547.0 1339.0 1482.0 1521.0 1391.0 1586.0 1573.0
# CHECK: 1612.0 1719.0 1683.0 1712.0 1637.0 1679.0 1760.0 1685.0
# CHECK: 1826.0 1819.0 1721.0 1636.0 1681.0 1661.0 1628.0 1725.0
# CHECK: 1500.0 1577.0 1615.0 1614.0 1743.0 1625.0 1676.0 1675.0
# CHECK: 1390.0 1551.0 1491.0 1574.0 1553.0 1337.0 1472.0 1373.0
# CHECK: 1478.0 1525.0 1403.0 1606.0 1705.0 1661.0 1682.0 1521.0
# CHECK: 1728.0 1661.0 1711.0 1800.0 1785.0 1679.0 1586.0 1597.0
# CHECK: 1672.0 1725.0 1713.0 1688.0 1793.0 1859.0 1886.0 1835.0
# CHECK: 1436.0 1537.0 1391.0 1414.0 1567.0 1499.0 1574.0 1389.0
# CHECK: 1560.0 1547.0 1339.0 1482.0 1521.0 1391.0 1586.0 1573.0
# CHECK: 1612.0 1719.0 1683.0 1712.0 1637.0 1679.0 1760.0 1685.0
# CHECK: 1826.0 1819.0 1721.0 1636.0 1681.0 1661.0 1628.0 1725.0
# CHECK: 1500.0 1577.0 1615.0 1614.0 1743.0 1625.0 1676.0 1675.0
# CHECK: 1390.0 1551.0 1491.0 1574.0 1553.0 1337.0 1472.0 1373.0
# CHECK: 1478.0 1525.0 1403.0 1606.0 1705.0 1661.0 1682.0 1521.0
# CHECK: 1728.0 1661.0 1711.0 1800.0 1785.0 1679.0 1586.0 1597.0
# CHECK: 1672.0 1725.0 1713.0 1688.0 1793.0 1859.0 1886.0 1835.0
# CHECK: 1436.0 1537.0 1391.0 1414.0 1567.0 1499.0 1574.0 1389.0
# CHECK: 1560.0 1547.0 1339.0 1482.0 1521.0 1391.0 1586.0 1573.0
# CHECK: 1612.0 1719.0 1683.0 1712.0 1637.0 1679.0 1760.0 1685.0
# CHECK: 1826.0 1819.0 1721.0 1636.0 1681.0 1661.0 1628.0 1725.0
# CHECK: 1500.0 1577.0 1615.0 1614.0 1743.0 1625.0 1676.0 1675.0
# CHECK: 1390.0 1551.0 1491.0 1574.0 1553.0 1337.0 1472.0 1373.0
# CHECK: 1478.0 1525.0 1403.0 1606.0 1705.0 1661.0 1682.0 1521.0
# CHECK: 1728.0 1661.0 1711.0 1800.0 1785.0 1679.0 1586.0 1597.0
# CHECK: 1672.0 1725.0 1713.0 1688.0 1793.0 1859.0 1886.0 1835.0
# CHECK: 1436.0 1537.0 1391.0 1414.0 1567.0 1499.0 1574.0 1389.0
def wgmma_f16_f16_f16_64x8x16_inst_64x8x32(ctx: DeviceContext):
    print("== wgmma_f16_f16_f16_64x8x16_inst_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[DType.float16, Layout.row_major(M, K)](ctx)
    arange(lhs.tensor(), end=18)

    var rhs = ManagedLayoutTensor[DType.float16, Layout.row_major(K, N)](ctx)
    arange(rhs.tensor(), end=13)

    var res = ManagedLayoutTensor[DType.float16, Layout.row_major(M, N)](ctx)

    alias a_smem_layout = tile_layout_k_major[DType.float16, BM=64, BK=16]()

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N16-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(8, 2), 8), IntTuple(IntTuple(1, 64), 8)
    )

    alias kernel = wgmma_f16_f16_f16_kernel[
        M,
        N,
        K,
        64,
        8,
        16,
        a_smem_layout,
        b_smem_layout,
    ]
    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = lhs^
    _ = rhs^
    _ = res^


fn wgmma_kernel[
    a_type: DType,
    b_type: DType,
    c_type: DType,
    a_layout: Layout,
    b_layout: Layout,
    c_layout: Layout,
    WMMA_M: Int,
    WMMA_N: Int,
    WMMA_K: Int,
    a_smem_layout: Layout,
    b_smem_layout: Layout,
    transpose_b: Bool = False,
](
    a_gmem: LayoutTensor[a_type, a_layout, MutableAnyOrigin],
    b_gmem: LayoutTensor[b_type, b_layout, MutableAnyOrigin],
    c_gmem: LayoutTensor[c_type, c_layout, MutableAnyOrigin],
):
    var a_smem_tile = LayoutTensor[
        DType.bfloat16,
        a_smem_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var b_smem_tile = LayoutTensor[
        DType.bfloat16,
        b_smem_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var c_reg = SIMD[DType.float32, 4](0)

    alias M = a_layout.shape[0].value()
    alias K = a_layout.shape[1].value()
    alias N = c_layout.shape[1].value()

    alias b_tile_dim0 = N if transpose_b else WMMA_K
    alias b_tile_dim1 = WMMA_K if transpose_b else N

    for k_i in range(K // WMMA_K):
        var a_gmem_tile = a_gmem.tile[M, WMMA_K](0, k_i)

        var b_tile_coord0 = 0 if transpose_b else k_i
        var b_tile_coord1 = k_i if transpose_b else 0
        var b_gmem_tile = b_gmem.tile[b_tile_dim0, b_tile_dim1](
            b_tile_coord0, b_tile_coord1
        )

        if thread_idx.x == 0:
            a_smem_tile.copy_from(a_gmem_tile)
            b_smem_tile.copy_from(b_gmem_tile)

        barrier()

        var mat_a_desc = _lhs_descriptor(a_smem_tile)
        var mat_b_desc = _rhs_descriptor[transpose_b](b_smem_tile)

        wgmma_fence_aligned()

        c_reg = wgmma_async[
            WMMA_M,
            WMMA_N,
            WMMA_K,
            a_type = DType.bfloat16,
            b_type = DType.bfloat16,
        ](mat_a_desc, mat_b_desc, c_reg)
        wgmma_commit_group_sync()
        wgmma_wait_group_sync()

    var warp_id = thread_idx.x // 32
    var lan_id = thread_idx.x % 32
    var th_local_res = (
        c_gmem.tile[16, 8](warp_id, 0)
        .vectorize[1, 2]()
        .distribute[Layout.row_major(8, 4)](lan_id)
    )
    th_local_res[0][0] = c_reg[0].cast[c_gmem.dtype]()
    th_local_res[0][1] = c_reg[1].cast[c_gmem.dtype]()
    th_local_res[1][0] = c_reg[2].cast[c_gmem.dtype]()
    th_local_res[1][1] = c_reg[3].cast[c_gmem.dtype]()


# CHECK-LABEL: wgmma_bf16_bf16_f32_64x8x16_transb_64x8x32
# CHECK: 10432.0 26240.0 42240.0 58112.0 73728.0 89600.0 105472.0 121344.0
# CHECK: 26240.0 74752.0 123392.0 172032.0 221184.0 270336.0 317440.0 366592.0
# CHECK: 42240.0 123392.0 204800.0 286720.0 368640.0 448512.0 532480.0 610304.0
# CHECK: 58112.0 172032.0 286720.0 401408.0 514048.0 630784.0 741376.0 856064.0
# CHECK: 73728.0 221184.0 368640.0 514048.0 663552.0 806912.0 954368.0 1105920.0
# CHECK: 89600.0 270336.0 448512.0 630784.0 806912.0 987136.0 1171456.0 1351680.0
# CHECK: 105472.0 317440.0 532480.0 741376.0 954368.0 1171456.0 1384448.0 1589248.0
# CHECK: 121344.0 366592.0 610304.0 856064.0 1105920.0 1351680.0 1589248.0 1835008.0
# CHECK: 137216.0 415744.0 692224.0 970752.0 1253376.0 1523712.0 1802240.0 2080768.0
# CHECK: 153600.0 464896.0 774144.0 1089536.0 1392640.0 1703936.0 2015232.0 2326528.0
# CHECK: 168960.0 512000.0 856064.0 1196032.0 1540096.0 1884160.0 2228224.0 2572288.0
# CHECK: 185344.0 561152.0 937984.0 1310720.0 1687552.0 2064384.0 2441216.0 2818048.0
# CHECK: 200704.0 610304.0 1019904.0 1425408.0 1835008.0 2244608.0 2654208.0 3063808.0
# CHECK: 217088.0 659456.0 1097728.0 1540096.0 1982464.0 2424832.0 2867200.0 3309568.0
# CHECK: 232448.0 708608.0 1179648.0 1654784.0 2129920.0 2605056.0 3080192.0 3555328.0
# CHECK: 248832.0 757760.0 1261568.0 1769472.0 2277376.0 2785280.0 3293184.0 3801088.0
# CHECK: 264192.0 802816.0 1343488.0 1884160.0 2424832.0 2965504.0 3506176.0 4046848.0
# CHECK: 280576.0 851968.0 1425408.0 1998848.0 2572288.0 3145728.0 3719168.0 4292608.0
# CHECK: 296960.0 901120.0 1507328.0 2113536.0 2719744.0 3325952.0 3932160.0 4521984.0
# CHECK: 311296.0 950272.0 1589248.0 2228224.0 2867200.0 3506176.0 4145152.0 4784128.0
# CHECK: 327680.0 999424.0 1671168.0 2342912.0 3014656.0 3686400.0 4358144.0 5013504.0
# CHECK: 344064.0 1048576.0 1753088.0 2457600.0 3162112.0 3866624.0 4554752.0 5275648.0
# CHECK: 360448.0 1097728.0 1835008.0 2572288.0 3309568.0 4046848.0 4784128.0 5505024.0
# CHECK: 374784.0 1146880.0 1916928.0 2686976.0 3457024.0 4227072.0 4980736.0 5767168.0
# CHECK: 391168.0 1196032.0 1998848.0 2801664.0 3604480.0 4390912.0 5210112.0 5996544.0
# CHECK: 407552.0 1245184.0 2080768.0 2916352.0 3751936.0 4587520.0 5406720.0 6258688.0
# CHECK: 423936.0 1294336.0 2162688.0 3031040.0 3899392.0 4751360.0 5636096.0 6488064.0
# CHECK: 438272.0 1343488.0 2244608.0 3145728.0 4046848.0 4947968.0 5832704.0 6750208.0
# CHECK: 454656.0 1384448.0 2326528.0 3260416.0 4194304.0 5111808.0 6062080.0 6979584.0
# CHECK: 471040.0 1433600.0 2408448.0 3375104.0 4325376.0 5308416.0 6258688.0 7241728.0
# CHECK: 487424.0 1482752.0 2490368.0 3489792.0 4489216.0 5472256.0 6488064.0 7471104.0
# CHECK: 501760.0 1531904.0 2572288.0 3604480.0 4620288.0 5668864.0 6684672.0 7733248.0
# CHECK: 518144.0 1581056.0 2654208.0 3719168.0 4784128.0 5832704.0 6914048.0 7962624.0
# CHECK: 532480.0 1630208.0 2736128.0 3833856.0 4915200.0 6029312.0 7110656.0 8224768.0
# CHECK: 548864.0 1679360.0 2818048.0 3932160.0 5079040.0 6193152.0 7340032.0 8454144.0
# CHECK: 565248.0 1728512.0 2883584.0 4046848.0 5210112.0 6389760.0 7536640.0 8716288.0
# CHECK: 581632.0 1777664.0 2965504.0 4161536.0 5373952.0 6553600.0 7766016.0 8978432.0
# CHECK: 598016.0 1826816.0 3047424.0 4292608.0 5505024.0 6750208.0 7962624.0 9175040.0
# CHECK: 614400.0 1875968.0 3129344.0 4390912.0 5668864.0 6914048.0 8192000.0 9437184.0
# CHECK: 630784.0 1925120.0 3211264.0 4521984.0 5799936.0 7110656.0 8388608.0 9699328.0
# CHECK: 647168.0 1974272.0 3293184.0 4620288.0 5963776.0 7274496.0 8585216.0 9961472.0
# CHECK: 659456.0 2023424.0 3375104.0 4751360.0 6094848.0 7471104.0 8847360.0 10158080.0
# CHECK: 675840.0 2072576.0 3457024.0 4849664.0 6258688.0 7634944.0 9043968.0 10420224.0
# CHECK: 692224.0 2113536.0 3538944.0 4980736.0 6389760.0 7831552.0 9240576.0 10682368.0
# CHECK: 708608.0 2162688.0 3620864.0 5079040.0 6553600.0 7995392.0 9437184.0 10944512.0
# CHECK: 724992.0 2211840.0 3702784.0 5210112.0 6684672.0 8192000.0 9699328.0 11141120.0
# CHECK: 741376.0 2260992.0 3784704.0 5308416.0 6848512.0 8355840.0 9895936.0 11403264.0
# CHECK: 757760.0 2310144.0 3866624.0 5439488.0 6979584.0 8519680.0 10092544.0 11665408.0
# CHECK: 774144.0 2359296.0 3948544.0 5537792.0 7143424.0 8716288.0 10289152.0 11862016.0
# CHECK: 786432.0 2408448.0 4030464.0 5668864.0 7274496.0 8912896.0 10485760.0 12124160.0
# CHECK: 802816.0 2457600.0 4112384.0 5767168.0 7405568.0 9043968.0 10747904.0 12386304.0
# CHECK: 819200.0 2506752.0 4194304.0 5865472.0 7569408.0 9240576.0 10944512.0 12648448.0
# CHECK: 835584.0 2555904.0 4259840.0 5996544.0 7700480.0 9437184.0 11141120.0 12845056.0
# CHECK: 851968.0 2605056.0 4358144.0 6094848.0 7864320.0 9633792.0 11337728.0 13107200.0
# CHECK: 868352.0 2654208.0 4423680.0 6225920.0 7995392.0 9764864.0 11599872.0 13369344.0
# CHECK: 884736.0 2703360.0 4521984.0 6324224.0 8159232.0 9961472.0 11796480.0 13631488.0
# CHECK: 901120.0 2752512.0 4587520.0 6455296.0 8290304.0 10158080.0 11993088.0 13828096.0
# CHECK: 913408.0 2801664.0 4685824.0 6553600.0 8454144.0 10354688.0 12189696.0 14090240.0
# CHECK: 929792.0 2850816.0 4751360.0 6684672.0 8585216.0 10485760.0 12451840.0 14352384.0
# CHECK: 946176.0 2899968.0 4849664.0 6782976.0 8716288.0 10682368.0 12648448.0 14614528.0
# CHECK: 962560.0 2949120.0 4915200.0 6914048.0 8912896.0 10878976.0 12845056.0 14811136.0
# CHECK: 978944.0 2998272.0 5013504.0 7012352.0 9043968.0 11075584.0 13041664.0 15073280.0
# CHECK: 995328.0 3047424.0 5079040.0 7143424.0 9175040.0 11206656.0 13303808.0 15335424.0
# CHECK: 1011712.0 3096576.0 5177344.0 7241728.0 9306112.0 11403264.0 13500416.0 15597568.0


def wgmma_bf16_bf16_f32_64x8x16_transb_64x8x32(ctx: DeviceContext):
    print("== wgmma_bf16_bf16_f32_64x8x16_transb_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.bfloat16,
        Layout.row_major(M, K),
    ](ctx)
    arange(lhs.tensor())
    var rhs = ManagedLayoutTensor[
        DType.bfloat16,
        Layout.row_major(N, K),
    ](ctx)
    arange(rhs.tensor())

    var res = ManagedLayoutTensor[
        DType.bfloat16,
        Layout.row_major(M, N),
    ](ctx)

    alias a_smem_layout = tile_layout_k_major[DType.bfloat16, BM=M, BK=16]()
    alias b_smem_layout = tile_layout_k_major[DType.bfloat16, BM=N, BK=16]()

    alias kernel = wgmma_kernel[
        DType.bfloat16,
        DType.bfloat16,
        DType.bfloat16,
        Layout.row_major(M, K),
        Layout.row_major(N, K),
        Layout.row_major(M, N),
        64,
        8,
        16,
        a_smem_layout,
        b_smem_layout,
        transpose_b=True,
    ]
    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = lhs^
    _ = rhs^
    _ = res^


def main():
    with DeviceContext() as ctx:
        wgmma_tf32_tf32_f32_64x8x8(ctx)
        wgmma_tf32_tf32_f32_64x8x8_inst_64x8x16(ctx)
        wgmma_bf16_bf16_f32_64x8x16(ctx)
        wgmma_bf16_bf16_f32_64x8x16_inst_64x8x32(ctx)
        wgmma_f16_f16_f32_64x8x16(ctx)
        wgmma_f16_f16_f32_64x8x16_inst_64x8x32(ctx)
        wgmma_f16_f16_f16_64x8x16(ctx)
        wgmma_f16_f16_f16_64x8x16_inst_64x8x32(ctx)
        wgmma_bf16_bf16_f32_64x8x16_transb_64x8x32(ctx)
