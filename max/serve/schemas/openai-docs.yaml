openapi: 3.1.0
info:
  title: MAX REST API reference
  description: |

    MAX is a high-performance, Python-based inference server for
    deploying large language models (LLMs) locally or in the cloud. It provides
    efficient request handling through advanced batching and scheduling, and
    an OpenAI-compatible REST endpoint.

    ## Get started

    With just a few commands, you can start a local endpoint with the GenAI
    model of your choice using our [`max`](/max/max-cli) CLI
    and start sending requests.

    When you want to deploy your model to a cloud-hosted endpoint, you can use
    our [MAX container](/max/container)â€”see our [Get started with
    MAX](/max/get-started) and then see our tutorials on using MAX:

    - [Start a chat endpoint](/max/tutorials/start-a-chat-endpoint)
    - [Serve an embedding model with MAX](/max/tutorials/run-embeddings-with-max-serve)
    - [Generate image descriptions with Llama 3.2 Vision](/max/tutorials/deploy-llama-vision)
    - [Deploy Llama 3 on GPU-powered Kubernetes clusters](/max/tutorials/max-serve-local-to-cloud)

    ## OpenAI API compatibility

    The MAX REST API is compatible with a subset of the [OpenAI REST
    API](https://platform.openai.com/docs/api-reference/introduction). This
    allows you to use many existing OpenAI client applications with MAX
    endpoints.

    - **Supported endpoints:**

        - [Chat completions](#operation/createChatCompletion) (`/v1/chat/completions`)
        - [Completions](#operation/createCompletion) (`/v1/completions`)
        - [Embeddings](#operation/createEmbedding) (`/v1/embeddings`)
        - [Batches](#operation/createBatch) (`/v1/batches`) - The `v1/batches` API is only available to users in the [Mammoth](/mammoth/) public preview.
        - [List models](#operation/listModels) (`/v1/models`)
        - [Health check](#operation/health) (`/health`)

    - **Parameter handling:** While aiming for high compatibility, not all
    OpenAI body parameters are implemented. Some may be accepted as no-ops
    (ignored but won't cause errors) to maintain client compatibility. The
    specific endpoint documentation below details only the parameters that
    actively affect behavior in MAX.

    In addition to the OpenAI APIs, MAX provides a Prometheus-formatted
    [metrics endpoint](/max/container#metrics) to help track your model's
    performance.

paths:
  /v1/chat/completions:
    post:
      operationId: createChatCompletion
      summary: Create chat completion
      description: |
        Creates a completion for the chat message. MAX supports a subset of
        OpenAI's chat completion parameters.
        Streaming is supported, but
        [chat-object](https://platform.openai.com/docs/api-reference/chat-streaming/streaming)
        and
        [chat-streaming](https://platform.openai.com/docs/api-reference/chat-streaming)
        response formats have limitations.
      x-codeSamples:
        - lang: Python
          source: |
            from openai import OpenAI

            client = OpenAI(
                base_url="http://0.0.0.0:8000/v1",
                api_key="EMPTY",
            )

            completion = client.chat.completions.create(
                model="modularai/Llama-3.1-8B-Instruct-GGUF",
                messages=[
                    {
                      "role": "user",
                      "content": "Who won the world series in 2020?"
                    },
                ],
                max_tokens=100
            )

            print(completion.choices[0].message.content)
        - lang: curl
          source: |
            curl -X POST http://0.0.0.0:8000/v1/chat/completions \
              -H "Content-Type: application/json" \
              -d '{
                "model": "modularai/Llama-3.1-8B-Instruct-GGUF",
                "messages": [
                  {
                    "role": "user",
                    "content": "Who won the world series in 2020?"
                  }
                ],
                "max_tokens": 100
              }'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateChatCompletionRequest'
            examples:
              basic:
                summary: Basic chat completion
                value:
                  model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                  messages:
                    - role: "user"
                      content: "Who won the world series in 2020?"
                  max_tokens: 100
              streaming:
                summary: Streaming chat completion
                value:
                  model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                  messages:
                    - role: "system"
                      content: "You are a helpful assistant."
                    - role: "user"
                      content: "Write a short poem about AI."
                  stream: true
                  max_tokens: 100
              withTools:
                summary: Chat completion with tools
                value:
                  model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                  messages:
                    - role: "system"
                      content: "You are a helpful assistant."
                    - role: "user"
                      content: "What's the weather in San Francisco?"
                  tools:
                    - type: "function"
                      function:
                        name: "get_weather"
                        description: "Get the current weather in a given location"
                        parameters:
                          type: "object"
                          properties:
                            location:
                              type: "string"
                          required: ["location"]
                  tool_choice: "auto"
                  max_tokens: 100
              jsonResponse:
                summary: Chat completion with JSON response format
                value:
                  model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                  messages:
                    - role: "system"
                      content: "You are a helpful assistant."
                    - role: "user"
                      content: "Generate a JSON object with name, age, and occupation fields"
                  response_format:
                    type: "json_object"
                  max_tokens: 100
              withImage:
                summary: Chat completion with image
                value:
                  model: "meta-llama/Llama-3.2-11B-Vision"
                  messages:
                    - role: "system"
                      content: "You are a helpful assistant that can analyze images."
                    - role: "user"
                      content: [
                        {
                          "type": "text",
                          "text": "What's in this image?"
                        },
                        {
                          "type": "image_url",
                          "image_url": {
                            "url": "https://example.com/image.jpg",
                          }
                        }
                      ]
                  max_tokens: 300
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionResponse'
              examples:
                basic:
                  summary: Basic chat completion response
                  value:
                    id: "bb41aa81eac24f4b9ed7ecd0ea593815"
                    object: "chat.completion"
                    created: 1743183374
                    model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                    choices:
                      - index: 0
                        message:
                          role: "assistant"
                          content: "The Los Angeles Dodgers won the 2020 World Series. They defeated the Tampa Bay Rays in the series 4 games to 2. This was the Dodgers' first World Series title since 1988."
                          refusal: ""
                          tool_calls: null
                          function_call: null
                        finish_reason: "stop"
                        logprobs: { "content": [], "refusal": [] }
                    service_tier: null
                    system_fingerprint: null
                    usage:
                      completion_tokens: 17
                      prompt_tokens: null
                      total_tokens: 17
                withTools:
                  summary: Chat completion response with tool calls
                  value:
                    id: "chatcmpl-123456789"
                    object: "chat.completion"
                    created: 1677858242
                    model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                    choices:
                      - index: 0
                        message:
                          role: "assistant"
                          content: null
                          tool_calls:
                            - id: "call_abc123"
                              type: "function"
                              function:
                                name: "get_weather"
                                arguments: "{\"location\":\"San Francisco, CA\"}"
        '400':
          description: Bad request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
  /v1/completions:
    post:
      operationId: createCompletion
      summary: Create completion
      description: |
        Creates a completion for the provided prompt.
        MAX supports a subset of OpenAI's completion parameters.
      x-codeSamples:
        - lang: Python
          source: |
            from openai import OpenAI

            client = OpenAI(
                base_url="http://0.0.0.0:8000/v1",
                api_key="EMPTY",
            )

            completion = client.completions.create(
                model="modularai/Llama-3.1-8B-Instruct-GGUF",
                prompt="Once upon a time",
                max_tokens=50
            )

            print(completion.choices[0].text)
        - lang: curl
          source: |
            curl -X POST http://0.0.0.0:8000/v1/completions \
              -H "Content-Type: application/json" \
              -d '{
                "model": "modularai/Llama-3.1-8B-Instruct-GGUF",
                "prompt": "Once upon a time",
                "max_tokens": 50
              }'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateCompletionRequest'
            examples:
              basic:
                summary: Basic completion
                value:
                  model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                  prompt: "Once upon a time"
                  max_tokens: 50
              withEcho:
                summary: Completion with echo
                value:
                  model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                  prompt: "Complete this sentence: The quick brown fox"
                  max_tokens: 20
                  echo: true
              withLogprobs:
                summary: Completion with logprobs
                value:
                  model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                  prompt: "The capital of France is"
                  max_tokens: 5
                  logprobs: 5
              streaming:
                summary: Streaming completion
                value:
                  model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                  prompt: "Write a short story about"
                  max_tokens: 100
                  stream: true
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateCompletionResponse'
              examples:
                basic:
                  summary: Basic completion response
                  value:
                    id: "2dd7bad9bd1b4caea0114cb292f5b23a"
                    object: "text_completion"
                    created: 1743183506
                    model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                    choices:
                      - index: 0
                        text: ", there was a brave knight who lived in a small village at the edge of a great forest. The knight was known throughout the land for his kindness and courage."
                        finish_reason: "stop"
                        logprobs:
                          text_offset: null
                          token_logprobs: []
                          tokens: null
                          top_logprobs: []
                    system_fingerprint: null
                    usage: null
                withLogprobs:
                  summary: Completion response with logprobs
                  value:
                    id: "cmpl-123456789"
                    object: "text_completion"
                    created: 1677858242
                    model: "modularai/Llama-3.1-8B-Instruct-GGUF"
                    choices:
                      - index: 0
                        text: " Paris"
                        logprobs:
                          tokens: [" Paris"]
                          token_logprobs: [-0.34]
                          top_logprobs: [
                            {" Paris": -0.34, " London": -2.1, " New": -2.5}
                          ]
                          text_offset: [24]
        '400':
          description: Bad request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
  /v1/embeddings:
    post:
      operationId: createEmbedding
      summary: Create embeddings
      description: |
        Creates an embedding vector representing the input text. MAX supports
        [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)
        model. Only string and list of strings are supported for the
        input parameter.
      x-codeSamples:
        - lang: Python
          source: |
            from openai import OpenAI

            client = OpenAI(
                base_url="http://0.0.0.0:8000/v1",
                api_key="EMPTY",
            )

            response = client.embeddings.create(
                model="sentence-transformers/all-mpnet-base-v2",
                input="The food was delicious and the service was excellent."
            )

            print(f"Embedding vector length: {len(response.data[0].embedding)}")
            print(f"First 5 values: {response.data[0].embedding[:5]}")
        - lang: curl
          source: |
            curl -X POST http://0.0.0.0:8000/v1/embeddings \
              -H "Content-Type: application/json" \
              -d '{
                "model": "sentence-transformers/all-mpnet-base-v2",
                "input": "The food was delicious and the service was excellent."
              }'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateEmbeddingRequest'
            examples:
              singleString:
                summary: Single string embedding
                value:
                  model: "sentence-transformers/all-mpnet-base-v2"
                  input: "The food was delicious and the service was excellent."
              multipleStrings:
                summary: Multiple strings embedding
                value:
                  model: "sentence-transformers/all-mpnet-base-v2"
                  input: [
                    "The food was delicious and the service was excellent.",
                    "The restaurant was very expensive and the food was mediocre."
                  ]
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateEmbeddingResponse'
              examples:
                singleString:
                  summary: Single string embedding response
                  value:
                    object: "list"
                    data: [
                      {
                        object: "embedding",
                        embedding: [0.0023064255, -0.009327292, 0.01842359, -0.0028842522, 0.0044457657],
                        index: 0
                      }
                    ]
                    model: "sentence-transformers/all-mpnet-base-v2"
                multipleStrings:
                  summary: Multiple strings embedding response
                  value:
                    object: "list"
                    data: [
                      {
                        object: "embedding",
                        embedding: [0.0023064255, -0.009327292, 0.01842359, -0.0028842522, 0.0044457657],
                        index: 0
                      },
                      {
                        object: "embedding",
                        embedding: [0.0018843175, -0.007852346, 0.02157294, -0.0031256843, 0.0051234568],
                        index: 1
                      }
                    ]
                    model: "sentence-transformers/all-mpnet-base-v2"
        '400':
          description: Bad request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
  /v1/batches:
    post:
      operationId: createBatch
      summary: Create batch
      description: |
        Creates and executes a batch from an uploaded file of requests.

        **Note:** The `v1/batches` API is only available to users in the [Mammoth](/mammoth/) public preview.
        If you create a batch and get the response `{"detail":"Not Found"}`, then you don't have access.
        [Get in touch](https://www.modular.com/company/talk-to-us) to learn about early access for enterprise teams.
      x-codeSamples:
        - lang: Python
          source: |
            from openai import OpenAI

            client = OpenAI(
                base_url="http://0.0.0.0:8000/v1",
                api_key="EMPTY",
            )

            client.batches.create(
                input_file_id=input_path,
                output_file_id=output_path,
                endpoint="/v1/chat/completions",
                completion_window="24h",
                metadata={
                    "model": "model-provider/model-ID"
                }
            )
        - lang: curl
          source: |
            curl -X POST http://0.0.0.0:8000/v1/batches \
              -H "Content-Type: application/json" \
              -d '{
                "input_file_id": "input_path",
                "output_file_id": "output_path",
                "endpoint": "/v1/chat/completions",
                "completion_window": "24h",
                "metadata": {
                    "model": "model_provider/model_ID"
                }
              }'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Batch'
            examples:
              singleString:
                summary: Batch request example for chat completions
                value:
                  completion_window: "24h"
                  endpoint: "/v1/chat/completions"
                  input_file_id: "input_path"
                  metadata:
                    model: "model_provider/model_ID"
                    metadata_key: "metadata_value"
                  output_file_id: "output_path"
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BatchResponse'
              examples:
                basic:
                  summary: Basic batch completion response
                  value:
                    id: "batch_ID"
                    object: "batch"
                    endpoint: "/v1/chat/completions"
                    errors: null
                    input_file_id: "input_path"
                    completion_window: "24h"
                    status: "validating"
                    output_file_id: "output_path"
                    error_file_id: null
                    created_at: 1711471533
                    in_progress_at: null
                    expires_at: null
                    finalizing_at: null
                    completed_at: null
                    failed_at: null
                    expired_at: null
                    cancelling_at: null
                    cancelled_at: null
                    request_counts:
                      total: 0
                      completed: 0
                      failed: 0
                    metadata:
                      model: "model_provider/model_ID"
                      metadata_key: "metadata_value"
        '400':
          description: Bad request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
  /v1/models:
    get:
      operationId: listModels
      summary: List models
      description: |
        Lists the currently available models.
        MAX returns only the one model that it's currently serving.
      x-codeSamples:
        - lang: Python
          source: |
            from openai import OpenAI

            client = OpenAI(
                base_url="http://0.0.0.0:8000/v1",
                api_key="EMPTY",
            )

            models = client.models.list()

            for model in models.data:
                print(f"Model ID: {model.id}")
                print(f"Object Type: {model.object}")
                print(f"Owned By: {model.owned_by}")
        - lang: curl
          source: |
            curl -X GET http://0.0.0.0:8000/v1/models \
              -H "Content-Type: application/json"
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListModelsResponse'
              examples:
                models:
                  summary: Models list response
                  value:
                    object: "list"
                    data: [
                      {
                        id: "modularai/Llama-3.1-8B-Instruct-GGUF",
                        object: "model",
                        created: null,
                        owned_by: ""
                      }
                    ]
        '400':
          description: Bad request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
  /health:
    get:
      operationId: health
      summary: Health check
      description: |
        Returns the health status of the service. Used by tools like lm-eval
        to determine when the service is ready to accept requests.
      x-codeSamples:
        - lang: curl
          source: |
            curl -X GET http://0.0.0.0:8000/health
      responses:
        '200':
          description: Service is healthy
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HealthResponse'
              examples:
                healthy:
                  summary: Healthy response
                  value:
                    status: "ok"
        '503':
          description: Service unavailable
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HealthResponse'
              examples:
                unhealthy:
                  summary: Unhealthy response
                  value:
                    status: "unavailable"
components:
  schemas:
    CreateChatCompletionRequest:
      type: object
      required:
        - model
        - messages
      properties:
        model:
          type: string
          description: |
            ID of the model to use.
            Supported by MAX but handled by frontend load balancer to route traffic to nodes.
        messages:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionMessage'
          description: |
            A list of messages comprising the conversation so far.
            MAX supports content and role parameters.
        max_tokens:
          type: integer
          minimum: 1
          description: |
            The maximum number of tokens to generate in the chat completion.
        logprobs:
          type: boolean
          description: |
            Whether to return log probabilities of the output tokens or not.
        response_format:
          type: object
          description: |
            An object specifying the format that the model must output.
          properties:
            type:
              type: string
              enum: [text, json_object]
              description: The format type
        stream:
          type: boolean
          default: false
          description: |
            If set, partial message deltas will be sent.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionTool'
          description: |
            A list of tools the model may call.
        tool_choice:
          oneOf:
            - type: string
              enum: [none, auto, required]
            - type: object
              required:
                - type
                - function
              properties:
                type:
                  type: string
                  enum: [function]
                function:
                  type: object
                  required:
                    - name
                  properties:
                    name:
                      type: string
          description: |
            Controls which (if any) tool is called by the model.

    ChatCompletionMessage:
      type: object
      required:
        - role
        - content
      properties:
        role:
          type: string
          enum: [system, user, assistant, function]
          description: |
            The role of the message author.
        content:
          oneOf:
            - type: string
              description: |
                The content of the message as a string.
            - type: array
              description: |
                The content of the message as an array of content parts. For user messages, content parts can be of type text or image_url.
              items:
                oneOf:
                  - type: object
                    required:
                      - type
                      - text
                    properties:
                      type:
                        type: string
                        enum: [text]
                        description: The type of the content part.
                      text:
                        type: string
                        description: The text content.
                  - type: object
                    required:
                      - type
                      - image_url
                    properties:
                      type:
                        type: string
                        enum: [image_url]
                        description: The type of the content part.
                      image_url:
                        type: object
                        required:
                          - url
                        properties:
                          url:
                            type: string
                            description: |
                              Either a URL of the image or a base64 encoded image with the data-uri scheme.
        name:
          type: string
          description: |
            The name of the author of this message.

    ChatCompletionTool:
      type: object
      required:
        - type
      properties:
        type:
          type: string
          enum: [function]
          description: The type of the tool
        function:
          $ref: '#/components/schemas/FunctionObject'
    FunctionObject:
      type: object
      required:
        - name
      properties:
        name:
          type: string
          description: The name of the function to be called
        description:
          type: string
          description: A description of what the function does
        parameters:
          type: object
          description: The parameters the function accepts, described as a JSON Schema object

    CreateChatCompletionResponse:
      type: object
      required:
        - id
        - object
        - created
        - model
        - choices
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion
        object:
          type: string
          enum: [chat.completion]
          description: The object type, which is always `chat.completion`
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion was created
        model:
          type: string
          description: The model used for the chat completion
        choices:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionChoice'
          description: |
            A list of chat completion choices.
            MAX does not support `finish_reason` and `logprobs` in choices.
        usage:
          $ref: '#/components/schemas/Usage'
          description: |
            Usage statistics for the completion request.

    ChatCompletionChoice:
      type: object
      required:
        - index
        - message
      properties:
        index:
          type: integer
          description: The index of the choice in the list of choices
        message:
          $ref: '#/components/schemas/ChatCompletionMessage'
          description: The chat completion message

    CreateCompletionRequest:
      type: object
      required:
        - model
        - prompt
      properties:
        model:
          type: string
          description: |
            ID of the model to use.
        prompt:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
          description: |
            The prompt to generate completions for.
        echo:
          type: boolean
          default: false
          description: |
            Echo back the prompt in addition to the completion.
        max_tokens:
          type: integer
          minimum: 1
          description: |
            The maximum number of tokens to generate in the completion.
        stream:
          type: boolean
          default: false
          description: |
            Whether to stream back partial progress.
        logprobs:
          type: integer
          minimum: 0
          description: |
            Include the log probabilities on the logprobs most likely tokens.

    CreateCompletionResponse:
      type: object
      required:
        - id
        - object
        - created
        - model
        - choices
      properties:
        id:
          type: string
          description: A unique identifier for the completion
        object:
          type: string
          enum: [text_completion]
          description: The object type, which is always `text_completion`
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the completion was created
        model:
          type: string
          description: The model used for the completion
        choices:
          type: array
          items:
            $ref: '#/components/schemas/CompletionChoice'
          description: A list of completion choices
        usage:
          $ref: '#/components/schemas/Usage'
          description: |
            Usage statistics for the completion request.

    CompletionChoice:
      type: object
      required:
        - index
        - text
      properties:
        index:
          type: integer
          description: The index of the choice in the list of choices
        text:
          type: string
          description: The generated text
        finish_reason:
          type: string
          enum: [stop, length, content_filter, null]
          description: The reason the completion finished

    CreateEmbeddingRequest:
      type: object
      required:
        - model
        - input
      properties:
        model:
          type: string
          description: |
            ID of the model to use. MAX supports
            [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2).
        input:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
          description: |
            The text to embed.
            MAX supports strings and list of strings.
        encoding_format:
          type: string
          enum: [float, base64]
          default: float
          description: |
            The format to return the embeddings in.
        dimensions:
          type: integer
          description: |
            The number of dimensions the resulting output embeddings should have.
        user:
          type: string
          description: |
            A unique identifier representing your end-user.

    CreateEmbeddingResponse:
      type: object
      required:
        - object
        - data
        - model
      properties:
        object:
          type: string
          enum: [list]
          description: The object type, which is always list
        data:
          type: array
          items:
            $ref: '#/components/schemas/Embedding'
          description: A list of embeddings
        model:
          type: string
          description: The model used for the embeddings
        usage:
          $ref: '#/components/schemas/Usage'
          description: Usage statistics for the embedding request

    Embedding:
      type: object
      required:
        - object
        - embedding
        - index
      properties:
        object:
          type: string
          enum: [embedding]
          description: The object type, which is always embedding
        embedding:
          type: array
          items:
            type: number
            format: float
          description: The embedding vector
        index:
          type: integer
          description: The index of the embedding in the list of embeddings

    Batch:
      type: object
      required:
        - completion_window
        - endpoint
        - input_file_id
        - output_file_id
        - metadata
      properties:
        completion_window:
          type: string
          description: |
            The time frame within which the batch should be processed.
            Supported completion windows include: `6h`, `12h`, `24h`, or `7d`.
        endpoint:
          type: string
          description: |
            The endpoint to be used for all requests in the batch.
            Currently only the `/v1/chat/completions` endpoint is supported.
        input_file_id:
          type: string
          description: |
            The path to an uploaded file that contains requests for the new
            batch.
        metadata:
          type: map
          description: |
            A set of key-value pairs that can be attached to an object. The
            `metadata["model"]` field is required.
        output_file_id:
          type: string
          description: |
            The path to the batch output file.

    BatchResponse:
      type: object
      properties:
        id:
          type: string
          description: |
            Unique identifier for the batch.
        object:
          type: string
          description: |
            Object type `"batch"`.
        endpoint:
          type: string
          description: |
            Target endpoint for the batch request.
        errors:
          type: string
          description: |
            List of errors if any.
        input_file_id:
          type: string
          description: |
            File ID containing batch input data.
        completion_window:
          type: string
          description: |
            Allowed processing window for the batch.
        status:
          type: string
          description: |
            Status of batch job creation.
        output_file_id:
          type: string
          description: |
            File ID containing the batch output results.
        error_file_id:
          type: string
          description: |
            File ID containing errors for failed batch items.
        created_at:
          type: integer
          description: |
            Unix timestamp for when the batch was created.
        in_progress_at:
          type: integer
          description: |
            Unix timestamp when the batch started processing.
        expires_at:
          type: integer
          description: |
            Unix timestamp when the batch will expire if not processed.
        completed_at:
          type: integer
          description: |
            Unix timestamp when the batch was completed.
        failed_at:
          type: integer
          description: |
            Unix timestamp when the batch failed.
        expired_at:
          type: integer
          description: |
            Unix timestamp when the batch expired based on completion window.
        cancelling_at:
          type: integer
          description: |
            Unix timestamp when the batch began cancelling.
        cancelled_at:
          type: integer
          description: |
            Unix timestamp when the batch was cancelled.
        request_counts:
          type: object
          description: |
            Count of requests in various states, including total number of
            requests, number of successfully completed requests, and number of
            failed requests.
          properties:
            total:
              type: integer
            completed:
              type: integer
            failed:
              type: integer
        metadata:
          type: object
          description: |
            Set of key-value pairs that can be attached to an object. This can
            be useful for specifying model, defining the batch output path, or
            storing additional information about the object in a structured
            format.
          additionalProperties:
            type: string

    ListModelsResponse:
      type: object
      required:
        - object
        - data
      properties:
        object:
          type: string
          enum: [list]
          description: The object type, which is always list
        data:
          type: array
          items:
            $ref: '#/components/schemas/Model'
          description: A list of model objects. MAX returns only one model.
    Model:
      type: object
      required:
        - id
        - object
        - created
        - owned_by
      properties:
        id:
          type: string
          description: The model identifier
        object:
          type: string
          enum: [model]
          description: The object type, which is always model
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the model was created
        owned_by:
          type: string
          description: The organization that owns the model
    Usage:
      type: object
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
      properties:
        prompt_tokens:
          type: integer
          description: Number of tokens in the prompt
        completion_tokens:
          type: integer
          description: Number of tokens in the generated completion
        total_tokens:
          type: integer
          description: Total number of tokens used in the request (prompt + completion)

    ErrorResponse:
      type: object
      description: |
        An error response from the MAX REST API.
      required:
        - error
      properties:
        error:
          type: object
          description: |
            The error object.
          required:
            - message
            - type
            - code
          properties:
            message:
              type: string
              description: A human-readable error message
            type:
              type: string
              description: The type of error
            code:
              type: string
              description: A machine-readable error code
            param:
              type: string
              description: The parameter that caused the error
    HealthResponse:
      type: object
      description: |
        Health check response from the MAX REST API.
      required:
        - status
      properties:
        status:
          type: string
          enum: [ok, unavailable]
          description: The health status of the service
