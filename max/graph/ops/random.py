# ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""Ops for generating random numbers."""

from __future__ import annotations

from contextvars import ContextVar

import numpy as np
from max.dtype import DType
from max.mlir.dialects import rmo

from .. import dtype_promotion
from ..graph import Graph
from ..type import DeviceRef, TensorType
from ..value import TensorValue, TensorValueLike

SEED = ContextVar[TensorValue]("Seed")
SeedType = TensorType(DType.int64, [], device=DeviceRef.CPU())


def _rotate_seed(seed: TensorValue):
    # Let's just get some different random numbers
    # from the initial seed for now.
    return seed + 1


def assert_scalar(value: TensorValueLike):
    if isinstance(value, (np.ndarray, TensorValue)) and value.shape:
        raise ValueError("Expected a scalar value")


def _next_seed():
    try:
        seed = SEED.get()
    except LookupError:
        raise RuntimeError("No seed set! Set with `ops.random.set_seed`.")
    SEED.set(_rotate_seed(seed))
    return seed


def set_seed(seed: TensorValue | int = 0):
    """Sets the seed for random numbers generated in the graph.

    This must be set at least once for each graph using random number utilities.
    - If set to a static value, random numbers generated by the graph will be
        deterministic with each graph execution.
    - To get different random values, expose a `seed` input to your graph
        and call `set_seed` with it.

    Args:
        seed: The seed value to use for future random operations. Each subsequent
            random operation will rotate this seed value automatically.
    """
    assert_scalar(seed)
    seed = dtype_promotion._promote_to_strong(
        seed, DType.int64, DeviceRef.CPU()
    )
    if seed.dtype != DType.int64:
        raise TypeError("Seed value must be int64")
    return SEED.set(seed)


def gaussian(
    like: TensorType,
    mean: TensorValueLike = 0.0,
    std: TensorValueLike = 1.0,
) -> TensorValue:
    assert_scalar(mean)
    assert_scalar(std)
    # Check whether we have a seed before we add other constants to the graph.
    seed = _next_seed()
    return Graph.current._add_op(
        rmo.mo_random_normal,
        result=like.to_mlir(),
        shape=TensorValue(like.shape),
        mean=dtype_promotion._promote_to_strong(
            mean, DType.float32, DeviceRef.CPU()
        ),
        variance=dtype_promotion._promote_to_strong(
            std, DType.float32, DeviceRef.CPU()
        ),
        seed=seed,
    )[0].tensor


# Alias normal <-> gaussian
normal = gaussian


def uniform(
    like: TensorType,
    range: tuple[TensorValueLike, TensorValueLike] = (0, 1),
) -> TensorValue:
    lower, upper = range

    assert_scalar(lower)
    assert_scalar(upper)
    # Check whether we have a seed before we add other constants to the graph.
    seed = _next_seed()
    return Graph.current._add_op(
        rmo.mo_random_uniform,
        result=like.to_mlir(),
        shape=TensorValue(like.shape),
        lower_bound=dtype_promotion._promote_to_strong(
            lower, like.dtype, DeviceRef.CPU()
        ),
        upper_bound=dtype_promotion._promote_to_strong(
            upper, like.dtype, DeviceRef.CPU()
        ),
        seed=seed,
    )[0].tensor
